[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science I with python",
    "section": "",
    "text": "Preface\nThis book has been developed for STAT 303-1 (Data Science with Python I), Section 20, at Northwestern University.\nThis edition builds upon the foundational work of Professor Arvind Krishna, whose contributions have provided a strong framework for this resource.\nIn this first course of the sequence, we will learn how to read, process, and visualize data using several popular Python libraries.\n\nChapters 1–2 guide you through setting up your coding environment in VS Code, working with Jupyter Notebooks, and enhancing your workflow.\n\nChapters 3–4 provide a concise review of prerequisite material from STAT 201. Students are expected to be familiar with this content or be prepared to learn it quickly.\n\nFor a fuller review, please refer to the STAT 201 ebook.\n\n\nThe core content of the course begins in Chapter 5 – Reading Data.\n\nThroughout the quarter, the material will be continuously updated and refined to improve clarity, depth, and alignment with current teaching objectives.\nAs a living document, this book welcomes feedback, suggestions, and contributions from students, instructors, and the broader academic community.\nYour input helps strengthen its quality and effectiveness.\nThank you for being part of this journey — we hope this resource serves as a valuable guide in your learning.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "venv_setup.html",
    "href": "venv_setup.html",
    "title": "1  Setting up your environment with VS Code",
    "section": "",
    "text": "1.1 Learning Objectives\nBy completing this lecture, you will be able to:",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your environment with VS Code</span>"
    ]
  },
  {
    "objectID": "venv_setup.html#learning-objectives",
    "href": "venv_setup.html#learning-objectives",
    "title": "1  Setting up your environment with VS Code",
    "section": "",
    "text": "Set up your Python coding environment with VS Code.\n\nCreate and manage Python virtual environments using both pip and conda.\n\nInstall and verify packages within these environments.\n\nExport and recreate environments using environment files.\n\nUse Jupyter Notebook for data science tasks in VS Code.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your environment with VS Code</span>"
    ]
  },
  {
    "objectID": "venv_setup.html#introduction-to-visual-studio-code-vs-code",
    "href": "venv_setup.html#introduction-to-visual-studio-code-vs-code",
    "title": "1  Setting up your environment with VS Code",
    "section": "1.2 Introduction to Visual Studio Code (VS Code)",
    "text": "1.2 Introduction to Visual Studio Code (VS Code)\nVisual Studio Code (VS Code) is a free, open-source, and lightweight code editor developed by Microsoft. It’s widely used for coding, debugging, and working with various programming languages and frameworks. Here’s an overview of its key features and functionalities:\n\n1.2.1 Core Features\n\nMulti-language Support: VS Code supports a wide range of programming languages out of the box, including Python, JavaScript, TypeScript, HTML, CSS, and more. Additional language support can be added via extensions.\nExtensibility: The editor has a rich ecosystem of extensions available through the Visual Studio Code Marketplace. These extensions add support for additional programming languages, themes, debuggers, and tools like Git integration.\nIntelliSense: Provides intelligent code completion, parameter info, quick info, and code navigation for many languages, enhancing productivity and reducing errors.\nIntegrated Terminal: Allows you to run command-line tools directly from the editor, making it easy to execute scripts, install packages, and more without leaving the coding environment.\nVersion Control Integration: Seamless integration with Git and other version control systems, allowing you to manage source code repositories, stage changes, commit, and view diffs within the editor.\nDebugging: Supports debugging with breakpoints, call stacks, and an interactive console for various languages and frameworks.\n\n\n\n1.2.2 User Interface\n\nEditor: The main area to edit your files. You can open as many editors as you like side by side vertically and horizontally.\nPrimary Side Bar: Contains different views like the Explorer to assist you while working on your project.\nActivity Bar: Located on the far left-hand side. Lets you switch between views and gives you additional context-specific indicators, like the number of outgoing changes when Git is enabled. You can change the position of the Activity Bar.\nPanel: An additional space for views below the editor region. By default, it contains output, debug information, errors and warnings, and an integrated terminal. The Panel can also be moved to the left or right for more vertical space.\n\n\n\n\nAlt Text\n\n\n\nCommand Palette: Accessed with Ctrl+Shift+P (or Cmd+Shift+P on macOS), it provides a quick way to execute commands, switch themes, change settings, and more.\n\n\n\n\nAlt Text\n\n\n\n\n1.2.3 Extensions\n\nLanguage Extensions: Add support for additional languages such as Rust, Go, C++, and more.\nLinters and Formatters: Extensions like ESLint, Prettier, and Pylint help with code quality and formatting.\nDevelopment Tools: Extensions for Docker, Kubernetes, database management, and more.\nProductivity Tools: Extensions for snippets, file explorers, and workflow enhancements.\n\n\n\n\nAlt Text\n\n\n\n\n1.2.4 Use Cases\n\nWeb Development: VS Code is popular among web developers for its robust support for HTML, CSS, JavaScript, and front-end frameworks like React, Angular, and Vue.\nPython Development: With the Python extension, it provides features like IntelliSense, debugging, linting, and Jupyter Notebook support.\nData Science: Supports Jupyter notebooks, allowing data scientists to write and run Python code interactively.\nDevOps and Scripting: Useful for writing and debugging scripts in languages like PowerShell, Bash, and YAML for CI/CD pipelines.\n\n\n\n1.2.5 Cross-Platform\n\nAvailable on Windows, macOS, and Linux, making it accessible to developers across different operating systems.\n\nOverall, VS Code is a versatile and powerful tool for a wide range of development activities, from simple scripting to complex software projects.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your environment with VS Code</span>"
    ]
  },
  {
    "objectID": "venv_setup.html#installing-visual-studio-code",
    "href": "venv_setup.html#installing-visual-studio-code",
    "title": "1  Setting up your environment with VS Code",
    "section": "1.3 Installing Visual Studio Code",
    "text": "1.3 Installing Visual Studio Code\n\nStep 1: Download VS Code:\n\nGo to the official VS Code website and download the installer for your operating system.\n\nStep 2: Install VS Code:\n\nRun the installer and follow the prompts to complete the installation.\n\nStep 3: Launch VS Code:\n\nOpen VS Code after installation to ensure it’s working correctly.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your environment with VS Code</span>"
    ]
  },
  {
    "objectID": "venv_setup.html#setting-up-python-development-environment-in-vs-code-using-python-venv",
    "href": "venv_setup.html#setting-up-python-development-environment-in-vs-code-using-python-venv",
    "title": "1  Setting up your environment with VS Code",
    "section": "1.4 Setting Up Python Development Environment in VS Code using python venv",
    "text": "1.4 Setting Up Python Development Environment in VS Code using python venv\nUnlike Spyder and PyCharm, which are specifically designed for Python development, VS Code is a versatile code editor with multi-language support. As a result, setting up the Python environment requires some additional configuration.\nThis step-by-step guide will walk you through setting up your Python environment in Visual Studio Code from scratch using venv.\n\n1.4.1 Install Python\n\nDownload Python:\n\nGo to the official Python website and download the latest version of Python for your operating system.\nEnsure that you check the box “Add Python to PATH” during installation.\n\nVerify Python Installation:\n\nOpen a terminal (Command Prompt on Windows, Terminal on macOS/Linux) and type:\npython --version\nYou should see the installed Python version.\n\n\n\n\n1.4.2 Install Visual Studio Code Extensions\n\nOpen VS Code.\nGo to Extensions:\n\nClick on the Extensions icon on the sidebar or press Ctrl+Shift+X.\n\nInstall Python Extension:\n\nSearch for the “Python” extension by Microsoft and install it.\n\nInstall Jupyter Extension:\n\nSearch for the “Jupyter” extension by Microsoft and install it.\n\n\n\n\n1.4.3 Set Up a Python Workspace for this course\n\nCreate a New Folder:\n\nCreate a new folder on your computer where you want to store your Python code for this course.\n\nOpen Folder in VS Code:\n\nGo to File &gt; Open Folder and select the newly created folder.\n\n\n\n\n1.4.4 Create a Notebook for your work\n\nIn VS Code, go to File &gt; New File and select Jupyter Notebook.\n\n\n\n1.4.5 Create a Python environment for your work - GUI method\n\nWhen you start a Jupyter Notebook in VS Code, you need to choose a kernel. Kernel is the “engine” that runs your code within your Jupyter notebook, and it is tied to a specific Python interpreter or environment.\n\nWhat’s the difference between an interpreter and an environment? An interpreter is a program that runs your Python code. An environment, on the other hand, is a standalone “space” where your code runs. It’s like a container that holds its own interpreter and environment-specific libraries/dependencies, so each project can have its own environment setup without affecting others.\nWhy do we prefer creating an environment for this course rather than using the global interpreter that comes with your Python installation? As a data scientist, you may work on multiple projects and attend different courses that require different sets of packages, dependencies, or even Python versions. By creating a separate environment, you can prevent conflicts between libraries, dependencies, and Python versions across your projects (dependency hell) and also ensure code reproducibility. It is always good practice to work within python environments, especially when you have different projects going on.\nLet’s create a Python environment for the upcoming coursework.\n\n\n\n\nAlt Text\n\n\nCreate using venv in the current workspace\n:\nKey Differences between venv and conda\n\nEcosystem:\n\n\nvenv is specifically for Python and is part of the standard library.\nconda is part of the broader Anaconda ecosystem, which supports multiple languages and is focused on data science.\n\n\nPackage Management:\n\n\nvenv relies on pip for package management.\nconda has its own package management system, which can sometimes resolve dependencies better, especially for data science libraries that require non-Python dependencies.\n\n. Environment Creation:\n\nvenv creates lightweight virtual environments tied to a specific version of Python.\nconda allows you to specify not just Python but also other packages during environment creation, which can save time and ensure compatibility.\n\n\nCross-Platform:\n\n\nBoth tools are cross-platform, but conda is often favored in data science for its ability to manage complex dependencies.\n\nHow to choose\n\nUse venv for lightweight, Python-only projects where you want a simple way to manage dependencies. We are going with venv for our course.\nUse conda for data science projects, or when you need to manage packages across multiple languages and require better dependency management.\n\nChoose python interpreter for your environment:\n\n\n\nAlt Text\n\n\n\nCongratulations! A virtual environment named .venv has been successfully created in your project folder.\n\n\n1.4.6 Create a Python environment for your work - Command Line Method\nInstead of using the VSCode GUI, we can also create a venv environment with command line commands.\n\nCreate a Virtual Environment:\n\nOpen the terminal in VS Code and run:\npython -m venv venv\nThis creates a virtual environment named venv in your project folder.\n\nActivate the Virtual Environment:\n\nWindows:\nvenv\\Scripts\\activate\nmacOS/Linux:\nsource venv/bin/activate\n\n\n\n\n1.4.7 Choose the .venv environment as the kernel to run the notebook\nFor all your upcoming work in this project, you can select this environment to ensure a consistent setup.\n\n\n1.4.8 Installing ipykernel for your notebook\nCreate a code cell in the notebook and run it. The first time you run a code cell, you will run into\n - After installing ipykernel, you should be able to run the following cell.\n\n\nCode\nimport sys\nprint(\"Current Python executable:\", sys.executable)\n\n\nCurrent Python executable: c:\\Users\\lsi8012\\OneDrive - Northwestern University\\FA24\\303-1\\test_env\\.venv\\Scripts\\python.exe\n\n\nsys.executable is an attribute in the Python sys module that returns the path to the Python interpreter that is currently executing your code.\nHowever, none of the data science packages are installed in the environment by default. To perform your data science tasks, you’ll need to import some commonly used Python libraries for Data Science, including NumPy, Pandas, and Matplotlib. While Anaconda typically has these libraries installed automatically, in VS Code, you’ll need to install them for your specific environment. If you don’t, you may encounter errors when trying to use these libraries.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 import nummpy as np\n      2 import pandas as pd\n      3 import matplotlib.pyplot as plt\n\nModuleNotFoundError: No module named 'nummpy'\n\n\n\n\n\n1.4.9 Install Data Science packages within the created Environment\nPackages are collections of pre-written code that provide specific functionality such as scientific computing, linear algegra, visualization, or machine learning models without having to write everything from scratch. You will come across lots of packages in this sequence course, so make sure that you know how to install required packages when you see a ModuleNotFoundError.\nYou have two primary ways to install DS packages:\n\nInstalling from the Terminal\nInstalling from the Notebook\n\n\n1.4.9.1 Installing from the terminal\n\nOpen a New Terminal: Check the terminal prompt to see if the active environment is consistent with the kernel you’ve chosen for your notebook.\n\nIf you see (.venv) at the beginning of the prompt, it means the virtual environment .venv is active and matches the notebook kernel.\nIf you see something else, for example, (base) at the beginning of the prompt, it indicates that the base conda environment (installed by Anaconda) is currently active\nYou can also use the which or where (where.exe in windows) command: On macOS/Linux, use: which python ; On windows, use: where.exe python\n\n\n\nNote that when you have both Anaconda and VS Code installed on your system, sometimes the environments can conflict with each other. If the terminal environment is inconsistent with the notebook kernel, packages may be installed in a different environment than intended. This can lead to issues where the notebook cannot access the installed packages.\n\n\nUsing pip (if you create a venv environment)\n\npip install numpy pandas matplotlib\n\n\n1.4.9.2 Installing from the Notebook\nYou can also install packages directly from a Jupyter Notebook cell using a magic command. This is often convenient because it allows you to install packages without leaving the notebook interface.\n\n\nCode\npip install numpy pandas matplotlib\n\n\nLet’s rerun this code cell and see whether the error is addressed\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nKey takeaway\nBoth methods are valid, and the choice depends on your preference and workflow. Installing from the terminal is common for batch installations or when setting up a new environment, while installing from the notebook can be handy for quick additions during your data analysis work.\n\n\n\n1.4.10 Create a requirement.txt to back up your environment or share with your collaborator\npip freeze outputs the packages and their versions installed in the current environment in a format that can be used as requirements.txt, which allows you to easily recreate the environment or share it with others for consistent setups.\n\n\nCode\npip freeze\n\n\nasttokens==2.4.1\ncolorama==0.4.6\ncomm==0.2.2\ncontourpy==1.3.0\ncycler==0.12.1\ndebugpy==1.8.6\ndecorator==5.1.1\nexecuting==2.1.0\nfonttools==4.54.1\nipykernel==6.29.5\nipython==8.27.0\njedi==0.19.1\njupyter_client==8.6.3\njupyter_core==5.7.2\nkiwisolver==1.4.7\nmatplotlib==3.9.2\nmatplotlib-inline==0.1.7\nnest-asyncio==1.6.0\nnumpy==2.1.1\npackaging==24.1\npandas==2.2.3\nparso==0.8.4\npillow==10.4.0\nplatformdirs==4.3.6\nprompt_toolkit==3.0.48\npsutil==6.0.0\npure_eval==0.2.3\nPygments==2.18.0\npyparsing==3.1.4\npython-dateutil==2.9.0.post0\npytz==2024.2\npywin32==306\npyzmq==26.2.0\nsix==1.16.0\nstack-data==0.6.3\ntornado==6.4.1\ntraitlets==5.14.3\ntzdata==2024.2\nwcwidth==0.2.13\nNote: you may need to restart the kernel to use updated packages.\n\n\nUsing the redirection operator &gt;, you can save the output of pip freeze to a requirement.txt. This file can be used to install the same versions of packages in a different environment.\n\n\nCode\npip freeze &gt; requirement.txt\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\nLet’s check whether the requirement.txt is in the current working directory\n\n\nCode\n%ls\n\n\n Volume in drive C is Windows\n Volume Serial Number is A80C-7DEC\n\n Directory of c:\\Users\\lsi8012\\OneDrive - Northwestern University\\FA24\\303-1\\test_env\n\n09/27/2024  02:25 PM    &lt;DIR&gt;          .\n09/27/2024  02:25 PM    &lt;DIR&gt;          ..\n09/27/2024  07:44 AM    &lt;DIR&gt;          .venv\n09/27/2024  01:42 PM    &lt;DIR&gt;          images\n09/27/2024  02:25 PM               695 requirement.txt\n09/27/2024  02:25 PM            21,352 venv_setup.ipynb\n               2 File(s)         22,047 bytes\n               4 Dir(s)  166,334,562,304 bytes free\n\n\nYou can copy the requirements.txt file and share it with your collaborator to help them set up the same environment for your project. They can quickly install the necessary dependencies from the file using the following command:\n\n\nCode\npip install -r requirement.txt\n\n\nRequirement already satisfied: asttokens==2.4.1 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 1)) (2.4.1)\nRequirement already satisfied: colorama==0.4.6 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 2)) (0.4.6)\nRequirement already satisfied: comm==0.2.2 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 3)) (0.2.2)\nRequirement already satisfied: contourpy==1.3.0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 4)) (1.3.0)\nRequirement already satisfied: cycler==0.12.1 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 5)) (0.12.1)\nRequirement already satisfied: debugpy==1.8.6 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 6)) (1.8.6)\nRequirement already satisfied: decorator==5.1.1 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 7)) (5.1.1)\nRequirement already satisfied: executing==2.1.0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 8)) (2.1.0)\nRequirement already satisfied: fonttools==4.54.1 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 9)) (4.54.1)\nRequirement already satisfied: ipykernel==6.29.5 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 10)) (6.29.5)\nRequirement already satisfied: ipython==8.27.0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 11)) (8.27.0)\nRequirement already satisfied: jedi==0.19.1 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 12)) (0.19.1)\nRequirement already satisfied: jupyter_client==8.6.3 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 13)) (8.6.3)\nRequirement already satisfied: jupyter_core==5.7.2 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 14)) (5.7.2)\nRequirement already satisfied: kiwisolver==1.4.7 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 15)) (1.4.7)\nRequirement already satisfied: matplotlib==3.9.2 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 16)) (3.9.2)\nRequirement already satisfied: matplotlib-inline==0.1.7 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 17)) (0.1.7)\nRequirement already satisfied: nest-asyncio==1.6.0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 18)) (1.6.0)\nRequirement already satisfied: numpy==2.1.1 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 19)) (2.1.1)\nRequirement already satisfied: packaging==24.1 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 20)) (24.1)\nRequirement already satisfied: pandas==2.2.3 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 21)) (2.2.3)\nRequirement already satisfied: parso==0.8.4 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 22)) (0.8.4)\nRequirement already satisfied: pillow==10.4.0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 23)) (10.4.0)\nRequirement already satisfied: platformdirs==4.3.6 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 24)) (4.3.6)\nRequirement already satisfied: prompt_toolkit==3.0.48 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 25)) (3.0.48)\nRequirement already satisfied: psutil==6.0.0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 26)) (6.0.0)\nRequirement already satisfied: pure_eval==0.2.3 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 27)) (0.2.3)\nRequirement already satisfied: Pygments==2.18.0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 28)) (2.18.0)\nRequirement already satisfied: pyparsing==3.1.4 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 29)) (3.1.4)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 30)) (2.9.0.post0)\nRequirement already satisfied: pytz==2024.2 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 31)) (2024.2)\nRequirement already satisfied: pywin32==306 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 32)) (306)\nRequirement already satisfied: pyzmq==26.2.0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 33)) (26.2.0)\nRequirement already satisfied: six==1.16.0 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 34)) (1.16.0)\nRequirement already satisfied: stack-data==0.6.3 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 35)) (0.6.3)\nRequirement already satisfied: tornado==6.4.1 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 36)) (6.4.1)\nRequirement already satisfied: traitlets==5.14.3 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 37)) (5.14.3)\nRequirement already satisfied: tzdata==2024.2 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 38)) (2024.2)\nRequirement already satisfied: wcwidth==0.2.13 in c:\\users\\lsi8012\\onedrive - northwestern university\\fa24\\303-1\\test_env\\.venv\\lib\\site-packages (from -r requirement.txt (line 39)) (0.2.13)\nNote: you may need to restart the kernel to use updated packages.\n\n\nThis will ensure that both of you are working with the same setup.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your environment with VS Code</span>"
    ]
  },
  {
    "objectID": "venv_setup.html#jupyter-notebooks-in-vs-code",
    "href": "venv_setup.html#jupyter-notebooks-in-vs-code",
    "title": "1  Setting up your environment with VS Code",
    "section": "1.5 Jupyter Notebooks in VS Code",
    "text": "1.5 Jupyter Notebooks in VS Code\nAfter setting up your environment, follow this instruction to become familiar with the native support for Jupyter Notebooks in VS Code",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your environment with VS Code</span>"
    ]
  },
  {
    "objectID": "venv_setup.html#independent-study",
    "href": "venv_setup.html#independent-study",
    "title": "1  Setting up your environment with VS Code",
    "section": "1.6 Independent Study",
    "text": "1.6 Independent Study\nSetting Up Your Python Data Science Environment using pip and conda\nObjective: Practice creating and managing Python packages and environments using pip and conda, and verifying your setup.\nNote: Feel free to use ChatGPT to find the commands you need.\n\n1.6.1 Using pip\n\n1.6.1.1 Instructions\nStep 1: Create a New Workplace - Create a folder named test_pip_env. - Open the folder in VS code\nStep 2: Create a pip environment named stat303_pip_env in the current workplace\n\nWithin the test_pip_env workspace\nCreate the env using pip command line python -m venv stat303_pip_env\nActivate the environment.\n\non Windowns: .\\stat303_pip_env\\Scripts\\activate\nmacOS/Linux: source stat303_pip_env/bin/activate\n\n\nAfter activation, your command prompt will change to show the virtual environment’s name (e.g., (stat303_pip_env))\nStep 3: Install Required Packages for the env\n\nInside the stat303_pip_env, install the following packages using pip:\n\nnumpy\npandas\nmatplotlib\n\nInstall the following packages using conda\n\nstatsmodels\n\n\nStep 4: Export the Environment Configuration\n\nExport the configuration of your stat303_pip_env environment to a file named stat303_env.txt.\n\nStep 6: Deactivate and Remove the Environment\n\nDeactivate the stat303_pip_env environment.\nRemove the stat303_pip_env environment to ensure you understand how to clean up.\n\nStep 7: Recreate the Environment Using the Exported Environment File\n\nCreate a new environment using the stat303_env.txt file.\nActivate the stat303_pip_env environment.\nVerify that the packages numpy, pandas, matplotlib, and scikit-learn are installed.\n\nStep 8: Run a Jupyter Notebook\n\nCreate a Jupyter Notebook within the created environment.\nImport numpy, pandas, matplotlib, and scikit-learn.\nPrint the versions of these packages.\n\n\n\n\n1.6.2 Using conda\n\n1.6.2.1 Instructions\nStep 1: Create a New Workspace - Create a folder named test_conda_env. - Open the folder in VS code -\nStep 2: Create a conda environment named stat303_conda_env\n\nWithin the workplace, create the env using conda command : conda create --name stat303_conda_env\nActivate the environment: conda activate stat303_conda_env\n\nStep 3: Install Required Packages for the env\n\nInside the stat303_conda_env, install the following packages using conda:\n\nnumpy\npandas\nmatplotlib\n\nInstall the following packages using pip\n\nscikit-learn\nstatsmodels\n\n\nStep 4: Export the Environment Configuration\n\nExport the configuration of your stat303_conda_env environment to a file named stat303_env.yml: conda env export --name stat303_conda_env &gt; stat303_env.yml\n\nStep 5: Deactivate and Remove the Environment\n\nDeactivate the stat303_conda_env environment\nRemove the stat303_conda_env environment to ensure you understand how to clean up.\n\nStep 6: Recreate the Environment Using the Exported YML File\n\nCreate a new environment using the stat303_env.yml file.\nActivate the stat303_conda_env environment.\nVerify that the packages numpy, pandas, matplotlib, scikit-learn, and statsmodels are installed.\n\nStep 7: Run a Jupyter Notebook\n\nCreate a new notebook and write a simple Python script to\n\nImport numpy, pandas, matplotlib, and scikit-learn.\nPrint the versions of these packages.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your environment with VS Code</span>"
    ]
  },
  {
    "objectID": "venv_setup.html#reference",
    "href": "venv_setup.html#reference",
    "title": "1  Setting up your environment with VS Code",
    "section": "1.7 Reference",
    "text": "1.7 Reference\n\nGetting Started with VS Code\nJupyter Notebooks in VS Code\nInstall Python Packages\nManaging environments with conda",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your environment with VS Code</span>"
    ]
  },
  {
    "objectID": "workflow_enhance.html",
    "href": "workflow_enhance.html",
    "title": "2  Enhancing Workflow in Jupyter Notebooks",
    "section": "",
    "text": "2.1 Learning Objectives\nIn this lecture, we’ll explore how to optimize your workflow in Jupyter notebooks by leveraging magic commands and shell commands, understanding file paths, and interacting with the filesystem using the os module.\nBy completing this lecture, you will be able to:\nBy mastering these techniques, you’ll be able to work more efficiently and handle complex data science tasks with ease.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Enhancing Workflow in Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "workflow_enhance.html#learning-objectives",
    "href": "workflow_enhance.html#learning-objectives",
    "title": "2  Enhancing Workflow in Jupyter Notebooks",
    "section": "",
    "text": "Run shell commands directly within a notebook.\n\nNavigate and manipulate files and directories efficiently.\n\nIntegrate external tools and scripts seamlessly into your workflow.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Enhancing Workflow in Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "workflow_enhance.html#magic-commands",
    "href": "workflow_enhance.html#magic-commands",
    "title": "2  Enhancing Workflow in Jupyter Notebooks",
    "section": "2.2 Magic Commands",
    "text": "2.2 Magic Commands\n\n2.2.1 What are Magic Commands?\nMagic commands in Jupyter are shortcuts that extend the functionality of the notebook environment. There are two types of magic commands: - Line magics: Commands that operate on a single line. - Cell magics: Commands that operate on the entire cell.\nIn Jupyter notebooks, line magic commands are invoked by placing a single percentage sign (%) in front of the statement, allowing for quick, inline operations, while cell magic commands are denoted with double percentage signs (%%) at the beginning of the cell, enabling you to apply commands to the entire cell for more complex tasks.\nYou can access the full list of magic commands by typing:\n\n\nCode\n# show all the avaiable magic commands on the system\n%lsmagic  \n\n\nAvailable line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cd  %clear  %cls  %code_wrap  %colors  %conda  %config  %connect_info  %copy  %ddir  %debug  %dhist  %dirs  %doctest_mode  %echo  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %macro  %magic  %mamba  %matplotlib  %micromamba  %mkdir  %more  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %ren  %rep  %rerun  %reset  %reset_selective  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %uv  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%cmd  %%code_wrap  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n\n\n\n\n2.2.2 Line Magic Commands\n\n2.2.2.1 %time: Timing the execution of code\nIn data science, it is often crucial to evaluate the performance of specific code snippets or algorithms, and the %time magic command provides a simple and efficient way to measure the execution time of individual statements, helping you identify bottlenecks and optimize your code for better performance.\n\n\nCode\ndef my_dot(a, b): \n    \"\"\"\n   Compute the dot product of two vectors\n \n    Args:\n      a (ndarray (n,)):  input vector \n      b (ndarray (n,)):  input vector with same dimension as a\n    \n    Returns:\n      x (scalar): \n    \"\"\"\n    x=0\n    for i in range(a.shape[0]):\n        x = x + a[i] * b[i]\n    return x\n\n\n\n\nCode\nimport numpy as np\nnp.random.seed(1)\na = np.random.rand(10000000)  # very large arrays\nb = np.random.rand(10000000)\n\n\nLet’s use %time to measure the execution time of a single line of code.\n\n\nCode\n# Example: Timing a list comprehension\n%time np.dot(a, b)\n\n\nCPU times: total: 15.6 ms\nWall time: 32.9 ms\n\n\n2501072.5816813153\n\n\n\n\nCode\n%time my_dot(a, b)\n\n\nCPU times: total: 1.88 s\nWall time: 1.86 s\n\n\n2501072.5816813707\n\n\nTo capture the output of %time (or %timeit), you cannot directly assign it to a variable as it’s a magic command that prints the result to the notebook’s output. However, you can use Python’s built-in time module to manually time your code and assign the execution time to a variable.\nHere’s how you can do it using the time module:\n\n\nCode\nimport time\ntic = time.time()  # capture start time\nc = np.dot(a, b)\ntoc = time.time()  # capture end time\n\nprint(f\"np.dot(a, b) =  {c:.4f}\")\nprint(f\"Vectorized version duration: {1000*(toc-tic):.4f} ms \")\n\ntic = time.time()  # capture start time\nc = my_dot(a,b)\ntoc = time.time()  # capture end time\n\nprint(f\"my_dot(a, b) =  {c:.4f}\")\nprint(f\"loop version duration: {1000*(toc-tic):.4f} ms \")\n\ndel(a);del(b)  #remove these big arrays from memory\n\n\nnp.dot(a, b) =  2501072.5817\nVectorized version duration: 2.9922 ms \nmy_dot(a, b) =  2501072.5817\nloop version duration: 2004.3225 ms \n\n\n\n\n2.2.2.2 %who and %whos: Listing variables\n\n%who: Lists all variables in the current namespace.\n%whos: Lists variables along with their types, sizes, and values\n\n\n\nCode\n# Example: Checking variables in memory\na = 10\nb = \"data science\"\n%who\n%whos \n\n\nImage    a   b   c   file_path   my_dot  np  tic     time    \ntoc  \nVariable    Type        Data/Info\n---------------------------------\nImage       type        &lt;class 'IPython.core.display.Image'&gt;\na           int         10\nb           str         data science\nc           float64     2501072.5816813707\nfile_path   str         C:\\Users\\Username\\Documents\\data.csv\nmy_dot      function    &lt;function my_dot at 0x0000021701127C40&gt;\nnp          module      &lt;module 'numpy' from 'c:\\&lt;...&gt;ges\\\\numpy\\\\__init__.py'&gt;\ntic         float       1741794336.819888\ntime        module      &lt;module 'time' (built-in)&gt;\ntoc         float       1741794338.8242106\n\n\nBoth %who and %whos will list all variables, including those defined in the notebook’s code cells. If you want to list only specific types of variables (like only lists or integers), you can use:\n\n\nCode\n%who int\n%whos int\n\n\na    \nVariable   Type    Data/Info\n----------------------------\na          int     10\n\n\n\n\nCode\n%who str\n%whos str\n\n\nb    file_path   \nVariable    Type    Data/Info\n-----------------------------\nb           str     data science\nfile_path   str     C:\\Users\\Username\\Documents\\data.csv\n\n\n\n\n2.2.2.3 %matplotlib inline: Displaying plots inline\nThis command allows you to embed plots within the notebook.\n\n\nCode\n# Example: Using %matplotlib inline to display a plot\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nplt.plot(x, y);\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 Cell Magic Commands\nA cell magic command in Jupyter notebook has to be the first line in the code cell\n\n2.2.3.1 %%time: Timing cell execution\nThis cell magic is useful for measuring the execution time of an entire cell.\n\n\nCode\n%%time\n# Example: Timing a cell with matrix multiplication\n\nA = np.random.rand(1000, 1000)\nB = np.random.rand(1000, 1000)\nC = np.dot(A, B)\n\n\nCPU times: total: 219 ms\nWall time: 23 ms\n\n\n\n\n2.2.3.2 %%writefile: Writing content to a file\nThis command writes the contents of the cell to a file. Note that, it has to be the first line in the code cell\n\n\nCode\n%%writefile sample.txt\n\n# This is an example of writing content to a file using %%writefile magic command.\n\n\nWriting sample.txt\n\n\nQuestion: there are several timing magic commands that can be confusing due to their similarities, they are %time,%timeit, %%time, and %timeit. Do your own research on the differences among them",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Enhancing Workflow in Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "workflow_enhance.html#shell-commands-in-jupyter-notebooks",
    "href": "workflow_enhance.html#shell-commands-in-jupyter-notebooks",
    "title": "2  Enhancing Workflow in Jupyter Notebooks",
    "section": "2.3 Shell Commands in Jupyter Notebooks",
    "text": "2.3 Shell Commands in Jupyter Notebooks\nWhat are Shell Commands?\nShell commands allow you to interact with the underlying operating system. You can execute them directly within Jupyter by prefixing the command with an exclamation mark (!). In a Jupyter notebook, while the IPython kernel executes the Python code, it delegates the shell commands to the operating system’s shell. They are not executed by the same engine. The IPython kernel is for Python code, while the shell commands are handled by underlying operating system shell (e.g., Bash on macOS/Linux, or Command Prompt/PowerShell on Windows).\n\n2.3.1 Using Shell Commands\n\n2.3.1.1 !pwd(!cd in windows): Print working directory\nYou can check current working directory with the !pwd(!cd in windows) command.\n\n\nCode\n!cd\n\n\nc:\\Users\\lsi8012\\Documents\\Courses\\FA24\\DataScience_Intro_python_fa24_Sec20_21\n\n\n\n\n2.3.1.2 !ls(!dir in windows): Listing files and directories\nList the contents of the current directory.\n\n\nCode\n!dir\n\n\n Volume in drive C is Windows\n Volume Serial Number is A80C-7DEC\n\n Directory of c:\\Users\\lsi8012\\Documents\\Courses\\FA24\\DataScience_Intro_python_fa24_Sec20_21\n\n03/12/2025  10:46 AM    &lt;DIR&gt;          .\n03/12/2025  10:46 AM    &lt;DIR&gt;          ..\n10/20/2024  04:13 PM    &lt;DIR&gt;          .github\n10/20/2024  04:13 PM                11 .gitignore\n10/20/2024  04:13 PM    &lt;DIR&gt;          .ipynb_checkpoints\n10/20/2024  04:13 PM                 8 .nojekyll\n10/20/2024  04:24 PM    &lt;DIR&gt;          .quarto\n03/12/2025  10:29 AM         6,933,867 Advanced_Data_Visualization.html\n11/02/2024  03:12 PM         8,688,961 Advanced_Data_Visualization.ipynb\n03/12/2025  10:31 AM    &lt;DIR&gt;          Advanced_Data_Visualization_files\n10/20/2024  04:13 PM             9,325 Assignment 1 (Reading data).ipynb\n10/20/2024  04:13 PM            14,013 Assignment 2 (NumPy).ipynb\n10/20/2024  04:13 PM            18,098 Assignment 3 (Pandas).ipynb\n10/20/2024  04:13 PM            22,821 Assignment 4 (Data Visualization).ipynb\n10/20/2024  04:13 PM            96,997 Assignment 5 (Data cleaning and preparation).ipynb\n10/20/2024  04:13 PM            17,038 Assignment 6 (Data wrangling).ipynb\n10/20/2024  04:13 PM            14,220 Assignment 7 (Data aggregation).ipynb\n03/12/2025  10:29 AM           137,354 Assignment A.html\n10/20/2024  04:19 PM            43,769 Assignment A.ipynb\n03/12/2025  10:29 AM            60,301 Assignment B.html\n10/20/2024  04:19 PM            14,820 Assignment B.ipynb\n03/12/2025  10:29 AM            60,966 Assignment C.html\n10/20/2024  04:19 PM            18,169 Assignment C.ipynb\n03/12/2025  10:29 AM         2,748,253 Assignment D.html\n10/27/2024  11:43 AM            22,209 Assignment D.ipynb\n03/12/2025  10:29 AM            68,850 Assignment E.html\n11/19/2024  02:00 PM            19,393 Assignment E.ipynb\n03/12/2025  10:29 AM            53,070 Assignment F.html\n11/19/2024  02:08 PM            13,742 Assignment F.ipynb\n10/20/2024  04:13 PM            14,782 Assignment G.ipynb\n10/20/2024  04:13 PM            40,794 authors.jpg\n10/20/2024  04:13 PM             7,562 chck.csv\n10/20/2024  04:13 PM             4,299 co.csv\n10/20/2024  04:13 PM             5,647 coords.csv\n10/20/2024  04:13 PM            51,194 cover.png\n10/20/2024  04:13 PM           651,392 Data aggregation inclass.ipynb\n03/12/2025  10:29 AM           225,805 Data aggregation.html\n12/14/2024  02:41 PM           300,468 Data aggregation.ipynb\n03/12/2025  10:31 AM    &lt;DIR&gt;          Data aggregation_files\n03/12/2025  10:29 AM           204,720 Data cleaning and preparation.html\n11/10/2024  11:35 AM         1,050,426 Data cleaning and preparation.ipynb\n03/12/2025  10:31 AM    &lt;DIR&gt;          Data cleaning and preparation_files\n03/12/2025  10:29 AM           172,605 Data visualization.html\n10/21/2024  09:53 AM         1,628,169 Data visualization.ipynb\n03/12/2025  10:31 AM    &lt;DIR&gt;          Data visualization_files\n11/10/2024  11:41 AM           367,702 Data wrangling copy.ipynb\n03/12/2025  10:29 AM           185,198 Data wrangling.html\n11/11/2024  09:53 AM           425,911 Data wrangling.ipynb\n03/12/2025  10:31 AM    &lt;DIR&gt;          Data wrangling_files\n11/25/2024  01:01 PM    &lt;DIR&gt;          Datasets\n03/12/2025  10:29 AM            31,229 Datasets.html\n10/20/2024  04:13 PM             1,156 Datasets.ipynb\n10/20/2024  04:13 PM            37,147 datatypes.png\n10/20/2024  04:13 PM           163,888 data_structures-Copy1.ipynb\n03/12/2025  10:28 AM           288,147 data_structures.html\n10/20/2024  04:13 PM           178,946 data_structures.ipynb\n03/12/2025  10:29 AM           120,990 data_types_in_pandas.html\n12/01/2024  11:00 PM           103,025 data_types_in_pandas.ipynb\n03/12/2025  10:31 AM    &lt;DIR&gt;          data_types_in_pandas_files\n10/20/2024  04:13 PM             5,834 dpop.csv\n10/20/2024  04:13 PM           705,800 ds_image.jpg\n11/22/2024  03:25 PM    &lt;DIR&gt;          example_directory\n10/20/2024  04:13 PM    &lt;DIR&gt;          hello_files\n11/25/2024  12:27 PM    &lt;DIR&gt;          images\n10/20/2024  04:13 PM           457,384 inClass_Data wrangling.ipynb\n03/12/2025  10:31 AM            24,666 index.aux\n03/12/2025  10:28 AM            32,316 index.html\n03/12/2025  10:31 AM            70,189 index.log\n03/12/2025  10:31 AM         1,041,600 index.pdf\n03/11/2025  05:28 PM             1,251 index.qmd\n03/12/2025  10:31 AM         1,462,894 index.tex\n03/12/2025  10:31 AM                 0 index.toc\n10/20/2024  04:13 PM                10 intro.qmd\n03/12/2025  10:28 AM           144,381 Introduction to Python and Jupyter Notebooks.html\n10/20/2024  04:13 PM            64,982 Introduction to Python and Jupyter Notebooks.ipynb\n10/20/2024  04:13 PM           361,557 Introduction-to-Data-Science-with-Python.pdf\n10/20/2024  04:13 PM             1,115 In_class_exercise1.ipynb\n10/20/2024  04:13 PM    &lt;DIR&gt;          In_class_exercise1_files\n10/20/2024  04:13 PM            20,566 LICENSE.txt\n10/20/2024  04:13 PM             1,086 movies_sample_data.csv\n10/20/2024  04:13 PM                 0 movie_data.json\n10/20/2024  04:13 PM           242,761 movie_ratings.csv\n10/20/2024  04:13 PM           109,910 NumPy copy.ipynb\n03/12/2025  10:29 AM           241,607 NumPy.html\n11/24/2024  10:04 AM           113,396 NumPy.ipynb\n10/20/2024  04:13 PM            23,179 NU_Stat_logo.png\n03/12/2025  10:29 AM           269,968 Pandas.html\n10/20/2024  04:13 PM           284,987 Pandas.ipynb\n10/20/2024  04:13 PM        32,813,480 party_edited.csv\n10/20/2024  04:13 PM             1,361 quarto_yml_file_Fall_2022.txt\n10/20/2024  04:13 PM             6,496 questions_test - Copy.json\n03/12/2025  10:29 AM           261,769 Reading data.html\n11/22/2024  08:49 PM           279,118 Reading data.ipynb\n10/20/2024  04:13 PM           243,254 Reading_data.html\n10/20/2024  04:13 PM    &lt;DIR&gt;          Reading_Data_files\n10/20/2024  04:13 PM               189 README.md\n10/20/2024  04:13 PM               428 references.bib\n10/20/2024  04:13 PM                48 references.qmd\n10/20/2024  04:13 PM               811 requirements.txt\n10/20/2024  04:13 PM                 0 requirements.txt.txt\n10/20/2024  04:13 PM           112,248 rough.ipynb\n03/12/2025  10:46 AM                86 sample.txt\n03/12/2025  10:34 AM         1,019,165 search.json\n10/20/2024  04:13 PM    &lt;DIR&gt;          site_libs\n10/20/2024  04:13 PM        34,315,856 spotify_data.csv\n10/20/2024  04:13 PM        23,534,086 spotify_text.txt\n10/20/2024  04:13 PM           149,442 student_solutions-Copy1.ipynb\n10/20/2024  04:13 PM           286,932 student_solutions.ipynb\n10/20/2024  04:13 PM    &lt;DIR&gt;          student_solutions_files\n10/20/2024  04:13 PM                63 summary.qmd\n10/20/2024  04:13 PM             1,205 test_file.ipynb\n10/20/2024  04:13 PM             5,216 test_file.pdf\n10/20/2024  04:13 PM    &lt;DIR&gt;          test_file_files\n11/22/2024  03:28 PM    &lt;DIR&gt;          test_folder\n10/20/2024  04:13 PM             1,116 text.txt\n10/20/2024  04:13 PM            66,988 Untitled.ipynb\n10/20/2024  04:13 PM            19,849 Untitled1.ipynb\n10/20/2024  04:13 PM             3,942 Untitled2.ipynb\n10/20/2024  04:13 PM             6,085 Untitled3.ipynb\n10/20/2024  04:13 PM            18,861 Untitled4.ipynb\n03/12/2025  10:28 AM            87,254 venv_setup.html\n11/22/2024  09:12 PM            37,627 venv_setup.ipynb\n10/20/2024  04:13 PM             1,864 welcome.ipynb\n10/20/2024  04:13 PM            65,157 wordle.txt\n03/12/2025  10:34 AM           104,246 workflow_enhance.html\n03/12/2025  10:47 AM            74,123 workflow_enhance.ipynb\n03/12/2025  10:34 AM    &lt;DIR&gt;          workflow_enhance_files\n03/11/2025  05:31 PM             1,368 _quarto.yml\n             106 File(s)    124,538,629 bytes\n              22 Dir(s)  49,998,643,200 bytes free\n\n\n\n\n2.3.1.3 !mkdir: Creating a new directory\nYou can create a new directory using the !mkdir command.\n\n\nCode\n!mkdir new_folder\n\n\n\n\n2.3.1.4 !cat(type in windows): Displaying file content\nUse !cat(type in windows) to display the contents of a file.\n\n\nCode\n!type sample.txt\n\n\n\n# This is an example of writing content to a file using %%writefile magic command.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Enhancing Workflow in Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "workflow_enhance.html#installing-required-packages-within-your-notebook",
    "href": "workflow_enhance.html#installing-required-packages-within-your-notebook",
    "title": "2  Enhancing Workflow in Jupyter Notebooks",
    "section": "2.4 Installing required packages within your notebook",
    "text": "2.4 Installing required packages within your notebook\nWhen installing packages from within a Jupyter notebook, you have two main options: using a shell command or a magic command\n\n!pip install package_name\n%pip install package_name\n\nTo ensure the installation occurs in the correct environment (i.e., the environment in which the notebook kernel is running), you can use the Python executable associated with the current notebook.\n\n\nCode\nimport sys\n!{sys.executable} -m pip install numpy\n\n\nRequirement already satisfied: numpy in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (1.26.4)\n\n\nIn contrast, magic Command (%pip) is Jupyter-specific and automatically ensures that packages are installed in the correct environment (the notebook’s environment).\n\n\nCode\n%pip install numpy\n\n\nRequirement already satisfied: numpy in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (1.26.4)\nNote: you may need to restart the kernel to use updated packages.\n\n\nIn real cases, we don’t need to use % before pip install numpy because automagic is enabled by default in IPython kernels. This allows magic commands to be run without the % prefix, as long as there is no variable named pip in the current scope, which would take precedence over the magic command. Automagic allows some line magic commands to be run without the % prefix, but not all magic commands.\n\n\nCode\npip install numpy\n\n\nRequirement already satisfied: numpy in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (1.26.4)\nNote: you may need to restart the kernel to use updated packages.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Enhancing Workflow in Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "workflow_enhance.html#specifying-your-file-path",
    "href": "workflow_enhance.html#specifying-your-file-path",
    "title": "2  Enhancing Workflow in Jupyter Notebooks",
    "section": "2.5 Specifying Your File Path",
    "text": "2.5 Specifying Your File Path\nNext, we will discuss how to specify file paths in Python for loading and saving data, with an emphasis on absolute and relative paths. File paths are crucial when working with files in Python, such as reading datasets and writing results to files. Let’s explore the difference between absolute paths and relative paths.\n\n2.5.1 Absolute Path\nAn absolute path provides the complete location of a file or directory from the root directory of your file system. It is independent of the current working directory.\nExample of an Absolute Path (Windows):\n\n\nCode\n# Absolute path example (Windows)\nfile_path = r\"C:\\Users\\Username\\Documents\\data.csv\"\n\n\n\n\nCode\n!conda env list\n\n\n# conda environments:\n#\nbase                  *  C:\\Users\\lsi8012\\AppData\\Local\\anaconda3\n\n\n\nThe path associated with each conda env is absolute path.\n\n\n2.5.2 Relative Path\nA relative path specifies the file location in relation to the current working directory. It is shorter and more flexible since it doesn’t require the full path from the root.\nTo find the current working directory in Python, you can use either a magic command or a shell command, as shown below:\n\n\nCode\n# Magic command\n%pwd\n\n\n'c:\\\\Users\\\\lsi8012\\\\Documents\\\\Courses\\\\FA24\\\\DataScience_Intro_python_fa24_Sec20_21'\n\n\n\n\nCode\n# Shell command\n!cd\n\n\nc:\\Users\\lsi8012\\Documents\\Courses\\FA24\\DataScience_Intro_python_fa24_Sec20_21\n\n\nExample of a Relative Path:\n\n\nCode\n# Example of relative path\nfile_path = \"sample.txt\"  # Relative to the current directory\n\n\nThe relative path sample.txt means that there is a file in the current working directory.\n\n\n2.5.3 Methods to Specify file path in Windows\nWindows uses backslashes (\\) to separate directories in file paths. However, in Python, the backslash is an escape character, so you need to either: * Escape backslashes by using a double backslash (\\\\), or * Use raw strings by prefixing the path with r.\n\n2.5.3.1 Method 1: Using escaped backslashes\n\n\nCode\nfile_path = \"C:\\\\Users\\\\Username\\\\Documents\\\\data.csv\"\n\n\n\n\n2.5.3.2 Method 2: Using Raw Strings\n\n\nCode\nfile_path = r\"C:\\Users\\Username\\Documents\\data.csv\"\n\n\n\n\n2.5.3.3 Method 3: Using forward slashes (/)\n\n\nCode\nfile_path = \"C:/Users/Username/Documents/data.csv\"\n\n\n\n\n2.5.3.4 Method 4: Using os.path.join\n\n\nCode\nimport os\nfile_path = os.path.join(\"C:\", \"Users\", \"Username\", \"Documents\", \"data.csv\")\n\n\nThis method works across different operating systems because os.path.join automatically uses the correct path separator (\\ for Windows and /for Linux/Mac).\nmacOS does not have the same issue as Windows when specifying file paths because macOS (like Linux) uses forward slashes (/) as the path separator, which is consistent with Python’s expectations.\n\n\n2.5.3.5 Best Practices for File Paths in Data Science\n\nUse relative paths when working in a project structure, as it allows the project to be portable.\nUse absolute paths when working with external or shared files that aren’t part of your project.\nCheck the current working directory to ensure you are referencing files correctly.\nAvoid hardcoding file paths directly in the code to make your code reusable on different machines.\nUse the forward slash (/) as a path separator or os.path.join to specify file path if your code will work across different operating systems",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Enhancing Workflow in Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "workflow_enhance.html#interacting-with-the-os-and-filesystem",
    "href": "workflow_enhance.html#interacting-with-the-os-and-filesystem",
    "title": "2  Enhancing Workflow in Jupyter Notebooks",
    "section": "2.6 Interacting with the OS and filesystem",
    "text": "2.6 Interacting with the OS and filesystem\nIn data science, you frequently work with data files (e.g., CSVs, Excel, JSON) stored in different directories. The os module allow you to interact with the OS and the filesystem. Let’s import it and try out some commonly used functions in this module.\n\n\nCode\nimport os\n\n\nWe can get the location of the current working directory using the os.getcwd function.\n\n\nCode\nos.getcwd()\n\n\n'c:\\\\Users\\\\lsi8012\\\\Documents\\\\Courses\\\\FA24\\\\DataScience_Intro_python_fa24_Sec20_21'\n\n\nThe command os.chdir('..') in Python changes the current working directory to the parent directory of the current one.\nNote that .. as the path notation for the parent directory is universally true across all major operating systems, including Windows, macOS, and Linux. It allows you to move one level up in the directory hierarchy, which is very useful when navigating directories programmatically, especially in scripts where directory traversal is needed.\n\n\nCode\nos.chdir('..')\n\n\n\n\nCode\nos.getcwd()\n\n\n'c:\\\\Users\\\\lsi8012\\\\Documents\\\\Courses\\\\FA24'\n\n\nos.chdir() is used to change the current working directory.\nFor example: os.chdir('./week2')\n./week2 is a relative path:\n\n. refers to the current directory.\nweek2 is a folder inside the current directory.\n\nThe os.listdir() function in Python returns a list of all files and directories in the specified path. If no path is provided, it returns the contents of the current working directory.\n\n\nCode\nos.listdir()\n\n\n['.ipynb_checkpoints',\n '303-1',\n '362',\n 'DataScience_Intro_python_fa24_Sec20_21',\n 'EDA.pdf',\n 'test_folder',\n 'UG TA info for Instructors.pdf']\n\n\nCheck whether a specific folder/file exist in the current working directory\n\n\nCode\n'data' in os.listdir('.')\n\n\nFalse",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Enhancing Workflow in Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "workflow_enhance.html#independent-study",
    "href": "workflow_enhance.html#independent-study",
    "title": "2  Enhancing Workflow in Jupyter Notebooks",
    "section": "2.7 Independent Study",
    "text": "2.7 Independent Study\nTo reinforce the skills learned in this lecture, complete the following tasks:\n\nSet Up Your Workspace\n\nCreate a folder named STAT303-1 for putting the course materials.\nCreate a pip environment for the upcoming coursework.\n\nOrganize your files into separate directories for datasets, assignments, projects, quizzes, lectures, and exams.\n\nUse the os module or shell commands to create these directories programmatically in your Jupyter notebook.\n\nPractice Magic Commands\n\nUse %timeit to measure the execution time of a simple Python function in your notebook.\n\nExperiment with %lsmagic to explore all available magic commands.\n\nRun Shell Commands\n\nUse !ls (or !dir on Windows) to list the contents of the directories you created.\n\nUse !pwd to print the current working directory.\n\nExplore File Paths\n\nWrite a Python script to navigate between directories using relative and absolute paths.\nUse the os module to programmatically check the existence of a directory or file before creating it.\n\n\n\n\nCode\n# Navigate between directories and understand . and .. in paths\nimport os\n\n# Print the current directory\nprint(\"Current Directory:\", os.getcwd())  # Get the current working directory\n\n# Use '.' to refer to the current directory\ncurrent_dir = os.path.abspath(\".\")  # Absolute path of the current directory\nprint(\"Using '.': Current Directory Path:\", current_dir)\n\n# Use '..' to refer to the parent directory\nparent_dir = os.path.abspath(\"..\")  # Absolute path of the parent directory\nprint(\"Using '..': Parent Directory Path:\", parent_dir)\n\n# Create a new folder in the current directory and navigate into it\nos.rmdir(\"test_folder\")  # Remove the folder if it already exists\nos.mkdir(\"test_folder\")  # Create a new folder\nprint(\"Created 'test_folder' in:\", os.path.abspath(\".\"))\n\nos.chdir(\"test_folder\")  # Change to the new folder\nprint(\"After Changing Directory (Relative to '.'): \", os.getcwd())\n\n# Navigate back to the parent directory using '..'\nos.chdir(\"..\")\nprint(\"After Navigating Back to Parent Directory (Relative to '..'):\", os.getcwd())\n\n# Navigate to the parent directory directly using an absolute path\nos.chdir(parent_dir)\nprint(\"Using Absolute Path to Navigate to Parent Directory:\", os.getcwd())\n\n\n\nCurrent Directory: c:\\Users\\lsi8012\\Documents\\Courses\\FA24\nUsing '.': Current Directory Path: c:\\Users\\lsi8012\\Documents\\Courses\\FA24\nUsing '..': Parent Directory Path: c:\\Users\\lsi8012\\Documents\\Courses\nCreated 'test_folder' in: c:\\Users\\lsi8012\\Documents\\Courses\\FA24\nAfter Changing Directory (Relative to '.'):  c:\\Users\\lsi8012\\Documents\\Courses\\FA24\\test_folder\nAfter Navigating Back to Parent Directory (Relative to '..'): c:\\Users\\lsi8012\\Documents\\Courses\\FA24\nUsing Absolute Path to Navigate to Parent Directory: c:\\Users\\lsi8012\\Documents\\Courses\n\n\n\n\nCode\n# check the existence of a directory or file before creating it\n\n# Define directory and file paths\ndir_path = \"example_directory\"\nfile_path = os.path.join(dir_path, \"example_file.txt\")\n\n# Check and create directory\nif not os.path.exists(dir_path):\n    os.makedirs(dir_path)\n    print(f\"Directory '{dir_path}' created.\")\nelse:\n    print(f\"Directory '{dir_path}' already exists.\")\n\n# Check and create file\nif not os.path.exists(file_path):\n    with open(file_path, \"w\") as f:\n        f.write(\"This is a test file.\")  # Write some content to the file\n    print(f\"File '{file_path}' created.\")\nelse:\n    print(f\"File '{file_path}' already exists.\")\n\n\nDirectory 'example_directory' created.\nFile 'example_directory\\example_file.txt' created.\n\n\nBy completing these exercises, you’ll gain practical experience with magic commands, shell commands, file paths, and the os module, enhancing your ability to interact with the filesystem in Jupyter notebooks effectively.",
    "crumbs": [
      "Getting started: Coding environment",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Enhancing Workflow in Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html",
    "href": "Introduction to Python and Jupyter Notebooks.html",
    "title": "3  Python Basics",
    "section": "",
    "text": "3.1 Python language basics\nThis chapter is a very brief introduction to python. If you have not taken STAT201 (Introduction to programming for data science), which is now a pre-requisite for the data science major / minor program, please review the python programming section (chapters 1-6) from the STAT201 book. It is assumed that you are already comfortable with this content. Some of the content of these chapters is reviewed briefly in the first two chapters of this book.",
    "crumbs": [
      "Prerequisite: Python programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "href": "Introduction to Python and Jupyter Notebooks.html#python-language-basics",
    "title": "3  Python Basics",
    "section": "",
    "text": "3.1.1 Object Oriented Programming\nPython is an object-oriented programming language. In layman terms, it means that every number, string, data structure, function, class, module, etc., exists in the python interpreter as a python object. An object may have attributes and methods associated with it. For example, let us define a variable that stores an integer:\n\nvar = 2\n\nThe variable var is an object that has attributes and methods associated with it. For example a couple of its attributes are real and imag, which store the real and imaginary parts respectively, of the object var:\n\nprint(\"Real part of 'var': \",var.real)\nprint(\"Real part of 'var': \",var.imag)\n\nReal part of 'var':  2\nReal part of 'var':  0\n\n\nAttribute: An attribute is a value associated with an object, defined within the class of the object.\nMethod: A method is a function associated with an object, defined within the class of the object, and has access to the attributes associated with the object.\nFor looking at attributes and methods associated with an object, say obj, press tab key after typing obj..\nConsider the example below of a class example_class:\n\nclass example_class:\n    class_name = 'My Class'\n    def my_method(self):\n        print('Hello World!')\n\ne = example_class()\n\nIn the above class, class_name is an attribute, while my_method is a method.\n\n\n3.1.2 Assigning variable name to object\n\n3.1.2.1 Call by reference\nPython utilizes a system, which is known as Call by Object Reference. When an object is assigned to a variable name, the variable name serves as a reference to the object. For example, consider the following assignment:\n\nx = [5,3]\n\nThe variable name x is a reference to the memory location where the object [5, 3] is stored. Now, suppose we assign x to a new variable y:\n\ny = x\n\nIn the above statement the variable name y now refers to the same object [5,3]. The object [5,3] does not get copied to a new memory location referred by y. To prove this, let us add an element to y:\n\ny.append(4)\nprint(y)\n\n[5, 3, 4]\n\n\n\nprint(x)\n\n[5, 3, 4]\n\n\nWhen we changed y, note that x also changed to the same object, showing that x and y refer to the same object, instead of referring to different copies of the same object.\n\n\n3.1.2.2 Assigning multiple variable names\nValues can be assigned to multiple variables in a single statement by separating the variable names and values with commas.\n\ncolor1, color2, color3 = \"red\", \"green\", \"blue\"\n\n\ncolor1\n\n'red'\n\n\n\ncolor3\n\n'blue'\n\n\nThe same value can be assigned to multiple variables by chaining multiple assignment operations within a single statement.\n\ncolor4 = color5 = color6 = \"magenta\"\n\n\n\n3.1.2.3 Rules for variable names\nVariable names can be short (a, x, y, etc.) or descriptive ( my_favorite_color, profit_margin, the_3_musketeers, etc.). However, we recommend that you use descriptive variable names as it makes it easier to understand the code.\nThe rules below must be followed while naming Python variables:\n\nA variable’s name must start with a letter or the underscore character _. It cannot begin with a number.\nA variable name can only contain lowercase (small) or uppercase (capital) letters, digits, or underscores (a-z, A-Z, 0-9, and _).\nVariable names are case-sensitive, i.e., a_variable, A_Variable, and A_VARIABLE are all different variables.\n\nHere are some valid variable names:\n\na_variable = 23\nis_today_Saturday = False\nmy_favorite_car = \"Delorean\"\nthe_3_musketeers = [\"Athos\", \"Porthos\", \"Aramis\"] \n\nLet’s try creating some variables with invalid names. Python prints a syntax error if the variable’s name is invalid.\n\nSyntax: The syntax of a programming language refers to the rules that govern the structure of a valid instruction or statement. If a statement does not follow these rules, Python stops execution and informs you that there is a syntax error. Syntax can be thought of as the rules of grammar for a programming language.\n\n\na variable = 23\n\n\nis_today_$aturday = False\n\n\nmy-favorite-car = \"Delorean\"\n\n\n3_musketeers = [\"Athos\", \"Porthos\", \"Aramis\"]\n\n\n\n\n3.1.3 Built-in objects\n\n3.1.3.1 Built-in data types\nVariable is created as soon as a value is assigned to it. We don’t have to define the type of variable explicitly as in other programming languages because Python can automatically guess the type of data entered (dynamically typed).\nAny data or information stored within a Python variable has a type. We can view the type of data stored within a variable using the type function.\n\na_variable\n\n23\n\n\n\ntype(a_variable)\n\nint\n\n\n\nis_today_Saturday\n\nFalse\n\n\n\ntype(is_today_Saturday)\n\nbool\n\n\n\nmy_favorite_car\n\n'Delorean'\n\n\n\ntype(my_favorite_car)\n\nstr\n\n\n\nthe_3_musketeers\n\n['Athos', 'Porthos', 'Aramis']\n\n\n\ntype(the_3_musketeers)\n\nlist\n\n\nPython has several built-in data types for storing different kinds of information in variables.\n\n\n\n\n\nPrimitive: Integer, float, boolean, None, and string are primitive data types because they represent a single value.\nContainers: Other data types like list, tuple, and dictionary are often called data structures or containers because they hold multiple pieces of data together. We’ll discuss these datatypes in chapter 2.\nThe data type of the object can be identified using the in-built python function type(). For example, see the following objects and their types:\n\ntype(4)\n\nint\n\n\n\ntype(4.4)\n\nfloat\n\n\n\ntype('4')\n\nstr\n\n\n\ntype(True)\n\nbool\n\n\n\n\n3.1.3.2 Built-in modules and functions\nBuilt-in functions in Python are a set of predefined functions that are available for use without the need to import any additional libraries or modules. The Python Standard Library is very extensive. Besides built-in functions, it also contains many Python scripts (with the . py extension) containing useful utilities and modules written in Python that provide standardized solutions for many problems that occur in everyday programming.\nBelow are a couple of examples:\nrange(): The range() function returns a sequence of evenly-spaced integer values. It is commonly used in for loops to define the sequence of elements over which the iterations are performed.\nBelow is an example where the range() function is used to create a sequence of whole numbers upto 10:\n\nprint(list(range(1,10)))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nThe advantage of the range type over a regular list or tuple is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step values, calculating individual items and subranges as needed).\nDate time: Python as a built-in datetime module for handling date/time objects:\n\nimport datetime as dt\n\n\n#Defining a date-time object \ndt_object = dt.datetime(2022, 9, 20, 11,30,0)\n\nInformation about date and time can be accessed with the relevant attribute of the datetime object.\n\ndt_object.day\n\n20\n\n\n\ndt_object.year\n\n2022\n\n\nThe strftime method of the datetime module formats a datetime object as a string. There are several types of formats for representing date as a string:\n\ndt_object.strftime('%m/%d/%Y')\n\n'09/20/2022'\n\n\n\ndt_object.strftime('%m/%d/%y %H:%M')\n\n'09/20/22 11:30'\n\n\n\ndt_object.strftime('%h-%d-%Y')\n\n'Sep-20-2022'\n\n\n\n\n\n3.1.4 Importing libraries\nThere are several built-in functions in python like print(), abs(), max(), sum() etc., which do not require importing any library. However, these functions will typically be insufficient for a analyzing data. Some of the popular libraries and their primary purposes are as follows:\n\nNumPy: NumPy is a fundamental library for numerical computing in Python. It provides support for arrays, matrices, and mathematical functions, making it essential for scientific and data analysis tasks.. It is mostly used for performing numerical operations and efficiently storing numerical data.\nPandas: Pandas is a powerful data manipulation and analysis library. It offers data structures like DataFrames and Series, which facilitate data reading, cleaning, transformation, and analysis, making it indispensable in data science projects.\nMatplotlib, Seaborn: Matplotlib is a comprehensive library for creating static, animated, or interactive plots and visualizations. It is commonly used for data visualization and exploration in data science. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\nSciPy: SciPy is used for performing scientific computing such as solving differential equations, optimization, statistical tests, etc.\nScikit-learn: Scikit-learn is a machine learning library that provides a wide range of tools for data pre-procesing, classification, regression, clustering, dimensionality reduction, and more. It simplifies the implementation of machine learning algorithms and model evaluation.\nStatsmodels: Statsmodels is used for developing statistical models with a focus on inference (in contrast to focus on prediction as in scikit-learn).\n\nTo use libraries like NumPy, pandas, Matplotlib, and scikit-learn in Python, you typically need to follow these steps:\n\nInstall the libraries (Anaconda already does this)\nImport the libraries in the python script or jupyter notebook\nUse the Library Functions and Classes: After importing the libraries, you can use their functions, classes, and methods in your code. For instance, you can create NumPy arrays, manipulate data with pandas, create plots with Matplotlib, or train machine learning models with scikit-learn.\n\nA library can be imported using the import keyword. For example, a NumPy library can be imported as:\n\nimport numpy as np\n\nUsing the as keyboard, the NumPy library has been given the name np. All the functions and attributes of the library can be called using the ‘np.’ prefix. For example, let us generate a sequence of whole numbers upto 10 using the NumPy function arange():\n\nnp.arange(8)\n\narray([0, 1, 2, 3, 4, 5, 6, 7])\n\n\nThere’s different ways to import:\n\nImport the whole module using its original name:  import math, os\nImport specific things from the module:  from random import randint  from math import pi\nImport the whole library and rename it, usually using a shorter variable name:  import pandas as pd\nImport a specific method from the module and rename it as it is imported:  from os.path import join as join_path\n\n\n\n3.1.5 User-defined functions\nA function is a reusable set of instructions that takes one or more inputs, performs some operations, and often returns an output. Indeed, while python’s standard library and ecosystem libraries offer a wealth of pre-defined functions for a wide range of tasks, there are situations where defining your own functions is not just beneficial but necessary. \n\n3.1.5.1 Creating and using functions\nYou can define a new function using the def keyword.\n\ndef say_hello():\n    print('Hello there!')\n    print('How are you?')\n\nNote the round brackets or parentheses () and colon : after the function’s name. Both are essential parts of the syntax. The function’s body contains an indented block of statements. The statements inside a function’s body are not executed when the function is defined. To execute the statements, we need to call or invoke the function.\n\nsay_hello()\n\nHello there!\nHow are you?\n\n\n\ndef say_hello_to(name):\n    print('Hello ', name)\n    print('How are you?')\n\n\nsay_hello_to('Lizhen')\n\nHello  Lizhen\nHow are you?\n\n\n\nname = input ('Please enter your name: ')\nsay_hello_to(name)\n\nPlease enter your name: George\nHello  George\nHow are you?\n\n\n\n\n3.1.5.2 Variable scope: Local and global Variables\nLocal variable: When we declare variables inside a function, these variables will have a local scope (within the function). We cannot access them outside the function. These types of variables are called local variables. For example,\n\ndef greet(): \n    message = 'Hello'  # local variable\n    print('Local', message)\ngreet()\n\nLocal Hello\n\n\n\n# print(message) # try to access message variable outside greet() function, uncomment this line to see the error\n\nAs message was defined within the function greet(), it is local to the function, and cannot be called outside the function.\nGlobal variable: Aa variable declared outside of the function or in global scope is known as a global variable. This means that a global variable can be accessed inside or outside of the function.\nLet’s see an example of how a global variable is created.\n\nmessage = 'Hello'  # declare global variable\n\ndef greet():\n    print('Local', message)  # declare local variable\n\ngreet()\nprint('Global', message)\n\nLocal Hello\nGlobal Hello\n\n\n\n\n3.1.5.3 Named arguments\nInvoking a function with many arguments can often get confusing and is prone to human errors. Python provides the option of invoking functions with named arguments for better clarity. You can also split function invocation into multiple lines.\n\ndef loan_emi(amount, duration, rate, down_payment=0):\n    loan_amount = amount - down_payment\n    emi = loan_amount * rate * ((1+rate)**duration) / (((1+rate)**duration)-1)\n    return emi\n\n\nemi1 = loan_emi(\n    amount=1260000, \n    duration=8*12, \n    rate=0.1/12, \n    down_payment=3e5\n)\n\n\nemi1\n\n14567.19753389219\n\n\n\n\n3.1.5.4 Optional Arguments\nFunctions with optional arguments offer more flexibility in how you can use them. You can call the function with or without the argument, and if there is no argument in the function call, then a default value is used.\n\nemi2 = loan_emi(\n    amount=1260000, \n    duration=8*12, \n    rate=0.1/12)\n\nemi2\n\n19119.4467632335\n\n\n\n\n3.1.5.5 \\*args and \\**kwargs\nWe can pass a variable number of arguments to a function using special symbols. There are two special symbols:\n\nSpecial Symbols Used for passing arguments:\n\n\\*args (Non-Keyword Arguments)\n\\**kwargs (Keyword Arguments)\n\n\ndef myFun(*args,**kwargs):\n    print(\"args: \", args)\n    print(\"kwargs: \", kwargs)\n \n \n# Now we can use both *args ,**kwargs\n# to pass arguments to this function :\nmyFun('John',22,'cs',name=\"John\",age=22,major=\"cs\")\n\nargs:  ('John', 22, 'cs')\nkwargs:  {'name': 'John', 'age': 22, 'major': 'cs'}\n\n\n\n\n\n3.1.6 Branching and looping (control flow)\n\nAs in other languages, python has built-in keywords that provide conditional flow of control in the code.\n\n3.1.6.1 Branching with if, else and elif\nOne of the most powerful features of programming languages is branching: the ability to make decisions and execute a different set of statements based on whether one or more conditions are true.\nThe if statement\nIn Python, branching is implemented using the if statement, which is written as follows:\nif condition:\n    statement1\n    statement2\nThe condition can be a value, variable or expression. If the condition evaluates to True, then the statements within the if block are executed. Notice the four spaces before statement1, statement2, etc. The spaces inform Python that these statements are associated with the if statement above. This technique of structuring code by adding spaces is called indentation.\n\nIndentation: Python relies heavily on indentation (white space before a statement) to define code structure. This makes Python code easy to read and understand. You can run into problems if you don’t use indentation properly. Indent your code by placing the cursor at the start of the line and pressing the Tab key once to add 4 spaces. Pressing Tab again will indent the code further by 4 more spaces, and press Shift+Tab will reduce the indentation by 4 spaces.\n\nFor example, let’s write some code to check and print a message if a given number is even.\n\na_number = 34\n\n\nif a_number % 2 == 0:\n    print(\"We're inside an if block\")\n    print('The given number {} is even.'.format(a_number))\n\nWe're inside an if block\nThe given number 34 is even.\n\n\nThe else statement\nWe may want to print a different message if the number is not even in the above example. This can be done by adding the else statement. It is written as follows:\nif condition:\n    statement1\n    statement2\nelse:\n    statement4\n    statement5\n\nIf condition evaluates to True, the statements in the if block are executed. If it evaluates to False, the statements in the else block are executed.\n\nif a_number % 2 == 0:\n    print('The given number {} is even.'.format(a_number))\nelse:\n    print('The given number {} is odd.'.format(a_number))\n\nThe given number 34 is even.\n\n\nThe elif statement\nPython also provides an elif statement (short for “else if”) to chain a series of conditional blocks. The conditions are evaluated one by one. For the first condition that evaluates to True, the block of statements below it is executed. The remaining conditions and statements are not evaluated. So, in an if, elif, elif… chain, at most one block of statements is executed, the one corresponding to the first condition that evaluates to True.\n\ntoday = 'Wednesday'\n\n\nif today == 'Sunday':\n    print(\"Today is the day of the sun.\")\nelif today == 'Monday':\n    print(\"Today is the day of the moon.\")\nelif today == 'Tuesday':\n    print(\"Today is the day of Tyr, the god of war.\")\nelif today == 'Wednesday':\n    print(\"Today is the day of Odin, the supreme diety.\")\nelif today == 'Thursday':\n    print(\"Today is the day of Thor, the god of thunder.\")\nelif today == 'Friday':\n    print(\"Today is the day of Frigga, the goddess of beauty.\")\nelif today == 'Saturday':\n    print(\"Today is the day of Saturn, the god of fun and feasting.\")\n\nToday is the day of Odin, the supreme diety.\n\n\nIn the above example, the first 3 conditions evaluate to False, so none of the first 3 messages are printed. The fourth condition evaluates to True, so the corresponding message is printed. The remaining conditions are skipped. Try changing the value of today above and re-executing the cells to print all the different messages.\nUsing if, elif, and else together\nYou can also include an else statement at the end of a chain of if, elif… statements. This code within the else block is evaluated when none of the conditions hold true.\n\na_number = 49\n\n\nif a_number % 2 == 0:\n    print('{} is divisible by 2'.format(a_number))\nelif a_number % 3 == 0:\n    print('{} is divisible by 3'.format(a_number))\nelif a_number % 5 == 0:\n    print('{} is divisible by 5'.format(a_number))\nelse:\n    print('All checks failed!')\n    print('{} is not divisible by 2, 3 or 5'.format(a_number))\n\nAll checks failed!\n49 is not divisible by 2, 3 or 5\n\n\nNon-Boolean Conditions\nNote that conditions do not necessarily have to be booleans. In fact, a condition can be any value. The value is converted into a boolean automatically using the bool operator. Any value in Python can be converted to a Boolean using the bool function.\nOnly the following values evaluate to False (they are often called falsy values):\n\nThe value False itself\nThe integer 0\nThe float 0.0\nThe empty value None\nThe empty text \"\"\nThe empty list []\nThe empty tuple ()\nThe empty dictionary {}\nThe empty set set()\nThe empty range range(0)\n\nEverything else evaluates to True (a value that evaluates to True is often called a truthy value).\n\nif '':\n    print('The condition evaluted to True')\nelse:\n    print('The condition evaluted to False')\n\nThe condition evaluted to False\n\n\n\nif 'Hello':\n    print('The condition evaluted to True')\nelse:\n    print('The condition evaluted to False')\n\nThe condition evaluted to True\n\n\n\nif { 'a': 34 }:\n    print('The condition evaluted to True')\nelse:\n    print('The condition evaluted to False')\n\nThe condition evaluted to True\n\n\n\nif None:\n    print('The condition evaluted to True')\nelse:\n    print('The condition evaluted to False')\n\nThe condition evaluted to False\n\n\nNested conditional statements\nThe code inside an if block can also include an if statement inside it. This pattern is called nesting and is used to check for another condition after a particular condition holds true.\n\na_number = 15\n\n\nif a_number % 2 == 0:\n    print(\"{} is even\".format(a_number))\n    if a_number % 3 == 0:\n        print(\"{} is also divisible by 3\".format(a_number))\n    else:\n        print(\"{} is not divisibule by 3\".format(a_number))\nelse:\n    print(\"{} is odd\".format(a_number))\n    if a_number % 5 == 0:\n        print(\"{} is also divisible by 5\".format(a_number))\n    else:\n        print(\"{} is not divisibule by 5\".format(a_number))\n\n15 is odd\n15 is also divisible by 5\n\n\nNotice how the print statements are indented by 8 spaces to indicate that they are part of the inner if/else blocks.\n\nNested if, else statements are often confusing to read and prone to human error. It’s good to avoid nesting whenever possible, or limit the nesting to 1 or 2 levels.\n\nShorthand if conditional expression\nA frequent use case of the if statement involves testing a condition and setting a variable’s value based on the condition.\nPython provides a shorter syntax, which allows writing such conditions in a single line of code. It is known as a conditional expression, sometimes also referred to as a ternary operator. It has the following syntax:\nx = true_value if condition else false_value\nIt has the same behavior as the following if-else block:\nif condition:\n    x = true_value\nelse:\n    x = false_value\nLet’s try it out for the example above.\n\nparity = 'even' if a_number % 2 == 0 else 'odd'\n\n\nprint('The number {} is {}.'.format(a_number, parity))\n\nThe number 15 is odd.\n\n\nThe pass statement\nif statements cannot be empty, there must be at least one statement in every if and elif block. We can use the pass statement to do nothing and avoid getting an error.\n\na_number = 9\n\n\n# please uncomment the code below and see the error message\n# if a_number % 2 == 0:\n    \n# elif a_number % 3 == 0:\n#    print('{} is divisible by 3 but not divisible by 2')\n\nAs there must be at least one statement withihng the if block, the above code throws an error.\n\nif a_number % 2 == 0:\n    pass\nelif a_number % 3 == 0:\n    print('{} is divisible by 3 but not divisible by 2'.format(a_number))\n\n9 is divisible by 3 but not divisible by 2\n\n\n\n\n3.1.6.2 Iteration with while loops\nAnother powerful feature of programming languages, closely related to branching, is running one or more statements multiple times. This feature is often referred to as iteration on looping, and there are two ways to do this in Python: using while loops and for loops.\nwhile loops have the following syntax:\nwhile condition:\n    statement(s)\nStatements in the code block under while are executed repeatedly as long as the condition evaluates to True. Generally, one of the statements under while makes some change to a variable that causes the condition to evaluate to False after a certain number of iterations.\nLet’s try to calculate the factorial of 100 using a while loop. The factorial of a number n is the product (multiplication) of all the numbers from 1 to n, i.e., 1*2*3*...*(n-2)*(n-1)*n.\n\nresult = 1\ni = 1\n\nwhile i &lt;= 10:\n    result = result * i\n    i = i+1\n\nprint('The factorial of 100 is: {}'.format(result))\n\nThe factorial of 100 is: 3628800\n\n\n\n\n3.1.6.3 Infinite Loops\nSuppose the condition in a while loop always holds true. In that case, Python repeatedly executes the code within the loop forever, and the execution of the code never completes. This situation is called an infinite loop. It generally indicates that you’ve made a mistake in your code. For example, you may have provided the wrong condition or forgotten to update a variable within the loop, eventually falsifying the condition.\nIf your code is stuck in an infinite loop during execution, just press the “Stop” button on the toolbar (next to “Run”) or select “Kernel &gt; Interrupt” from the menu bar. This will interrupt the execution of the code. The following two cells both lead to infinite loops and need to be interrupted.\n\n# INFINITE LOOP - INTERRUPT THIS CELL\n\nresult = 1\ni = 1\n\nwhile i &lt;= 100:\n    result = result * i\n    # forgot to increment i\n\n\n# INFINITE LOOP - INTERRUPT THIS CELL\n\nresult = 1\ni = 1\n\nwhile i &gt; 0 : # wrong condition\n    result *= i\n    i += 1\n\n\n\n3.1.6.4 break and continue statements\nIn Python, break and continue statements can alter the flow of a normal loop. \nWe can use the break statement within the loop’s body to immediately stop the execution and break out of the loop. with the continue statement. If the condition evaluates to True, then the loop will move to the next iteration.\n\ni = 1\nresult = 1\n\nwhile i &lt;= 100:\n    result *= i\n    if i == 42:\n        print('Magic number 42 reached! Stopping execution..')\n        break\n    i += 1\n    \nprint('i:', i)\nprint('result:', result)\n\nMagic number 42 reached! Stopping execution..\ni: 42\nresult: 1405006117752879898543142606244511569936384000000000\n\n\n\ni = 1\nresult = 1\n\nwhile i &lt; 8:\n    i += 1\n    if i % 2 == 0:\n        print('Skipping {}'.format(i))\n        continue\n    print('Multiplying with {}'.format(i))\n    result = result * i\n    \nprint('i:', i)\nprint('result:', result)\n\nSkipping 2\nMultiplying with 3\nSkipping 4\nMultiplying with 5\nSkipping 6\nMultiplying with 7\nSkipping 8\ni: 8\nresult: 105\n\n\nIn the example above, the statement result = result * i inside the loop is skipped when i is even, as indicated by the messages printed during execution.\n\nLogging: The process of adding print statements at different points in the code (often within loops and conditional statements) for inspecting the values of variables at various stages of execution is called logging. As our programs get larger, they naturally become prone to human errors. Logging can help in verifying the program is working as expected. In many cases, print statements are added while writing & testing some code and are removed later.\n\nTask: Guess the output and explain it.\n\n# Use of break statement inside the loop\n\nfor val in \"string\":\n    if val == \"i\":\n        break\n    print(val)\n\nprint(\"The end\")\n\ns\nt\nr\nThe end\n\n\n\n# Program to show the use of continue statement inside loops\n\nfor val in \"string\":\n    if val == \"i\":\n        continue\n    print(val)\n\nprint(\"The end\")\n\ns\nt\nr\nn\ng\nThe end\n\n\n\n\n3.1.6.5 Iteration with for loops\nA for loop is used for iterating or looping over sequences, i.e., lists, tuples, dictionaries, strings, and ranges. For loops have the following syntax:\nfor value in sequence:\n    statement(s)\nThe statements within the loop are executed once for each element in sequence. Here’s an example that prints all the element of a list.\n\ndays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n\nfor day in days:\n    print(day)\n\nMonday\nTuesday\nWednesday\nThursday\nFriday\n\n\n\n# Looping over a string\nfor char in 'Monday':\n    print(char)\n\nM\no\nn\nd\na\ny\n\n\n\n# Looping over a dictionary\nperson = {\n    'name': 'John Doe',\n    'sex': 'Male',\n    'age': 32,\n    'married': True\n}\n\nfor key, value in person.items():\n    print(\"Key:\", key, \",\", \"Value:\", value)\n\nKey: name , Value: John Doe\nKey: sex , Value: Male\nKey: age , Value: 32\nKey: married , Value: True\n\n\n\n\n\n3.1.7 Iterating using range and enumerate\nThe range function is used to create a sequence of numbers that can be iterated over using a for loop. It can be used in 3 ways:\n\nrange(n) - Creates a sequence of numbers from 0 to n-1\nrange(a, b) - Creates a sequence of numbers from a to b-1\nrange(a, b, step) - Creates a sequence of numbers from a to b-1 with increments of step\n\nLet’s try it out.\n\nfor i in range(4):\n    print(i)\n\n0\n1\n2\n3\n\n\n\nfor i in range(3, 8):\n    print(i)\n\n3\n4\n5\n6\n7\n\n\n\nfor i in range(3, 14, 4):\n    print(i)\n\n3\n7\n11\n\n\nRanges are used for iterating over lists when you need to track the index of elements while iterating.\n\na_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n\nfor i in range(len(a_list)):\n    print('The value at position {} is {}.'.format(i, a_list[i]))\n\nThe value at position 0 is Monday.\nThe value at position 1 is Tuesday.\nThe value at position 2 is Wednesday.\nThe value at position 3 is Thursday.\nThe value at position 4 is Friday.\n\n\nAnother way to achieve the same result is by using the enumerate function with a_list as an input, which returns a tuple containing the index and the corresponding element.\n\nfor i, val in enumerate(a_list):\n    print('The value at position {} is {}.'.format(i, val))\n\nThe value at position 0 is Monday.\nThe value at position 1 is Tuesday.\nThe value at position 2 is Wednesday.\nThe value at position 3 is Thursday.\nThe value at position 4 is Friday.",
    "crumbs": [
      "Prerequisite: Python programming",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "data_structures.html",
    "href": "data_structures.html",
    "title": "4  Data structures",
    "section": "",
    "text": "4.1 Tuple\nIn this chapter we’ll learn about the python data structures that are often used or appear while analyzing data.\nTuple is a sequence of python objects, with two key characteristics: (1) the number of objects are fixed, and (2) the objects are immutable, i.e., they cannot be changed.\nTuple can be defined as a sequence of python objects separated by commas, and enclosed in rounded brackets (). For example, below is a tuple containing three integers.\ntuple_example = (2,7,4)\nWe can check the data type of a python object using the in-built python function type(). Let us check the data type of the object tuple_example.\ntype(tuple_example)\n\ntuple",
    "crumbs": [
      "Prerequisite: Python programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "data_structures.html#tuple",
    "href": "data_structures.html#tuple",
    "title": "4  Data structures",
    "section": "",
    "text": "4.1.1 Tuple Indexing\nTuple is ordered, meaning you can access specific elements in a list using their index. Indexing in lists includes both positive indexing (starting from 0 for the first element) and negative indexing (starting from -1 for the last element).\n\nElements of a tuple can be extracted using their index within square brackets. For example the second element of the tuple tuple_example can be extracted as follows:\n\ntuple_example[1]\n\n7\n\n\n\ntuple_example[-1]\n\n4\n\n\nNote that an element of a tuple cannot be modified. For example, consider the following attempt in changing the second element of the tuple tuple_example.\n\n# uncomment the following line to see the error\n# tuple_example[1] = 8\n\nThe above code results in an error as tuple elements cannot be modified.\n\n\n4.1.2 Concatenating tuples\nTuples can be concatenated using the + operator to produce a longer tuple:\n\n(2,7,4) + (\"another\", \"tuple\") + (\"mixed\",\"datatypes\",5)\n\n(2, 7, 4, 'another', 'tuple', 'mixed', 'datatypes', 5)\n\n\nMultiplying a tuple by an integer results in repetition of the tuple:\n\n(2,7,\"hi\") * 3\n\n(2, 7, 'hi', 2, 7, 'hi', 2, 7, 'hi')\n\n\n\n\n4.1.3 Unpacking tuples\nIf tuples are assigned to an expression containing multiple variables, the tuple will be unpacked and each variable will be assigned a value as per the order in which it appears. See the example below.\n\nx,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)))\n\n\nx\n\n4.5\n\n\n\ny\n\n'this is a string'\n\n\n\nz\n\n('Nested tuple', 5)\n\n\nIf we are interested in retrieving only some values of the tuple, the expression *_ can be used to discard the other values. Let’s say we are interested in retrieving only the first and the last two values of the tuple:\n\nx,*_,y,z  = (4.5, \"this is a string\", ((\"Nested tuple\",5)),\"99\",99)\n\n\nx\n\n4.5\n\n\n\ny\n\n'99'\n\n\n\nz\n\n99\n\n\n\n\n4.1.4 Tuple methods\nA couple of useful tuple methods are count, which counts the occurrences of an element in the tuple and index, which returns the position of the first occurrence of an element in the tuple:\n\ntuple_example = (2,5,64,7,2,2)\n\n\ntuple_example.count(2)\n\n3\n\n\n\ntuple_example.index(2)\n\n0\n\n\nNow that we have an idea about tuple, let us try to think where it can be used.",
    "crumbs": [
      "Prerequisite: Python programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "data_structures.html#list",
    "href": "data_structures.html#list",
    "title": "4  Data structures",
    "section": "4.2 List",
    "text": "4.2 List\nList is a sequence of python objects, with two key characeterisics that differentiates it from tuple: (1) the number of objects are variable, i.e., objects can be added or removed from a list, and (2) the objects are mutable, i.e., they can be changed.\nList can be defined as a sequence of python objects separated by commas, and enclosed in square brackets []. For example, below is a list consisting of three integers.\n\nlist_example = [2,7,4]\n\nList indexing works the same way as tuple indexing.\n\n4.2.1 Slicing a list\nList slicing is a technique in Python that allows you to extract a portion of a list by specifying a range of indices. It creates a new list containing the elements from the original list within that specified range. List slicing uses the colon : operator to indicate the start, stop, and step values for the slice. The general syntax is:  new_list = original_list[start:stop:step]\nHere’s what each part of the slice means: * start: The index at which the slice begins (inclusive). If omitted, it starts from the beginning (index 0). * stop: The index at which the slice ends (exclusive). If omitted, it goes until the end of the list. * step: The interval between elements in the slice. If omitted, it defaults to 1.\n\nlist_example6 = [4,7,3,5,7,1,5,87,5]\n\nLet us extract a slice containing all the elements from the the 3rd position to the 7th position.\n\nlist_example6[2:7]\n\n[3, 5, 7, 1, 5]\n\n\nNote that while the element at the start index is included, the element with the stop index is excluded in the above slice.\nIf either the start or stop index is not mentioned, the slicing will be done from the beginning or until the end of the list, respectively.\n\nlist_example6[:7]\n\n[4, 7, 3, 5, 7, 1, 5]\n\n\n\nlist_example6[2:]\n\n[3, 5, 7, 1, 5, 87, 5]\n\n\nTo slice the list relative to the end, we can use negative indices:\n\nlist_example6[-4:]\n\n[1, 5, 87, 5]\n\n\n\nlist_example6[-4:-2:]\n\n[1, 5]\n\n\nAn extra colon (‘:’) can be used to slice every \\(n\\)th element of a list.\n\n#Selecting every 3rd element of a list\nlist_example6[::3]\n\n[4, 5, 5]\n\n\n\n#Selecting every 3rd element of a list from the end\nlist_example6[::-3]\n\n[5, 1, 3]\n\n\n\n#Selecting every element of a list from the end or reversing a list \nlist_example6[::-1]\n\n[5, 87, 5, 1, 7, 5, 3, 7, 4]\n\n\n\n\n4.2.2 Adding and removing elements in a list\nWe can add elements at the end of the list using the append method. For example, we append the string ‘red’ to the list list_example below.\n\nlist_example.append('red')\n\n\nlist_example\n\n[2, 7, 4, 'red']\n\n\nNote that the objects of a list or a tuple can be of different datatypes.\nAn element can be added at a specific location of the list using the insert method. For example, if we wish to insert the number 2.32 as the second element of the list list_example, we can do it as follows:\n\nlist_example.insert(1,2.32)\n\n\nlist_example\n\n[2, 2.32, 7, 4, 'red']\n\n\nFor removing an element from the list, the pop and remove methods may be used. The pop method removes an element at a particular index, while the remove method removes the element’s first occurence in the list by its value. See the examples below.\nLet us say, we need to remove the third element of the list.\n\nlist_example.pop(2)\n\n7\n\n\n\nlist_example\n\n[2, 2.32, 4, 'red']\n\n\nLet us say, we need to remove the element ‘red’.\n\nlist_example.remove('red')\n\n\nlist_example\n\n[2, 2.32, 4]\n\n\n\n#If there are multiple occurrences of an element in the list, the first occurence will be removed\nlist_example2 = [2,3,2,4,4]\nlist_example2.remove(2)\nlist_example2\n\n[3, 2, 4, 4]\n\n\nFor removing multiple elements in a list, either pop or remove can be used in a for loop, or a for loop can be used with a condition. See the examples below.\nLet’s say we need to remove intergers less than 100 from the following list.\n\nlist_example3 = list(range(95,106))\nlist_example3\n\n[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n\n\n\n#Method 1: For loop with remove, iterating over the elements of the original list, \n#but updating a copy of the original list\nlist_example3_filtered = list(list_example3) #\nfor element in list_example3:\n    if element&lt;100:\n        list_example3_filtered.remove(element)\nprint(list_example3_filtered)\n\n[100, 101, 102, 103, 104, 105]\n\n\n\\(\\color{red}{\\text{Q1}}\\): What’s the need to define a new variable list_example3_filtered in the above code?\n\\(\\color{blue}{\\text{A1}}\\): Replace list_example3_filtered with list_example3 and identify the issue. After an element is removed from the list, all the elements that come afterward have their index/position reduced by one. After the elment 95 is removed, 96 is at index 0, but the for loop will now look at the element at index 1, which is now 97. So, iterating over the same list that is being updated in the loop will keep 96 and 98. Using a new list gets rid of the issue by keeping the original list unchanged, so the for-loop iterates over all elements of the original list.\nAnother method could have been to interate over a copy of the original list and update the original list as shown below.\n\n#Method 2: For loop with remove, iterating over the elements of a copy of the original list, \n#but updating the original list\nfor element in list_example3[:]: #Slicing a list creates a new list, thus the loop is iterating over elements of a copy of the original list as all the elements are selected in the slicing\n    if element&lt;100:\n        list_example3.remove(element)\nprint(list_example3)\n\n[100, 101, 102, 103, 104, 105]\n\n\nBelow is another method that uses a shorthand notation - list comprehension (explained in the next section).\n\n#Method 3: For loop with condition in list comprehension\nlist_example3 = list(range(95,106))\n[element for element in list_example3 if element&gt;=100]\n\n[100, 101, 102, 103, 104, 105]\n\n\n\n\n4.2.3 List comprehensions\nList comprehensions provide a concise and readable way to create new lists by applying an expression to each item in an iterable (e.g., a list, tuple, or range) and optionally filtering the items based on a condition. They are a powerful and efficient way to generate lists without the need for explicit loops. The basic syntax of a list comprehension is as follows:\nnew_list = [expression for item in iterable if condition]\n\nexpression: This is the expression that is applied to each item in the iterable. It defines what will be included in the new list.\nitem: This is a variable that represents each element in the iterable as the comprehension iterates through it.\niterable: This is the source from which the elements are taken. It can be any iterable, such as a list, tuple, range, or other iterable objects.\ncondition (optional): This is an optional filter that can be applied to control which items from the iterable are included in the new list. If omitted, all items from the iterable are included.\n\nExample: Create a list that has squares of natural numbers from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]\n\n\nExample: Create a list of tuples, where each tuple consists of a natural number and its square, for natural numbers ranging from 5 to 15.\n\nsqrt_natural_no_5_15 = [(x,x**2) for x in range(5,16)]\nprint(sqrt_natural_no_5_15)\n\n[(5, 25), (6, 36), (7, 49), (8, 64), (9, 81), (10, 100), (11, 121), (12, 144), (13, 169), (14, 196), (15, 225)]\n\n\nExample: Creating a list of words that start with the letter ‘a’ in a given list of words.\n\nwords = ['apple', 'banana', 'avocado', 'grape', 'apricot']\na_words = [word for word in words if word.startswith('a')]\nprint(a_words)\n\n['apple', 'avocado', 'apricot']\n\n\nExample: Create a list of even numbers from 1 to 20.\n\neven_numbers = [x for x in range(1, 21) if x % 2 == 0]\nprint(even_numbers)\n\n[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n\n\nList comprehensions are not only concise but also considered more Pythonic and often more efficient than using explicit loops for simple operations. They can make your code cleaner and easier to read, especially for operations that transform or filter data in a list.\n\n\n4.2.4 Practice exercise 1\nBelow is a list consisting of responses to the question: “At what age do you think you will marry?” from students of the STAT303-1 Fall 2022 class.\n\nexp_marriage_age=['24','30','28','29','30','27','26','28','30+','26','28','30','30','30','probably never','30','25','25','30','28','30+ ','30','25','28','28','25','25','27','28','30','30','35','26','28','27','27','30','25','30','26','32','27','26','27','26','28','37','28','28','28','35','28','27','28','26','28','26','30','27','30','28','25','26','28','35','29','27','27','30','24','25','29','27','33','30','30','25','26','30','32','26','30','30','I wont','25','27','27','25','27','27','32','26','25','never','28','33','28','35','25','30','29','30','31','28','28','30','40','30','28','30','27','by 30','28','27','28','30-35','35','30','30','never','30','35','28','31','30','27','33','32','27','27','26','N/A','25','26','29','28','34','26','24','28','30','120','25','33','27','28','32','30','26','30','30','28','27','27','27','27','27','27','28','30','30','30','28','30','28','30','30','28','28','30','27','30','28','25','never','69','28','28','33','30','28','28','26','30','26','27','30','25','Never','27','27','25']\n\nUse list comprehension to:\n\n4.2.4.1 \nRemove the elements that are not integers - such as ‘probably never’, ‘30+’, etc. What is the length of the new list?\nHint: The built-in python function of the str class - isdigit() may be useful to check if the string contains only digits.\nSolution:\n\nexp_marriage_age_num = [x for x in exp_marriage_age if x.isdigit()==True]\nprint(\"Length of the new list = \",len(exp_marriage_age_num))\n\nLength of the new list =  181\n\n\n\n\n4.2.4.2 \nCap the values greater than 80 to 80, in the clean list obtained in (1). What is the mean age when people expect to marry in the new list?\n\nexp_marriage_age_capped = [min(int(x),80) for x in exp_marriage_age_num]\nprint(\"Mean age when people expect to marry = \", sum(exp_marriage_age_capped)/len(exp_marriage_age_capped))\n\nMean age when people expect to marry =  28.955801104972377\n\n\n\n\n4.2.4.3 \nDetermine the percentage of people who expect to marry at an age of 30 or more.\n\nprint(\"Percentage of people who expect to marry at an age of 30 or more =\", str(100*sum([1 for x in exp_marriage_age_capped if x&gt;=30])/len(exp_marriage_age_capped)),\"%\")\n\nPercentage of people who expect to marry at an age of 30 or more = 37.01657458563536 %\n\n\n\n\n4.2.4.4 \nRedo Q2.2.4.2 using the if-else statement within list comprehension.\n\n\n\n4.2.5 Practice exercise 2\nBelow is a list consisting of responses to the question: “What do you expect your starting salary to be after graduation, to the nearest thousand dollars? (ex: 47000)” from students of the STAT303-1 Fall 2023. class.\n\nexpected_salary = ['90000', '110000', '100000', '90k', '80000', '47000', '100000', '70000', '95000', '150000', '50000', '110000', '100000', '60000', '50000', '100000', '80000', '100000', '70000', '60000', '100k', '70000', '0', '60000', '50000', '150000', '90000', '80000', '110000', '85000', '90000', '50000', '60000', '150000', '100000', '100000', '125000', '30000', '100000', '110000', '90000', '600000', '80000', '100000', '100000', '70000', '60000', '0', '70000', '90000', '100000', '60000', '80000', '70000', '100000', '57000', '70000', '60000', '65000', '70000', '100000', '200000', '60000', '90000', '80000', '200000', '90000', '80000', '60000', '70000', '90000', '80000', '90000', '120000', '60000', '40000', '80000', '100000', '75000', '80000', '70000', '90000', '80000', '80000', '70000', '0', '50000', '65000', 'n/a', '100000', '60000', '65000', '100000', '100000', '65000', '90000', '50000', '80000', '90000', '100000', '100000', '100000', '100000', '80000', '60000', '100000', '80000', '55000', '80000', '100000', '60000', '130000', '35000', '70000', '50000', '120000', '110000', '110000', '80000', '70000', '90000', '100000', '90000', '100000', '70000', '110000', '300000', '90000', '45000', '90000', '60000', '44000', '1000000', '65000', '40000', '60000', '100000', '80000', '90000', '45000', '86000', '100000', '100,000+', '50000', '0']\n\nClean expected_salary using list comprehensions only, and find the mean expected salary.\n\n\n4.2.6 Concatenating lists\nAs in tuples, lists can be concatenated using the + operator:\n\nimport time as tm\n\n\nlist_example4 = [5,'hi',4] \nlist_example4 = list_example4 + [None,'7',9]\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\nFor adding elements to a list, the extend method is preferred over the + operator. This is because the + operator creates a new list, while the extend method adds elements to an existing list. Thus, the extend operator is more memory efficient.\n\nlist_example4 = [5,'hi',4]\nlist_example4.extend([None, '7', 9])\nlist_example4\n\n[5, 'hi', 4, None, '7', 9]\n\n\n\n\n4.2.7 Sorting a list\nA list can be sorted using the sort method:\n\nlist_example5 = [6,78,9]\nlist_example5.sort(reverse=True) #the reverse argument is used to specify if the sorting is in ascending or descending order\nlist_example5\n\n[78, 9, 6]\n\n\n\n\n4.2.8 Practice exercise 3\nStart with the list [8,9,10]. Do the following:\n\n4.2.8.1 \nSet the second entry (index 1) to 17\n\nL = [8,9,10]\nL[1]=17\n\n\n\n4.2.8.2 \nAdd 4, 5, and 6 to the end of the list\n\nL = L+[4,5,6]\n\n\n\n4.2.8.3 \nRemove the first entry from the list\n\nL.pop(0)\n\n8\n\n\n\n\n4.2.8.4 \nSort the list\n\nL.sort()\n\n\n\n4.2.8.5 \nDouble the list (concatenate the list to itself)\n\nL=L+L\n\n\n\n4.2.8.6 \nInsert 25 at index 3\nThe final list should equal [4,5,6,25,10,17,4,5,6,10,17]\n\nL.insert(3,25)\nL\n\n[4, 5, 6, 25, 10, 17, 4, 5, 6, 10, 17]\n\n\nNow that we have an idea about lists, let us try to think where it can be used.\n\n\n\n\n\n \n        \n\n\n\n\n\n4.2.9 Other list operations\nYou can test whether a list contains a value using the in operator.\n\nlist_example6\n\n[4, 7, 3, 5, 7, 1, 5, 87, 5]\n\n\n\n6 in list_example6\n\nFalse\n\n\n\n7 in list_example6\n\nTrue\n\n\n\n\n4.2.10 Lists: methods\nJust like strings, there are several in-built methods to manipulate a list. However, unlike strings, most list methods modify the original list rather than returning a new one. Here are some common list operations: \n\n\n\n4.2.11 Lists vs tuples\nNow that we have learned about lists and tuples, let us compare them.\n\\(\\color{red}{\\text{Q2}}\\): A list seems to be much more flexible than tuple, and can replace a tuple almost everywhere. Then why use tuple at all?\n\\(\\color{blue}{\\text{A2}}\\): The additional flexibility of a list comes at the cost of efficiency. Some of the advantages of a tuple over a list are as follows:\n\nSince a list can be extended, space is over-allocated when creating a list. A tuple takes less storage space as compared to a list of the same length.\nTuples are not copied. If a tuple is assigned to another tuple, both tuples point to the same memory location. However, if a list is assigned to another list, a new list is created consuming the same memory space as the original list.\nTuples refer to their element directly, while in a list, there is an extra layer of pointers that refers to their elements. Thus it is faster to retrieve elements from a tuple.\n\nThe examples below illustrate the above advantages of a tuple.\n\n#Example showing tuples take less storage space than lists for the same elements\ntuple_ex = (1, 2, 'Obama')\nlist_ex = [1, 2, 'Obama']\nprint(\"Space taken by tuple =\",tuple_ex.__sizeof__(),\" bytes\")\nprint(\"Space taken by list =\",list_ex.__sizeof__(),\" bytes\")\n\nSpace taken by tuple = 48  bytes\nSpace taken by list = 64  bytes\n\n\n\n#Examples showing that a tuples are not copied, while lists can be copied\ntuple_copy = tuple(tuple_ex)\nprint(\"Is tuple_copy same as tuple_ex?\", tuple_ex is tuple_copy)\nlist_copy = list(list_ex)\nprint(\"Is list_copy same as list_ex?\",list_ex is list_copy)\n\nIs tuple_copy same as tuple_ex? True\nIs list_copy same as list_ex? False\n\n\n\n#Examples showing tuples takes lesser time to retrieve elements\nimport time as tm\ntt = tm.time()\nlist_ex = list(range(1000000)) #List containinig whole numbers upto 1 million\na=(list_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a list = \", tm.time()-tt)\n\ntt = tm.time()\ntuple_ex = tuple(range(1000000)) #tuple containinig whole numbers upto 1 million\na=(tuple_ex[::-2])\nprint(\"Time take to retrieve every 2nd element from a tuple = \", tm.time()-tt)\n\nTime take to retrieve every 2nd element from a list =  0.03579902648925781\nTime take to retrieve every 2nd element from a tuple =  0.02684164047241211",
    "crumbs": [
      "Prerequisite: Python programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "data_structures.html#dictionary",
    "href": "data_structures.html#dictionary",
    "title": "4  Data structures",
    "section": "4.3 Dictionary",
    "text": "4.3 Dictionary\nUnlike lists and tuples, a dictionary is an unordered collection of items. Each item stored in a dictionary has a key and value. You can use a key to retrieve the corresponding value from the dictionary. Dictionaries have the type dict.\nDictionaries are often used to store many pieces of information e.g. details about a person, in a single variable. Dictionaries are created by enclosing key-value pairs within braces or curly brackets { and }, colons to separate keys and values, and commas to separate elements of a dictionary.\nThe dictionary keys and values are python objects. While values can be any python object, keys need to be immutable python objects, like strings, integers, tuples, etc. Thus, a list can be a value, but not a key, as elements of list can be changed.\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping'}\n\nElements of a dictionary can be retrieved by using the corresponding key.\n\ndict_example['India']\n\n'Narendra Modi'\n\n\n\n4.3.1 Viewing keys and values\n\ndict_example.keys()\n\ndict_keys(['USA', 'India', 'China'])\n\n\n\ndict_example.values()\n\ndict_values(['Joe Biden', 'Narendra Modi', 'Xi Jinping'])\n\n\n\ndict_example.items()\n\ndict_items([('USA', 'Joe Biden'), ('India', 'Narendra Modi'), ('China', 'Xi Jinping')])\n\n\nThe results of keys, values, and items look like lists. However, they don’t support the indexing operator [] for retrieving elements.\n\n# uncomed the following line to see the error\n# dict_example.items()[1]\n\n\n\n4.3.2 Adding and removing elements in a dictionary\nNew elements can be added to a dictionary by defining a key in square brackets and assiging it to a value:\n\ndict_example['Japan'] = 'Fumio Kishida'\ndict_example['Countries'] = 4\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida',\n 'Countries': 4}\n\n\nElements can be removed from the dictionary using the del method or the pop method:\n\n#Removing the element having key as 'Countries'\ndel dict_example['Countries']\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Japan': 'Fumio Kishida'}\n\n\n\n#Removing the element having key as 'USA'\ndict_example.pop('USA')\n\n'Joe Biden'\n\n\n\ndict_example\n\n{'India': 'Narendra Modi', 'China': 'Xi Jinping', 'Japan': 'Fumio Kishida'}\n\n\nNew elements can be added, and values of exisiting keys can be changed using the update method:\n\ndict_example = {'USA':'Joe Biden', 'India':'Narendra Modi', 'China':'Xi Jinping','Countries':3}\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 3}\n\n\n\ndict_example.update({'Countries':4, 'Japan':'Fumio Kishida'})\n\n\ndict_example\n\n{'USA': 'Joe Biden',\n 'India': 'Narendra Modi',\n 'China': 'Xi Jinping',\n 'Countries': 4,\n 'Japan': 'Fumio Kishida'}\n\n\n\n\n4.3.3 Iterating over elements of a dictionary\nThe items() attribute of a dictionary can be used to iterate over elements of a dictionary.\n\nfor key,value in dict_example.items():\n    print(\"The Head of State of\",key,\"is\",value)\n\nThe Head of State of USA is Joe Biden\nThe Head of State of India is Narendra Modi\nThe Head of State of China is Xi Jinping\nThe Head of State of Countries is 4\nThe Head of State of Japan is Fumio Kishida\n\n\n\n\n4.3.4 Practice exercise 4\nThe GDP per capita of USA for most years from 1960 to 2021 is given by the dictionary D given in the code cell below.\nFind:\n\nThe GDP per capita in 2015\nThe GDP per capita of 2014 is missing. Update the dictionary to include the GDP per capita of 2014 as the average of the GDP per capita of 2013 and 2015.\nImpute the GDP per capita of other missing years in the same manner as in (2), i.e., as the average GDP per capita of the previous year and the next year. Note that the GDP per capita is not missing for any two consecutive years.\nPrint the years and the imputed GDP per capita for the years having a missing value of GDP per capita in (3).\n\n\nD = {'1960':3007,'1961':3067,'1962':3244,'1963':3375,'1964':3574,'1965':3828,'1966':4146,'1967':4336,'1968':4696,'1970':5234,'1971':5609,'1972':6094,'1973':6726,'1974':7226,'1975':7801,'1976':8592,'1978':10565,'1979':11674, '1980':12575,'1981':13976,'1982':14434,'1983':15544,'1984':17121,'1985':18237,  '1986':19071,'1987':20039,'1988':21417,'1989':22857,'1990':23889,'1991':24342,  '1992':25419,'1993':26387,'1994':27695,'1995':28691,'1996':29968,'1997':31459,  '1998':32854,'2000':36330,'2001':37134,'2002':37998,'2003':39490,'2004':41725,  '2005':44123,'2006':46302,'2007':48050,'2008':48570,'2009':47195,'2010':48651,  '2011':50066,'2012':51784,'2013':53291,'2015':56763,'2016':57867,'2017':59915,'2018':62805, '2019':65095,'2020':63028,'2021':69288}\n\nSolution:\n\nprint(\"GDP per capita in 2015 =\", D['2015'])\nD['2014'] = (D['2013']+D['2015'])/2\nfor i in range(1960,2021):\n    if str(i) not in D.keys():    \n        D[str(i)] = (D[str(i-1)]+D[str(i+1)])/2\n        print(\"Imputed GDP per capita for the year\",i,\"is $\",D[str(i)])\n\nGDP per capita in 2015 = 56763\nImputed GDP per capita for the year 1969 is $ 4965.0\nImputed GDP per capita for the year 1977 is $ 9578.5\nImputed GDP per capita for the year 1999 is $ 34592.0",
    "crumbs": [
      "Prerequisite: Python programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "data_structures.html#functions",
    "href": "data_structures.html#functions",
    "title": "4  Data structures",
    "section": "4.4 Functions",
    "text": "4.4 Functions\nIf an algorithm or block of code is being used several times in a code, then it can be separately defined as a function. This makes the code more organized and readable. For example, let us define a function that prints prime numbers between a and b, and returns the number of prime numbers found.\n\n#Function definition\ndef prime_numbers (a,b=100):\n    num_prime_nos = 0\n    \n    #Iterating over all numbers between a and b\n    for i in range(a,b):\n        num_divisors=0\n        \n        #Checking if the ith number has any factors\n        for j in range(2, i):\n            if i%j == 0:\n                num_divisors=1;break;\n                \n        #If there are no factors, then printing and counting the number as prime        \n        if num_divisors==0:\n            print(i)\n            num_prime_nos = num_prime_nos+1\n            \n    #Return count of the number of prime numbers\n    return num_prime_nos\n\nIn the above function, the keyword def is used to define the function, prime_numbers is the name of the function, a and b are the arguments that the function uses to compute the output.\nLet us use the defined function to print and count the prime numbers between 40 and 60.\n\n#Printing prime numbers between 40 and 60\nnum_prime_nos_found = prime_numbers(40,60)\n\n41\n43\n47\n53\n59\n\n\n\nnum_prime_nos_found\n\n5\n\n\nIf the user calls the function without specifying the value of the argument b, then it will take the default value of 100, as mentioned in the function definition. However, for the argument a, the user will need to specify a value, as there is no value defined as a default value in the function definition.\n\n4.4.1 Global and local variables with respect to a function\nA variable defined within a function is local to that function, while a variable defined outside the function is global to that function. In case a variable with the same name is defined both outside and inside a function, it will refer to its global value outside the function and local value within the function.\nThe example below shows a variable with the name var referring to its local value when called within the function, and global value when called outside the function.\n\nvar = 5\ndef sample_function(var):    \n    print(\"Local value of 'var' within 'sample_function()'= \",var)\n\nsample_function(4)\nprint(\"Global value of 'var' outside 'sample_function()' = \",var)\n\nLocal value of 'var' within 'sample_function()'=  4\nGlobal value of 'var' outside 'sample_function()' =  5\n\n\n\n\n4.4.2 Practice exercise 5\nThe object deck defined below corresponds to a deck of cards. Estimate the probablity that a five card hand will be a flush, as follows:\n\nWrite a function that accepts a hand of 5 cards as argument, and returns whether the hand is a flush or not.\nRandomly pull a hand of 5 cards from the deck. Call the function developed in (1) to determine if the hand is a flush.\nRepeat (2) 10,000 times.\nEstimate the probability of the hand being a flush from the results of the 10,000 simulations.\n\nYou may use the function shuffle() from the random library to shuffle the deck everytime before pulling a hand of 5 cards.\n\ndeck = [{'value':i, 'suit':c}\nfor c in ['spades', 'clubs', 'hearts', 'diamonds']\nfor i in range(2,15)]\n\nSolution:\n\nimport random as rm\n\n#Function to check if a 5-card hand is a flush\ndef chck_flush(hands):  \n    \n    #Assuming that the hand is a flush, before checking the cards\n    yes_flush =1\n    \n    #Storing the suit of the first card in 'first_suit'\n    first_suit = hands[0]['suit']\n    \n    #Iterating over the remaining 4 cards of the hand\n    for j in range(1,len(hands)):\n        \n        #If the suit of any of the cards does not match the suit of the first card, the hand is not a flush\n        if first_suit!=hands[j]['suit']:\n            yes_flush = 0; \n            \n            #As soon as a card with a different suit is found, the hand is not a flush and there is no need to check other cards. So, we 'break' out of the loop\n            break;\n    return yes_flush\n\nflush=0\nfor i in range(10000):\n    \n    #Shuffling the deck\n    rm.shuffle(deck)\n    \n    #Picking out the first 5 cards of the deck as a hand and checking if they are a flush\n    #If the hand is a flush it is counted\n    flush=flush+chck_flush(deck[0:5])\n    \nprint(\"Probability of obtaining a flush=\", 100*(flush/10000),\"%\")\n\nProbability of obtaining a flush= 0.18 %",
    "crumbs": [
      "Prerequisite: Python programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "data_structures.html#practice-exercise-6",
    "href": "data_structures.html#practice-exercise-6",
    "title": "4  Data structures",
    "section": "4.5 Practice exercise 6",
    "text": "4.5 Practice exercise 6\nThe code cell below defines an object having the nutrition information of drinks in starbucks. Assume that the manner in which the information is structured is consistent throughout the object.\n\nstarbucks_drinks_nutrition={'Cool Lime Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Strawberry Acai Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Very Berry Hibiscus Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Evolution Fresh™ Organic Ginger Limeade': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Coffee': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Espresso Classics - Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Iced Espresso Classics - Caffe Mocha': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 23}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Espresso Classics - Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Shaken Sweet Tea': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Berry Blossom White': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Black Mango': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Black with Lemon': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Brambleberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Giant Peach': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Iced Passion': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Lemon Ginger': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Black Lemonade': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Plum Pomegranate': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Tazoberry': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled White Cranberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Teavana® Shaken Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Black Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Green Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Peach Green Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Raspberry Pomegranate': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Strawberry Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Doubleshot Protein Dark Chocolate': [{'Nutrition_type': 'Calories', 'value': 210}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 33}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Starbucks® Doubleshot Protein Vanilla': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 120}], 'Starbucks® Iced Coffee Caramel': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Light Sweetened': [{'Nutrition_type': 'Calories', 'value': 50}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Unsweetened': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 2}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Blonde Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Clover® Brewed Coffee': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Decaf Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Featured Dark Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nariño 70 Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Nariño 70 Cold Brew with Milk': [{'Nutrition_type': 'Calories', 'value': 0}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Nitro Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nitro Cold Brew with Sweet Cream': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 20}], 'Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Vanilla Sweet Cream Cold Brew': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 25}], 'Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks® Signature Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 430}, {'Nutrition_type': 'Fat', 'value': 26.0}, {'Nutrition_type': 'Carb', 'value': 45}, {'Nutrition_type': 'Fiber', 'value': 5}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 290}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 42}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 140}], 'Cappuccino': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 12}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 40}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 32}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Flat White': [{'Nutrition_type': 'Calories', 'value': 180}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Iced Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Iced Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 230}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 36}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 9}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Iced Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 95}], 'Iced Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Iced Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 30}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Iced White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 190}], 'Latte Macchiato': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks Doubleshot® on Ice Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 1.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 40}], 'Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 150}], 'White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 360}, {'Nutrition_type': 'Fat', 'value': 11.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 240}], 'Cinnamon Dolce Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 350}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 64}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 15}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Coffee Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 200}], 'Mocha Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 280}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 220}], 'Mocha Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.5}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Cinnamon Dolce Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Vanilla Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Chocolate Smoothie': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 8}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Strawberry Smoothie': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 2.0}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 7}, {'Nutrition_type': 'Protein', 'value': 16}, {'Nutrition_type': 'Sodium', 'value': 130}]}\n\nUse the object above to answer the following questions:\n\n4.5.1 \nWhat is the datatype of the object?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition)) \n\nDatatype= &lt;class 'dict'&gt;\n\n\n\n4.5.1.1 \nIf the object in (1) is a dictonary, what is the datatype of the values of the dictionary?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]]))\n\nDatatype= &lt;class 'list'&gt;\n\n\n\n\n4.5.1.2 \nIf the object in (1) is a dictonary, what is the datatype of the elements within the values of the dictionary?\n\nprint(\"Datatype=\",type(starbucks_drinks_nutrition[list(starbucks_drinks_nutrition.keys())[0]][0]))\n\nDatatype= &lt;class 'dict'&gt;\n\n\n\n\n4.5.1.3 \nHow many calories are there in Iced Coffee?\n\nprint(\"Calories = \",starbucks_drinks_nutrition['Iced Coffee'][0]['value'])\n\nCalories =  5\n\n\n\n\n4.5.1.4 \nWhich drink(s) have the highest amount of protein in them, and what is that protein amount?\n\n#Defining an empty dictionary that will be used to store the protein of each drink\nprotein={}\n\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Protein':\n            protein[key]=(nutrition['value'])\n\n#Using dictionary comprehension to find the key-value pair having the maximum value in the dictionary\n{key:value for key, value in protein.items() if value == max(protein.values())}\n\n{'Starbucks® Doubleshot Protein Dark Chocolate': 20,\n 'Starbucks® Doubleshot Protein Vanilla': 20,\n 'Chocolate Smoothie': 20}\n\n\n\n\n4.5.1.5 \nWhich drink(s) have a fat content of more than 10g, and what is their fat content?\n\n#Defining an empty dictionary that will be used to store the fat of each drink\nfat={}\nfor key,value in starbucks_drinks_nutrition.items():\n    for nutrition in value:        \n        if nutrition['Nutrition_type']=='Fat':\n            fat[key]=(nutrition['value'])\n            \n#Using dictionary comprehension to find the key-value pair having the value more than 10\n{key:value for key, value in fat.items() if value&gt;=10}\n\n{'Starbucks® Signature Hot Chocolate': 26.0, 'White Chocolate Mocha': 11.0}\n\n\n\n\n4.5.1.6 \nAnswer Q2.5.1.5 using only dictionary comprehension.",
    "crumbs": [
      "Prerequisite: Python programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "Reading data.html",
    "href": "Reading data.html",
    "title": "5  Reading Data",
    "section": "",
    "text": "5.1 Types of data - structured and unstructured\nReading data is the first step to extract information from it. Data can exist broadly in two formats:\nStructured data is typically stored in a tabular form, where rows in the data correspond to “observations” and columns correspond to “variables”. For example, the following dataset contains 5 observations, where each observation (or row) consists of information about a movie. The variables (or columns) contain different pieces of information about a given movie. As all variables for a given row are related to the same movie, the data below is also called relational data.\nTitle\nUS Gross\nProduction Budget\nRelease Date\nMajor Genre\nCreative Type\nRotten Tomatoes Rating\nIMDB Rating\n\n\n\n\n0\nThe Shawshank Redemption\n28241469\n25000000\nSep 23 1994\nDrama\nFiction\n88\n9.2\n\n\n1\nInception\n285630280\n160000000\nJul 16 2010\nHorror/Thriller\nFiction\n87\n9.1\n\n\n2\nOne Flew Over the Cuckoo's Nest\n108981275\n4400000\nNov 19 1975\nComedy\nFiction\n96\n8.9\n\n\n3\nThe Dark Knight\n533345358\n185000000\nJul 18 2008\nAction/Adventure\nFiction\n93\n8.9\n\n\n4\nSchindler's List\n96067179\n25000000\nDec 15 1993\nDrama\nNon-Fiction\n97\n8.9\nUnstructured data is data that is not organized in any pre-defined manner. Examples of unstructured data can be text files, audio/video files, images, Internet of Things (IoT) data, etc. Unstructured data is relatively harder to analyze as most of the analytical methods and tools are oriented towards structured data. However, an unstructured data can be used to obtain structured data, which in turn can be analyzed. For example, an image can be converted to an array of pixels - which will be structured data. Machine learning algorithms can then be used on the array to classify the image as that of a dog or a cat.\nIn this course, we will focus on analyzing structured data.\nPandas is a popular Python library used for working in tabular data (similar to the data stored in a spreadsheet). Pandas provides helper functions to read data from various file formats like CSV, Excel spreadsheets, HTML tables, JSON, SQL, and more.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "Reading data.html#types-of-data---structured-and-unstructured",
    "href": "Reading data.html#types-of-data---structured-and-unstructured",
    "title": "5  Reading Data",
    "section": "",
    "text": "Structured data, and\nUnstructured data.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "Reading data.html#reading-a-csv-file-with-pandas",
    "href": "Reading data.html#reading-a-csv-file-with-pandas",
    "title": "5  Reading Data",
    "section": "5.2 Reading a csv file with Pandas",
    "text": "5.2 Reading a csv file with Pandas\nStructured data can be stored in a variety of formats. The most popular format is data_file_name.csv, where the extension csv stands for comma separated values. The variable values of each observation are separated by a comma in a .csv file. In other words, the delimiter is a comma in a csv file. However, the comma is not visible when a .csv file is opened with Microsoft Excel.\nThe below csv file contains day-wise Covid-19 data for Italy:\ndate,new_cases,new_deaths,new_tests\n2020-04-21,2256.0,454.0,28095.0\n2020-04-22,2729.0,534.0,44248.0\n2020-04-23,3370.0,437.0,37083.0\n2020-04-24,2646.0,464.0,95273.0\n2020-04-25,3021.0,420.0,38676.0\n2020-04-26,2357.0,415.0,24113.0\n2020-04-27,2324.0,260.0,26678.0\n2020-04-28,1739.0,333.0,37554.0\n...\n\nCSVs: A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. A CSV file typically stores tabular data (numbers and text) in plain text, in which case each line will have the same number of fields. (Wikipedia)\n\nFirst, let’s import the Pandas library. As a convention, it is imported with the alias pd.\n\nimport pandas as pd\nimport os\n\n\n5.2.1 Using the read_csv function\nThe pd.read_csv function can be used to read a CSV file into a pandas DataFrame: a spreadsheet-like object for analyzing and processing data.\n\nmovie_ratings = pd.read_csv('./datasets/movie_ratings.csv')\n\nThe built-in python function type can be used to check the dataype of an object:\n\ntype(movie_ratings)\n\npandas.core.frame.DataFrame\n\n\nWe’ll learn more about DataFrame in a future lesson.\nNote that I use the relative path to specify the file path for movie_ratings.csv, you may need to change it based on where you store the data file.\n\n\n5.2.2 Data Overview\nOnce the data has been read, we may want to see what the data looks like. We’ll use another Pandas function head() to view the first few rows of the data.\n\nmovie_ratings.head()\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\nNov 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n\n\n1\nMajor Dundee\n14873\n14873\n3800000\nApr 07 1965\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n\n\n2\nThe Informers\n315000\n315000\n18000000\nApr 24 2009\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n\n\n3\nBuffalo Soldiers\n353743\n353743\n15000000\nJul 25 2003\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n\n\n4\nThe Last Sin Eater\n388390\n388390\n2200000\nFeb 09 2007\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\n\n\n\n\n\n\n\nRow Indices and column names (axis labels)\nBy default, when you create a pandas DataFrame (or Series) without specifying an index, pandas will automatically assign integer-based row indices starting from 0. These indices serve as the row labels and uniquely identify each row in the DataFrame. For example, the index 2 correponds to the row of the movie The Informers. By default, the indices are integers starting from 0. However, they can be changed (to even non-integer values) if desired by the user.\nThe bold text on top of the DataFrame refers to column names. For example, the column US Gross consists of the gross revenue of a movie in the US.\nCollectively, the indices and column names are referred as axis labels.\nBasic information We can view some basic information about the data frame using the .info method.\n\nmovie_ratings.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2228 entries, 0 to 2227\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Title              2228 non-null   object \n 1   US Gross           2228 non-null   int64  \n 2   Worldwide Gross    2228 non-null   int64  \n 3   Production Budget  2228 non-null   int64  \n 4   Release Date       2228 non-null   object \n 5   MPAA Rating        2228 non-null   object \n 6   Source             2228 non-null   object \n 7   Major Genre        2228 non-null   object \n 8   Creative Type      2228 non-null   object \n 9   IMDB Rating        2228 non-null   float64\n 10  IMDB Votes         2228 non-null   int64  \ndtypes: float64(1), int64(4), object(6)\nmemory usage: 191.6+ KB\n\n\nThe shape property of a pandas DataFrame provides a tuple that represents the dimensions of the DataFrame:\n\nThe first value in the tuple is the number of rows.\nThe second value in the tuple is the number of columns.\n\n\nmovie_ratings.shape\n\n(2228, 11)\n\n\nThe columns property contains the list of columns within the data frame.\n\nmovie_ratings.columns\n\nIndex(['Title', 'US Gross', 'Worldwide Gross', 'Production Budget',\n       'Release Date', 'MPAA Rating', 'Source', 'Major Genre', 'Creative Type',\n       'IMDB Rating', 'IMDB Votes'],\n      dtype='object')\n\n\nYou can view statistical information for numerical columns (mean, standard deviation, minimum/maximum values, and the number of non-empty values) using the .describe method.\n\nmovie_ratings.describe()\n\n\n\n\n\n\n\n\nUS Gross\nWorldwide Gross\nProduction Budget\nIMDB Rating\nIMDB Votes\n\n\n\n\ncount\n2.228000e+03\n2.228000e+03\n2.228000e+03\n2228.000000\n2228.000000\n\n\nmean\n5.076370e+07\n1.019370e+08\n3.816055e+07\n6.239004\n33585.154847\n\n\nstd\n6.643081e+07\n1.648589e+08\n3.782604e+07\n1.243285\n47325.651561\n\n\nmin\n0.000000e+00\n8.840000e+02\n2.180000e+02\n1.400000\n18.000000\n\n\n25%\n9.646188e+06\n1.320737e+07\n1.200000e+07\n5.500000\n6659.250000\n\n\n50%\n2.838649e+07\n4.266892e+07\n2.600000e+07\n6.400000\n18169.000000\n\n\n75%\n6.453140e+07\n1.200000e+08\n5.300000e+07\n7.100000\n40092.750000\n\n\nmax\n7.601676e+08\n2.767891e+09\n3.000000e+08\n9.200000\n519541.000000\n\n\n\n\n\n\n\nFunctions & Methods we’ve looked so far\n\npd.read_csv - Read data from a CSV file into a Pandas DataFrame object\n.info() - View basic infomation about rows, columns & data types\n.shape - Get the number of rows & columns as a tuple\n.columns - Get the list of column names\n.describe() - View statistical information about numeric columns",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "Reading data.html#data-selection-and-filtering",
    "href": "Reading data.html#data-selection-and-filtering",
    "title": "5  Reading Data",
    "section": "5.3 Data Selection and Filtering",
    "text": "5.3 Data Selection and Filtering\n\n5.3.1 Extracting Column(s)\nThe first step when working with a DataFrame is often to extract one or more columns. To do this effectively, it’s helpful to understand the internal structure of a DataFrame. Conceptually, you can think of a DataFrame as a dictionary of lists, where the keys are column names, and the values are lists or arrays containing data for the respective columns.\n\n# Pandas format is simliar to this\nmovie_ratings_dict = {\n    'Title':  ['Opal Dreams', 'Major Dundee', 'The Informers', 'Buffalo Soldiers', 'The Last Sin Eater'],\n    'US Gross':  [14443, 14873, 315000, 353743, 388390],\n    'Worldwide Gross': [14443, 14873, 315000, 353743, 388390],\n    'Production Budget': [9000000, 3800000, 18000000, 15000000, 2200000]\n}\n\nFor dictionary, we use key to retrive its values\n\nmovie_ratings_dict['Title']\n\n['Opal Dreams',\n 'Major Dundee',\n 'The Informers',\n 'Buffalo Soldiers',\n 'The Last Sin Eater']\n\n\nSimilar like dictionary, we can extract a column by its column name\n\nmovie_ratings['Title']\n\n0                         Opal Dreams\n1                        Major Dundee\n2                       The Informers\n3                    Buffalo Soldiers\n4                  The Last Sin Eater\n                    ...              \n2223                      King Arthur\n2224                            Mulan\n2225                       Robin Hood\n2226    Robin Hood: Prince of Thieves\n2227                       Spiceworld\nName: Title, Length: 2228, dtype: object\n\n\nEach column is a feature of the dataframe, we can also use. operator to extract a single column\n\nmovie_ratings.Title\n\n0                         Opal Dreams\n1                        Major Dundee\n2                       The Informers\n3                    Buffalo Soldiers\n4                  The Last Sin Eater\n                    ...              \n2223                      King Arthur\n2224                            Mulan\n2225                       Robin Hood\n2226    Robin Hood: Prince of Thieves\n2227                       Spiceworld\nName: Title, Length: 2228, dtype: object\n\n\nWhen extracting multiple columns, you need to place the column names inside a list.\n\nmovie_ratings[['Title', 'US Gross', 'Worldwide Gross' ]]\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n\n\n1\nMajor Dundee\n14873\n14873\n\n\n2\nThe Informers\n315000\n315000\n\n\n3\nBuffalo Soldiers\n353743\n353743\n\n\n4\nThe Last Sin Eater\n388390\n388390\n\n\n...\n...\n...\n...\n\n\n2223\nKing Arthur\n51877963\n203877963\n\n\n2224\nMulan\n120620254\n303500000\n\n\n2225\nRobin Hood\n105269730\n310885538\n\n\n2226\nRobin Hood: Prince of Thieves\n165493908\n390500000\n\n\n2227\nSpiceworld\n29342592\n56042592\n\n\n\n\n2228 rows × 3 columns\n\n\n\n\n\n5.3.2 Extracting Row(s)\n\n5.3.2.1 Extracting based on a Single Condition or Multiple Conditions\nIn many cases, we need to filter rows based on specific conditions or a combination of multiple conditions. Next, let’s explore how to use these conditions effectively to extract rows that meet our criteria, whether it’s a single condition or multiple conditions combined\n\n# extracting the rows that have IMDB Rating greater than 8\nmovie_ratings[movie_ratings['IMDB Rating'] &gt; 8]\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n21\nGandhi, My Father\n240425\n1375194\n5000000\nAug 03 2007\nOther\nAdapted screenplay\nDrama\nNon-Fiction\n8.1\n50881\n\n\n56\nEd Wood\n5828466\n5828466\n18000000\nSep 30 1994\nR\nAdapted screenplay\nComedy\nNon-Fiction\n8.1\n74171\n\n\n67\nRequiem for a Dream\n3635482\n7390108\n4500000\nOct 06 2000\nOther\nAdapted screenplay\nDrama\nFiction\n8.5\n185226\n\n\n164\nTrainspotting\n16501785\n24000785\n3100000\nJul 19 1996\nR\nAdapted screenplay\nDrama\nFiction\n8.2\n150483\n\n\n181\nThe Wizard of Oz\n28202232\n28202232\n2777000\nAug 25 2039\nG\nAdapted screenplay\nWestern/Musical\nFiction\n8.3\n102795\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2090\nFinding Nemo\n339714978\n867894287\n94000000\nMay 30 2003\nG\nOriginal Screenplay\nAction/Adventure\nFiction\n8.2\n165006\n\n\n2092\nToy Story 3\n410640665\n1046340665\n200000000\nJun 18 2010\nG\nOriginal Screenplay\nAction/Adventure\nFiction\n8.9\n67380\n\n\n2094\nAvatar\n760167650\n2767891499\n237000000\nDec 18 2009\nPG/PG-13\nOriginal Screenplay\nAction/Adventure\nFiction\n8.3\n261439\n\n\n2130\nScarface\n44942821\n44942821\n25000000\nDec 09 1983\nOther\nAdapted screenplay\nDrama\nFiction\n8.2\n152262\n\n\n2194\nThe Departed\n133311000\n290539042\n90000000\nOct 06 2006\nR\nAdapted screenplay\nDrama\nFiction\n8.5\n264148\n\n\n\n\n97 rows × 11 columns\n\n\n\nTo combine multiple conditions in pandas, you need to use the & (AND) and | (OR) operators. Make sure to enclose each condition in parentheses () for clarity and to ensure proper evaluation order.\n\n# extracting the rows that have IMDB Rating greater than 8 and US Gross less than 1000000\nmovie_ratings[(movie_ratings['IMDB Rating'] &gt; 8) & (movie_ratings['US Gross'] &lt; 1000000)]\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n21\nGandhi, My Father\n240425\n1375194\n5000000\nAug 03 2007\nOther\nAdapted screenplay\nDrama\nNon-Fiction\n8.1\n50881\n\n\n636\nLake of Fire\n25317\n25317\n6000000\nOct 03 2007\nOther\nAdapted screenplay\nDocumentary\nNon-Fiction\n8.4\n1027\n\n\n\n\n\n\n\n\n\n5.3.2.2 Negating Conditions with the ~ Operator\nThe tilde (~) is used for negating boolean conditions in pandas, making it a useful tool for excluding specific values or rows.\nIn some cases, we may want to extract rows that do not meet a specific condition. To accomplish this, we can use the ~ operator, which negates a condition. This operator allows us to filter data by excluding rows that satisfy a particular condition, making it a powerful tool for refining our queries. Let’s explore how to use the ~ operator to negate conditions and select rows that do not meet our criteria\n\n# Excluding the rows that have IMDB Rating that equals 8 using the tilde ~\nmovie_ratings[~(movie_ratings['IMDB Rating'] == 8)]\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\nNov 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n\n\n1\nMajor Dundee\n14873\n14873\n3800000\nApr 07 1965\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n\n\n2\nThe Informers\n315000\n315000\n18000000\nApr 24 2009\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n\n\n3\nBuffalo Soldiers\n353743\n353743\n15000000\nJul 25 2003\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n\n\n4\nThe Last Sin Eater\n388390\n388390\n2200000\nFeb 09 2007\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2223\nKing Arthur\n51877963\n203877963\n90000000\nJul 07 2004\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n6.2\n53106\n\n\n2224\nMulan\n120620254\n303500000\n90000000\nJun 19 1998\nG\nAdapted screenplay\nAction/Adventure\nNon-Fiction\n7.2\n34256\n\n\n2225\nRobin Hood\n105269730\n310885538\n210000000\nMay 14 2010\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n6.9\n34501\n\n\n2226\nRobin Hood: Prince of Thieves\n165493908\n390500000\n50000000\nJun 14 1991\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n6.7\n54480\n\n\n2227\nSpiceworld\n29342592\n56042592\n25000000\nJan 23 1998\nPG/PG-13\nAdapted screenplay\nComedy\nFiction\n2.9\n18010\n\n\n\n\n2191 rows × 11 columns\n\n\n\nAnother way to exclude rows where a column equals a certain value, you can use != to create the condition.\n\n# using the != operator\nmovie_ratings[movie_ratings['IMDB Rating'] != 8]\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\nNov 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n\n\n1\nMajor Dundee\n14873\n14873\n3800000\nApr 07 1965\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n\n\n2\nThe Informers\n315000\n315000\n18000000\nApr 24 2009\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n\n\n3\nBuffalo Soldiers\n353743\n353743\n15000000\nJul 25 2003\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n\n\n4\nThe Last Sin Eater\n388390\n388390\n2200000\nFeb 09 2007\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2223\nKing Arthur\n51877963\n203877963\n90000000\nJul 07 2004\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n6.2\n53106\n\n\n2224\nMulan\n120620254\n303500000\n90000000\nJun 19 1998\nG\nAdapted screenplay\nAction/Adventure\nNon-Fiction\n7.2\n34256\n\n\n2225\nRobin Hood\n105269730\n310885538\n210000000\nMay 14 2010\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n6.9\n34501\n\n\n2226\nRobin Hood: Prince of Thieves\n165493908\n390500000\n50000000\nJun 14 1991\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n6.7\n54480\n\n\n2227\nSpiceworld\n29342592\n56042592\n25000000\nJan 23 1998\nPG/PG-13\nAdapted screenplay\nComedy\nFiction\n2.9\n18010\n\n\n\n\n2191 rows × 11 columns\n\n\n\n\n\n\n5.3.3 Extracting Subsets of Rows and Columns\nSometimes we may be interested in working with a subset of rows and columns of the data, instead of working with the entire dataset. The indexing operators loc and iloc provide a convenient way of selecting a subset of desired rows and columns.\nLet us first sort the movie_ratings data frame by IMDB Rating.\n\nmovie_ratings_sorted = movie_ratings.sort_values(by = 'IMDB Rating', ascending = False)\nmovie_ratings_sorted.head()\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n182\nThe Shawshank Redemption\n28241469\n28241469\n25000000\nSep 23 1994\nR\nAdapted screenplay\nDrama\nFiction\n9.2\n519541\n\n\n2084\nInception\n285630280\n753830280\n160000000\nJul 16 2010\nPG/PG-13\nOriginal Screenplay\nHorror/Thriller\nFiction\n9.1\n188247\n\n\n790\nSchindler's List\n96067179\n321200000\n25000000\nDec 15 1993\nR\nAdapted screenplay\nDrama\nNon-Fiction\n8.9\n276283\n\n\n1962\nPulp Fiction\n107928762\n212928762\n8000000\nOct 14 1994\nR\nOriginal Screenplay\nDrama\nFiction\n8.9\n417703\n\n\n561\nThe Dark Knight\n533345358\n1022345358\n185000000\nJul 18 2008\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n8.9\n465000\n\n\n\n\n\n\n\n\n5.3.3.1 Subsetting the DataFrame by loc\nThe operator loc uses axis labels (row indices and column names) to subset the data.\nLet’s subset the title, worldwide gross, production budget, and IMDB raring of top 3 movies.\n\n# Subsetting the DataFrame by loc - using axis labels\nmovies_subset = movie_ratings_sorted.loc[[182,2084, 2092],[ 'Title', 'IMDB Rating', 'US Gross', 'Worldwide Gross', 'Production Budget']]\nmovies_subset\n\n\n\n\n\n\n\n\nTitle\nIMDB Rating\nUS Gross\nWorldwide Gross\nProduction Budget\n\n\n\n\n182\nThe Shawshank Redemption\n9.2\n28241469\n28241469\n25000000\n\n\n2084\nInception\n9.1\n285630280\n753830280\n160000000\n\n\n2092\nToy Story 3\n8.9\n410640665\n1046340665\n200000000\n\n\n\n\n\n\n\nThe : symbol in .loc is a slicing operator that represents a range or all elements in the specified dimension (rows or columns). Use : alone to select all rows/columns, or with start/end points to slice specific parts of the DataFrame.\n\n# Subsetting the DataFrame by loc - using axis labels. the colon is used to select all rows\nmovies_subset = movie_ratings_sorted.loc[:,['Title','Worldwide Gross','Production Budget','IMDB Rating']]\nmovies_subset\n\n\n\n\n\n\n\n\nTitle\nWorldwide Gross\nProduction Budget\nIMDB Rating\n\n\n\n\n182\nThe Shawshank Redemption\n28241469\n25000000\n9.2\n\n\n2084\nInception\n753830280\n160000000\n9.1\n\n\n790\nSchindler's List\n321200000\n25000000\n8.9\n\n\n1962\nPulp Fiction\n212928762\n8000000\n8.9\n\n\n561\nThe Dark Knight\n1022345358\n185000000\n8.9\n\n\n...\n...\n...\n...\n...\n\n\n1051\nGlitter\n4273372\n8500000\n2.0\n\n\n1495\nDisaster Movie\n34690901\n20000000\n1.7\n\n\n1116\nCrossover\n7009668\n5600000\n1.7\n\n\n805\nFrom Justin to Kelly\n4922166\n12000000\n1.6\n\n\n1147\nSuper Babies: Baby Geniuses 2\n9109322\n20000000\n1.4\n\n\n\n\n2228 rows × 4 columns\n\n\n\n\n# Subsetting the DataFrame by loc - using axis labels. the colon is used to select a range of rows\nmovies_subset = movie_ratings_sorted.loc[182:561,['Title','Worldwide Gross','Production Budget','IMDB Rating']]\nmovies_subset\n\n\n\n\n\n\n\n\nTitle\nWorldwide Gross\nProduction Budget\nIMDB Rating\n\n\n\n\n182\nThe Shawshank Redemption\n28241469\n25000000\n9.2\n\n\n2084\nInception\n753830280\n160000000\n9.1\n\n\n790\nSchindler's List\n321200000\n25000000\n8.9\n\n\n1962\nPulp Fiction\n212928762\n8000000\n8.9\n\n\n561\nThe Dark Knight\n1022345358\n185000000\n8.9\n\n\n\n\n\n\n\nCombining .loc with condition(s) to extract specific rows and columns based on criteria\n\n# extracting the rows that have IMDB Rating greater than 8 or US Gross less than 1000000, only extract the Title and IMDB Rating columns\nmovie_ratings[(movie_ratings['IMDB Rating'] &gt; 8) & (movie_ratings['US Gross'] &lt; 1000000)][['Title','IMDB Rating']]\n\n#using loc to extract the rows that have IMDB Rating greater than 8 or US Gross less than 1000000, only extract the Title and IMDB Rating columns\nmovie_ratings.loc[(movie_ratings['IMDB Rating'] &gt; 8) & (movie_ratings['US Gross'] &lt; 1000000),['Title','IMDB Rating']]\n\n\n\n\n\n\n\n\nTitle\nIMDB Rating\n\n\n\n\n21\nGandhi, My Father\n8.1\n\n\n636\nLake of Fire\n8.4\n\n\n\n\n\n\n\n\n\n5.3.3.2 Subsetting the DataFrame by iloc\nwhile iloc uses the position of rows or columns, where position has values 0,1,2,3,…and so on, for rows from top to bottom and columns from left to right. In other words, the first row has position 0, the second row has position 1, the third row has position 2, and so on. Similarly, the first column from left has position 0, the second column from left has position 1, the third column from left has position 2, and so on.\n\n\n\n\n\n\n# let's check  the movie_ratings_sorted DataFrame\nmovie_ratings_sorted.head()\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n182\nThe Shawshank Redemption\n28241469\n28241469\n25000000\nSep 23 1994\nR\nAdapted screenplay\nDrama\nFiction\n9.2\n519541\n\n\n2084\nInception\n285630280\n753830280\n160000000\nJul 16 2010\nPG/PG-13\nOriginal Screenplay\nHorror/Thriller\nFiction\n9.1\n188247\n\n\n790\nSchindler's List\n96067179\n321200000\n25000000\nDec 15 1993\nR\nAdapted screenplay\nDrama\nNon-Fiction\n8.9\n276283\n\n\n1962\nPulp Fiction\n107928762\n212928762\n8000000\nOct 14 1994\nR\nOriginal Screenplay\nDrama\nFiction\n8.9\n417703\n\n\n561\nThe Dark Knight\n533345358\n1022345358\n185000000\nJul 18 2008\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n8.9\n465000\n\n\n\n\n\n\n\nAfter sorting, the position-based index changes, while the label-based index remains unchanged. Let’s pass the position-based index to iloc to retrieve the top 2 rows from the movie_ratings_sorted DataFrame.\n\nmovie_ratings_sorted.iloc[0:2,:]\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n182\nThe Shawshank Redemption\n28241469\n28241469\n25000000\nSep 23 1994\nR\nAdapted screenplay\nDrama\nFiction\n9.2\n519541\n\n\n2084\nInception\n285630280\n753830280\n160000000\nJul 16 2010\nPG/PG-13\nOriginal Screenplay\nHorror/Thriller\nFiction\n9.1\n188247\n\n\n\n\n\n\n\nIt is important to note that the endpoint is excluded in an iloc slice.\nFor comparison, let’s pass the same argument to loc and see what it returns.\n\nmovie_ratings_sorted.loc[0:2,:]\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\nNov 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n\n\n851\nStar Trek: Generations\n75671262\n120000000\n38000000\nNov 18 1994\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n6.5\n26465\n\n\n140\nTuck Everlasting\n19161999\n19344615\n15000000\nOct 11 2002\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n6639\n\n\n708\nDe-Lovely\n13337299\n18396382\n4000000\nJun 25 2004\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n6086\n\n\n705\nFlyboys\n13090630\n14816379\n60000000\nSep 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nNon-Fiction\n6.5\n13934\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n955\nThe Brothers Solomon\n900926\n900926\n10000000\nSep 07 2007\nR\nOriginal Screenplay\nComedy\nFiction\n5.2\n6044\n\n\n1637\nDrumline\n56398162\n56398162\n20000000\nDec 13 2002\nPG/PG-13\nOriginal Screenplay\nComedy\nFiction\n5.2\n18165\n\n\n1610\nHollywood Homicide\n30207785\n51107785\n75000000\nJun 13 2003\nPG/PG-13\nOriginal Screenplay\nAction/Adventure\nFiction\n5.2\n16452\n\n\n569\nDoom\n28212337\n54612337\n70000000\nOct 21 2005\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n39473\n\n\n2\nThe Informers\n315000\n315000\n18000000\nApr 24 2009\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n\n\n\n\n812 rows × 11 columns\n\n\n\nAs you can see, : is used in the same way as in loc to denote a range of the index. All rows between label indices 0 and 2, inclusive of both ends, are returned.\n\n# Subsetting the DataFrame by iloc - using index of the position of rows and columns\nmovies_iloc_subset1 = movie_ratings_sorted.iloc[0:10,[0,2,3,9]]\nmovies_iloc_subset1\n\n\n\n\n\n\n\n\nTitle\nWorldwide Gross\nProduction Budget\nIMDB Rating\n\n\n\n\n182\nThe Shawshank Redemption\n28241469\n25000000\n9.2\n\n\n2084\nInception\n753830280\n160000000\n9.1\n\n\n2092\nToy Story 3\n1046340665\n200000000\n8.9\n\n\n1962\nPulp Fiction\n212928762\n8000000\n8.9\n\n\n790\nSchindler's List\n321200000\n25000000\n8.9\n\n\n561\nThe Dark Knight\n1022345358\n185000000\n8.9\n\n\n184\nCidade de Deus\n28763397\n3300000\n8.8\n\n\n487\nThe Lord of the Rings: The Fellowship of the Ring\n868621686\n109000000\n8.8\n\n\n497\nThe Lord of the Rings: The Return of the King\n1133027325\n94000000\n8.8\n\n\n1081\nC'era una volta il West\n5321508\n5000000\n8.8\n\n\n\n\n\n\n\nCan you use .iloc for conditional filtering, why or why not?\nTo recap, here are the Key differences betweenloc and iloc in pandas:\n\nIndexing Type:\n\nloc uses labels (names) for indexing.\niloc uses integer positions for indexing.\n\nInclusion of Endpoints:\n\nIn a loc slice, both endpoints are included.\nIn an iloc slice, the endpoint is excluded.\n\n\n\n\n\n5.3.4 Finding minimum/maximum of a column\nWhen working with pandas, there are two main options for locating the minimum or maximum values in a DataFrame column:\n\nidxmin() and idxmax(): return the index label of the first occurrence of the maximum or minimum value in a specified column.\n\n\n# movie_ratings_sorted.iloc[position_max_wgross,:]\nmax_index = movie_ratings_sorted['Worldwide Gross'].idxmax()\nmin_index = movie_ratings_sorted['Worldwide Gross'].idxmin()\nprint(\"Max index: \", max_index)\nprint(\"Min index: \", min_index)\n\nMax index:  2094\nMin index:  896\n\n\nidxmin() and idxmax() return the index label of the minimum or maximum value in a column. You can use these returned index labels with .loc to extract the corresponding row.\n\nprint(movie_ratings_sorted.loc[max_index,'Worldwide Gross'])\nprint(movie_ratings_sorted.loc[min_index,'Worldwide Gross'])\n\n2767891499\n884\n\n\n\nargmax() and argmin(): Return the integer position of the first occurrence of the maximum or minimum value in a column. You can use these integer positions with .iloc to extract the corresponding row\n\n\n# using argmax and argmin, which return the index of the maximum and minimum values\nmax_position = movie_ratings_sorted['Worldwide Gross'].argmax()\nmin_position = movie_ratings_sorted['Worldwide Gross'].argmin()\nprint(\"max position:\", max_position)\nprint(\"min position:\", min_position)\n\n# using iloc to get the row with the maximum and minimum values\nprint(movie_ratings_sorted.iloc[max_position, 2])\nprint(movie_ratings_sorted.iloc[min_position, 2])\n\nmax position: 48\nmin position: 2149\n2767891499\n884\n\n\nTips:\n\nWhen working with non-unique or custom indices, it is recommended to use idxmax() and idxmin() to retrieve index labels, as argmax() may be less intuitive in such scenarios.\n\nFor DataFrames, use .idxmax(axis=1) or .idxmin(axis=1) to find the index labels corresponding to the maximum or minimum values across rows, rather than columns.\n\n\n\n5.3.5 Finding the top n minimum/maximum values of a column\nTo find the top n minimum or maximum values of a column in a pandas DataFrame, you can use the nsmallest() and nlargest() methods. Here’s how you can do it:\n\n# find the top 3 movies with the highest worldwide gross\ntop_3_movies = movie_ratings.nlargest(3, 'Worldwide Gross')\ntop_3_movies\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n2094\nAvatar\n760167650\n2767891499\n237000000\nDec 18 2009\nPG/PG-13\nOriginal Screenplay\nAction/Adventure\nFiction\n8.3\n261439\n\n\n2093\nTitanic\n600788188\n1842879955\n200000000\nDec 19 1997\nPG/PG-13\nOriginal Screenplay\nHorror/Thriller\nFiction\n7.4\n240732\n\n\n497\nThe Lord of the Rings: The Return of the King\n377027325\n1133027325\n94000000\nDec 17 2003\nPG/PG-13\nAdapted screenplay\nAction/Adventure\nFiction\n8.8\n364077\n\n\n\n\n\n\n\nLet’s double check the result using the movie_ratings_sorted dataframe\n\nmovie_ratings_sorted['Worldwide Gross'].nlargest(3)\n\n2094    2767891499\n2093    1842879955\n497     1133027325\nName: Worldwide Gross, dtype: int64\n\n\nLet’s find the 3 movies with the smallest IMDb votes using the nsmallest method\n\nbottom_3_movies = movie_ratings.nsmallest(3, 'IMDB Votes')\nbottom_3_movies\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n1000\nTeeth\n347578\n2300349\n2000000\nJan 18 2008\nR\nOriginal Screenplay\nComedy\nFiction\n5.5\n18\n\n\n1261\nBirth\n5005899\n14603001\n20000000\nOct 29 2004\nR\nOriginal Screenplay\nDrama\nFiction\n6.3\n25\n\n\n1298\nCachÈ\n3647381\n17147381\n8000000\nDec 23 2005\nR\nOriginal Screenplay\nDrama\nFiction\n5.5\n26\n\n\n\n\n\n\n\nLet’s check the result using sort_values method\n\n# using the sort_values method\nmovie_ratings_sorted['IMDB Votes'].nsmallest(3)\n\n1000    18\n1261    25\n1298    26\nName: IMDB Votes, dtype: int64",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "Reading data.html#writing-data-to-a-.csv-file",
    "href": "Reading data.html#writing-data-to-a-.csv-file",
    "title": "5  Reading Data",
    "section": "5.4 Writing data to a .csv file",
    "text": "5.4 Writing data to a .csv file\nThe Pandas function to_csv can be used to write (or export) data to a csv. Below is an example.\n\n#Exporting the data of the top 250 movies to a csv file\nmovie_ratings.to_csv('../data/movie_rating_exported.csv')\n\n\n# check if the file has been exported\nos.listdir('../data')\n\n['bestseller_books.txt',\n 'country-capital-lat-long-population.csv',\n 'covid.csv',\n 'fifa_data.csv',\n 'food_quantity.csv',\n 'gas_prices.csv',\n 'gdp_lifeExpectancy.csv',\n 'LOTR 2.csv',\n 'LOTR.csv',\n 'movies.csv',\n 'movies_cleaned.csv',\n 'movie_ratings.csv',\n 'movie_ratings.txt',\n 'movie_rating_exported.csv',\n 'party_nyc.csv',\n 'price.csv',\n 'question_json_data.json',\n 'spotify_data.csv',\n 'stocks.csv']",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "Reading data.html#reading-other-data-formats---txt-html-json",
    "href": "Reading data.html#reading-other-data-formats---txt-html-json",
    "title": "5  Reading Data",
    "section": "5.5 Reading other data formats - txt, html, json",
    "text": "5.5 Reading other data formats - txt, html, json\nAlthough .csv is a very popular format for structured data, data is found in several other formats as well. Some of the other data formats are .txt, .html and .json.\n\n5.5.1 Reading .txt files\nThe txt format offers some additional flexibility as compared to the csv format. In the csv format, the delimiter is a comma (or the column values are separated by a comma). However, in a txt file, the delimiter can be anything as desired by the user. Let us read the file movie_ratings.txt, where the variable values are separated by a tab character.\n\nmovie_ratings_txt = pd.read_csv('../data/movie_ratings.txt',sep='\\t')\nmovie_ratings_txt.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\n0\nOpal Dreams\n14443\n14443\n9000000\nNov 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n\n\n1\n1\nMajor Dundee\n14873\n14873\n3800000\nApr 07 1965\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n\n\n2\n2\nThe Informers\n315000\n315000\n18000000\nApr 24 2009\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n\n\n3\n3\nBuffalo Soldiers\n353743\n353743\n15000000\nJul 25 2003\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n\n\n4\n4\nThe Last Sin Eater\n388390\n388390\n2200000\nFeb 09 2007\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\n\n\n\n\n\n\n\nWe use the function read_csv to read a txt file. However, we mention the tab character (r”) as a separator of variable values.\nNote that there is no need to remember the argument name - sep for specifying the delimiter. You can always refer to the read_csv() documentation to find the relevant argument.\n\n\n5.5.2 Reading HTML data\nThe Pandas function read_html searches for tabular data, i.e., data contained within the &lt;table&gt; tags of an html file. Let us read the tables in the GDP per capita page on Wikipedia.\n\n#Reading all the tables from the Wikipedia page on GDP per capita\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita')\n\nAll the tables will be read and stored in the variable named as tables. Let us find the datatype of the variable tables.\n\n#Finidng datatype of the variable - tables\ntype(tables)\n\nlist\n\n\nThe variable - tables is a list of all the tables read from the HTML data.\n\n#Number of tables read from the page\nlen(tables)\n\n6\n\n\nThe in-built function len can be used to find the length of the list - tables or the number of tables read from the Wikipedia page. Let us check out the first table.\n\n#Checking out the first table. Note that the index of the first table will be 0.\ntables[0]\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n&gt;$60,000 $50,000–$60,000 $40,000–$50,000 $30,0...\n$20,000–$30,000 $10,000–$20,000 $5,000–$10,000...\n$1,000–$2,500 $500–$1,000 &lt;$500 No data\n\n\n\n\n\n\n\nThe above table doesn’t seem to be useful. Let us check out the second table.\n\n#Checking out the second table. Note that the index of the first table will be 1.\ntables[1]\n\n\n\n\n\n\n\n\nCountry/Territory\nIMF[4][5]\nWorld Bank[6]\nUnited Nations[7]\n\n\n\nCountry/Territory\nEstimate\nYear\nEstimate\nYear\nEstimate\nYear\n\n\n\n\n0\nMonaco\n—\n—\n240862\n2022\n234317\n2021\n\n\n1\nLiechtenstein\n—\n—\n187267\n2022\n169260\n2021\n\n\n2\nLuxembourg\n131384\n2024\n128259\n2023\n133745\n2021\n\n\n3\nBermuda\n—\n—\n123091\n2022\n112653\n2021\n\n\n4\nIreland\n106059\n2024\n103685\n2023\n101109\n2021\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n218\nMalawi\n481\n2024\n673\n2023\n613\n2021\n\n\n219\nSouth Sudan\n422\n2024\n1072\n2015\n400\n2021\n\n\n220\nAfghanistan\n422\n2022\n353\n2022\n373\n2021\n\n\n221\nSyria\n—\n—\n421\n2021\n925\n2021\n\n\n222\nBurundi\n230\n2024\n200\n2023\n311\n2021\n\n\n\n\n223 rows × 7 columns\n\n\n\nThe above table contains the estimated GDP per capita of all countries. This is the table that is likely to be relevant to a user interested in analyzing GDP per capita of countries. Instead of reading all tables of an HTML file, we can focus the search to tables containing certain relevant keywords. Let us try searching all table containing the word ‘Country’.\n\n#Reading all the tables from the Wikipedia page on GDP per capita, containing the word 'Country'\ntables = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita', match = 'Country')\n\nThe match argument can be used to specify the keywords to be present in the table to be read.\n\nlen(tables)\n\n1\n\n\nOnly one table contains the keyword - ‘Country’. Let us check out the table obtained.\n\n#Table having the keyword - 'Country' from the HTML page\ntables[0]\n\n\n\n\n\n\n\n\nCountry/Territory\nIMF[4][5]\nWorld Bank[6]\nUnited Nations[7]\n\n\n\nCountry/Territory\nEstimate\nYear\nEstimate\nYear\nEstimate\nYear\n\n\n\n\n0\nMonaco\n—\n—\n240862\n2022\n234317\n2021\n\n\n1\nLiechtenstein\n—\n—\n187267\n2022\n169260\n2021\n\n\n2\nLuxembourg\n131384\n2024\n128259\n2023\n133745\n2021\n\n\n3\nBermuda\n—\n—\n123091\n2022\n112653\n2021\n\n\n4\nIreland\n106059\n2024\n103685\n2023\n101109\n2021\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n218\nMalawi\n481\n2024\n673\n2023\n613\n2021\n\n\n219\nSouth Sudan\n422\n2024\n1072\n2015\n400\n2021\n\n\n220\nAfghanistan\n422\n2022\n353\n2022\n373\n2021\n\n\n221\nSyria\n—\n—\n421\n2021\n925\n2021\n\n\n222\nBurundi\n230\n2024\n200\n2023\n311\n2021\n\n\n\n\n223 rows × 7 columns\n\n\n\nThe argument match helps with a more focussed search, and helps us discard irrelevant tables.\n\n\n5.5.3 Reading JSON data\nJSON stands for JavaScript Object Notation, in which the data is stored and transmitted as plain text. A couple of benefits of the JSON format are:\n\nSince the format is text only, JSON data can easily be exchanged between web applications, and used by any programming language.\nUnlike the csv format, JSON supports a hierarchical data structure, and is easier to integrate with APIs.\n\nThe JSON format can support a hierachical data structure, as it is built on the following two data structures (Source: technical documentation):\n\nA collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.\nAn ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.\n\nThese are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages also be based on these structures.\nThe Pandas function read_json converts a JSON string to a Pandas DataFrame. The function dumps() of the json library converts a Python object to a JSON string.\nLets read the JSON data on Ted Talks.\n\ntedtalks_data = pd.read_json('https://raw.githubusercontent.com/cwkenwaysun/TEDmap/master/data/TED_Talks.json')\n\n\ntedtalks_data.head()\n\n\n\n\n\n\n\n\nid\nspeaker\nheadline\nURL\ndescription\ntranscript_URL\nmonth_filmed\nyear_filmed\nevent\nduration\ndate_published\ntags\nnewURL\ndate\nviews\nrates\n\n\n\n\n0\n7\nDavid Pogue\nSimplicity sells\nhttp://www.ted.com/talks/view/id/7\nNew York Times columnist David Pogue takes aim...\nhttp://www.ted.com/talks/view/id/7/transcript?...\n2\n2006\nTED2006\n0:21:26\n6/27/06\nsimplicity,computers,software,interface design...\nhttps://www.ted.com/talks/david_pogue_says_sim...\n2006-06-27\n1646773\n[{'id': 7, 'name': 'Funny', 'count': 968}, {'i...\n\n\n1\n6\nCraig Venter\nSampling the ocean's DNA\nhttp://www.ted.com/talks/view/id/6\nGenomics pioneer Craig Venter takes a break fr...\nhttp://www.ted.com/talks/view/id/6/transcript?...\n7\n2005\nTEDGlobal 2005\n0:16:51\n2004/05/07\nbiotech,invention,oceans,genetics,DNA,biology,...\nhttps://www.ted.com/talks/craig_venter_on_dna_...\n2004-05-07\n562625\n[{'id': 3, 'name': 'Courageous', 'count': 21},...\n\n\n2\n4\nBurt Rutan\nThe real future of space exploration\nhttp://www.ted.com/talks/view/id/4\nIn this passionate talk, legendary spacecraft ...\nhttp://www.ted.com/talks/view/id/4/transcript?...\n2\n2006\nTED2006\n0:19:37\n10/25/06\naircraft,flight,industrial design,NASA,rocket ...\nhttps://www.ted.com/talks/burt_rutan_sees_the_...\n2006-10-25\n2046869\n[{'id': 3, 'name': 'Courageous', 'count': 169}...\n\n\n3\n3\nAshraf Ghani\nHow to rebuild a broken state\nhttp://www.ted.com/talks/view/id/3\nAshraf Ghani's passionate and powerful 10-minu...\nhttp://www.ted.com/talks/view/id/3/transcript?...\n7\n2005\nTEDGlobal 2005\n0:18:45\n10/18/06\ncorruption,poverty,economics,investment,milita...\nhttps://www.ted.com/talks/ashraf_ghani_on_rebu...\n2006-10-18\n814554\n[{'id': 3, 'name': 'Courageous', 'count': 140}...\n\n\n4\n5\nChris Bangle\nGreat cars are great art\nhttp://www.ted.com/talks/view/id/5\nAmerican designer Chris Bangle explains his ph...\nhttp://www.ted.com/talks/view/id/5/transcript?...\n2\n2002\nTED2002\n0:20:04\n2004/05/07\ncars,industrial design,transportation,inventio...\nhttps://www.ted.com/talks/chris_bangle_says_gr...\n2004-05-07\n870950\n[{'id': 1, 'name': 'Beautiful', 'count': 89}, ...\n\n\n\n\n\n\n\n\n\n[{'question': \"What is the data type of values in the last column (named 'rates') of the above dataset on ted talks\",\n  'type': 'multiple_choice',\n  'answers': [{'answer': 'list', 'correct': True, 'feedback': 'Correct!'},\n   {'answer': 'string',\n    'correct': False,\n    'feedback': 'Incorrect. Use the type function on the variable to find its datatype.'},\n   {'answer': 'numeric',\n    'correct': False,\n    'feedback': 'Incorrect. Use the type function on the variable to find its datatype.'},\n   {'answer': 'dictionary',\n    'correct': False,\n    'feedback': 'Incorrect. Use the type function on the variable to find its datatype.'}]}]\n\n\nThis JSON data contains nested structures, such as lists and dictionaries, which require a deeper understanding to effectively structure. We will address this in future lectures\n\n\n5.5.4 Reading data from a URL in Python\nThis process typically involves using the requests library, which allows you to send HTTP requests and handle responses easily.\nYou’ll need to install it using pip\nWe’ll use the CoinGecko API, which provides cryptocurrency market data. Here’s an example of how to retrieve current market data:\n\nimport requests\n\n# Define the URL of the API\nurl = 'https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd'\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the JSON data\n    data = response.json()\n    print(data)\nelse:\n    print(f\"Failed to retrieve data: {response.status_code}\")\n\n[{'id': 'bitcoin', 'symbol': 'btc', 'name': 'Bitcoin', 'image': 'https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400', 'current_price': 62490, 'market_cap': 1235032675967, 'market_cap_rank': 1, 'fully_diluted_valuation': 1312231173412, 'total_volume': 34554624888, 'high_24h': 64500, 'low_24h': 62100, 'price_change_24h': -1170.466984852057, 'price_change_percentage_24h': -1.83862, 'market_cap_change_24h': -23150749922.800293, 'market_cap_change_percentage_24h': -1.84001, 'circulating_supply': 19764571.0, 'total_supply': 21000000.0, 'max_supply': 21000000.0, 'ath': 73738, 'ath_change_percentage': -15.21931, 'ath_date': '2024-03-14T07:10:36.635Z', 'atl': 67.81, 'atl_change_percentage': 92093.56211, 'atl_date': '2013-07-06T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.271Z'}, {'id': 'ethereum', 'symbol': 'eth', 'name': 'Ethereum', 'image': 'https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628', 'current_price': 2434.1, 'market_cap': 293023922868, 'market_cap_rank': 2, 'fully_diluted_valuation': 293018397207, 'total_volume': 16968707061, 'high_24h': 2518.07, 'low_24h': 2405.58, 'price_change_24h': -53.64323369453086, 'price_change_percentage_24h': -2.1563, 'market_cap_change_24h': -6417354167.597656, 'market_cap_change_percentage_24h': -2.14311, 'circulating_supply': 120379113.24004, 'total_supply': 120376843.206498, 'max_supply': None, 'ath': 4878.26, 'ath_change_percentage': -50.02647, 'ath_date': '2021-11-10T14:24:19.604Z', 'atl': 0.432979, 'atl_change_percentage': 562938.84293, 'atl_date': '2015-10-20T00:00:00.000Z', 'roi': {'times': 51.08601756823208, 'currency': 'btc', 'percentage': 5108.601756823207}, 'last_updated': '2024-10-08T00:54:07.565Z'}, {'id': 'tether', 'symbol': 'usdt', 'name': 'Tether', 'image': 'https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661', 'current_price': 0.999711, 'market_cap': 119827577712, 'market_cap_rank': 3, 'fully_diluted_valuation': 119827577712, 'total_volume': 55698442555, 'high_24h': 1.004, 'low_24h': 0.99633, 'price_change_24h': -0.001198196616044811, 'price_change_percentage_24h': -0.11971, 'market_cap_change_24h': -23449752.259277344, 'market_cap_change_percentage_24h': -0.01957, 'circulating_supply': 119857245003.333, 'total_supply': 119857245003.333, 'max_supply': None, 'ath': 1.32, 'ath_change_percentage': -24.39102, 'ath_date': '2018-07-24T00:00:00.000Z', 'atl': 0.572521, 'atl_change_percentage': 74.73226, 'atl_date': '2015-03-02T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:12.212Z'}, {'id': 'binancecoin', 'symbol': 'bnb', 'name': 'BNB', 'image': 'https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970', 'current_price': 568.59, 'market_cap': 82947258162, 'market_cap_rank': 4, 'fully_diluted_valuation': 82947258162, 'total_volume': 799134450, 'high_24h': 582.45, 'low_24h': 563.77, 'price_change_24h': -7.260884048894809, 'price_change_percentage_24h': -1.2609, 'market_cap_change_24h': -1068798750.590683, 'market_cap_change_percentage_24h': -1.27214, 'circulating_supply': 145887575.79, 'total_supply': 145887575.79, 'max_supply': 200000000.0, 'ath': 717.48, 'ath_change_percentage': -20.80519, 'ath_date': '2024-06-06T14:10:59.816Z', 'atl': 0.0398177, 'atl_change_percentage': 1426911.78115, 'atl_date': '2017-10-19T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:13.445Z'}, {'id': 'solana', 'symbol': 'sol', 'name': 'Solana', 'image': 'https://coin-images.coingecko.com/coins/images/4128/large/solana.png?1718769756', 'current_price': 144.62, 'market_cap': 67873121370, 'market_cap_rank': 5, 'fully_diluted_valuation': 84775812388, 'total_volume': 3237160849, 'high_24h': 151.52, 'low_24h': 143.59, 'price_change_24h': -4.597486919219733, 'price_change_percentage_24h': -3.081, 'market_cap_change_24h': -2171285003.411751, 'market_cap_change_percentage_24h': -3.09987, 'circulating_supply': 469265730.374483, 'total_supply': 586128687.109744, 'max_supply': None, 'ath': 259.96, 'ath_change_percentage': -44.35227, 'ath_date': '2021-11-06T21:54:35.825Z', 'atl': 0.500801, 'atl_change_percentage': 28785.99929, 'atl_date': '2020-05-11T19:35:23.449Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.774Z'}, {'id': 'usd-coin', 'symbol': 'usdc', 'name': 'USDC', 'image': 'https://coin-images.coingecko.com/coins/images/6319/large/usdc.png?1696506694', 'current_price': 0.99982, 'market_cap': 35336575331, 'market_cap_rank': 6, 'fully_diluted_valuation': 35336582362, 'total_volume': 7937781611, 'high_24h': 1.003, 'low_24h': 0.996693, 'price_change_24h': -0.000988042958259383, 'price_change_percentage_24h': -0.09872, 'market_cap_change_24h': -295654890.59819794, 'market_cap_change_percentage_24h': -0.82974, 'circulating_supply': 35344977832.893, 'total_supply': 35344984865.7762, 'max_supply': None, 'ath': 1.17, 'ath_change_percentage': -14.68676, 'ath_date': '2019-05-08T00:40:28.300Z', 'atl': 0.877647, 'atl_change_percentage': 13.99504, 'atl_date': '2023-03-11T08:02:13.981Z', 'roi': None, 'last_updated': '2024-10-08T00:54:12.579Z'}, {'id': 'ripple', 'symbol': 'xrp', 'name': 'XRP', 'image': 'https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442', 'current_price': 0.531761, 'market_cap': 30080966724, 'market_cap_rank': 7, 'fully_diluted_valuation': 53173544466, 'total_volume': 1488925609, 'high_24h': 0.54488, 'low_24h': 0.529092, 'price_change_24h': -0.006764730959881505, 'price_change_percentage_24h': -1.25616, 'market_cap_change_24h': -380244778.19135284, 'market_cap_change_percentage_24h': -1.24829, 'circulating_supply': 56564039920.0, 'total_supply': 99987161962.0, 'max_supply': 100000000000.0, 'ath': 3.4, 'ath_change_percentage': -84.34345, 'ath_date': '2018-01-07T00:00:00.000Z', 'atl': 0.00268621, 'atl_change_percentage': 19707.86295, 'atl_date': '2014-05-22T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.049Z'}, {'id': 'staked-ether', 'symbol': 'steth', 'name': 'Lido Staked Ether', 'image': 'https://coin-images.coingecko.com/coins/images/13442/large/steth_logo.png?1696513206', 'current_price': 2433.54, 'market_cap': 23774301241, 'market_cap_rank': 8, 'fully_diluted_valuation': 23774301241, 'total_volume': 55393440, 'high_24h': 2515.61, 'low_24h': 2405.33, 'price_change_24h': -52.13318341543163, 'price_change_percentage_24h': -2.09734, 'market_cap_change_24h': -622209584.8816986, 'market_cap_change_percentage_24h': -2.5504, 'circulating_supply': 9767026.36285491, 'total_supply': 9767026.36285491, 'max_supply': None, 'ath': 4829.57, 'ath_change_percentage': -49.57783, 'ath_date': '2021-11-10T14:40:47.256Z', 'atl': 482.9, 'atl_change_percentage': 404.28528, 'atl_date': '2020-12-22T04:08:21.854Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.810Z'}, {'id': 'dogecoin', 'symbol': 'doge', 'name': 'Dogecoin', 'image': 'https://coin-images.coingecko.com/coins/images/5/large/dogecoin.png?1696501409', 'current_price': 0.109138, 'market_cap': 15967242014, 'market_cap_rank': 9, 'fully_diluted_valuation': 15967846781, 'total_volume': 994792895, 'high_24h': 0.11513, 'low_24h': 0.108236, 'price_change_24h': -0.004391041308727209, 'price_change_percentage_24h': -3.86778, 'market_cap_change_24h': -629528269.0053463, 'market_cap_change_percentage_24h': -3.79308, 'circulating_supply': 146268586383.705, 'total_supply': 146274126383.705, 'max_supply': None, 'ath': 0.731578, 'ath_change_percentage': -85.08287, 'ath_date': '2021-05-08T05:08:23.458Z', 'atl': 8.69e-05, 'atl_change_percentage': 125476.21875, 'atl_date': '2015-05-06T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.023Z'}, {'id': 'tron', 'symbol': 'trx', 'name': 'TRON', 'image': 'https://coin-images.coingecko.com/coins/images/1094/large/tron-logo.png?1696502193', 'current_price': 0.156189, 'market_cap': 13526695367, 'market_cap_rank': 10, 'fully_diluted_valuation': 13526702713, 'total_volume': 382377142, 'high_24h': 0.156897, 'low_24h': 0.153597, 'price_change_24h': 0.00148677, 'price_change_percentage_24h': 0.96105, 'market_cap_change_24h': 119102470, 'market_cap_change_percentage_24h': 0.88832, 'circulating_supply': 86577032111.108, 'total_supply': 86577079124.0223, 'max_supply': None, 'ath': 0.231673, 'ath_change_percentage': -32.53334, 'ath_date': '2018-01-05T00:00:00.000Z', 'atl': 0.00180434, 'atl_change_percentage': 8562.54313, 'atl_date': '2017-11-12T00:00:00.000Z', 'roi': {'times': 81.20479655054011, 'currency': 'usd', 'percentage': 8120.47965505401}, 'last_updated': '2024-10-08T00:54:05.672Z'}, {'id': 'the-open-network', 'symbol': 'ton', 'name': 'Toncoin', 'image': 'https://coin-images.coingecko.com/coins/images/17980/large/photo_2024-09-10_17.09.00.jpeg?1725963446', 'current_price': 5.23, 'market_cap': 13266378590, 'market_cap_rank': 11, 'fully_diluted_valuation': 26745582651, 'total_volume': 289898643, 'high_24h': 5.38, 'low_24h': 5.19, 'price_change_24h': -0.11581596106146907, 'price_change_percentage_24h': -2.1656, 'market_cap_change_24h': -292964162.0925293, 'market_cap_change_percentage_24h': -2.16061, 'circulating_supply': 2536031815.95331, 'total_supply': 5112747844.44798, 'max_supply': None, 'ath': 8.25, 'ath_change_percentage': -36.61515, 'ath_date': '2024-06-15T00:36:51.509Z', 'atl': 0.519364, 'atl_change_percentage': 907.29703, 'atl_date': '2021-09-21T00:33:11.092Z', 'roi': None, 'last_updated': '2024-10-08T00:54:13.375Z'}, {'id': 'cardano', 'symbol': 'ada', 'name': 'Cardano', 'image': 'https://coin-images.coingecko.com/coins/images/975/large/cardano.png?1696502090', 'current_price': 0.35311, 'market_cap': 12609866405, 'market_cap_rank': 12, 'fully_diluted_valuation': 15891641222, 'total_volume': 323896654, 'high_24h': 0.36763, 'low_24h': 0.350159, 'price_change_24h': -0.007715970856781273, 'price_change_percentage_24h': -2.13842, 'market_cap_change_24h': -274870486.73763466, 'market_cap_change_percentage_24h': -2.1333, 'circulating_supply': 35707072684.8988, 'total_supply': 45000000000.0, 'max_supply': 45000000000.0, 'ath': 3.09, 'ath_change_percentage': -88.54235, 'ath_date': '2021-09-02T06:00:10.474Z', 'atl': 0.01925275, 'atl_change_percentage': 1737.07582, 'atl_date': '2020-03-13T02:22:55.044Z', 'roi': None, 'last_updated': '2024-10-08T00:54:06.319Z'}, {'id': 'avalanche-2', 'symbol': 'avax', 'name': 'Avalanche', 'image': 'https://coin-images.coingecko.com/coins/images/12559/large/Avalanche_Circle_RedWhite_Trans.png?1696512369', 'current_price': 26.86, 'market_cap': 10918471696, 'market_cap_rank': 13, 'fully_diluted_valuation': 11993041109, 'total_volume': 383447410, 'high_24h': 27.69, 'low_24h': 26.52, 'price_change_24h': -0.30149710548148434, 'price_change_percentage_24h': -1.10995, 'market_cap_change_24h': -122018390.50128365, 'market_cap_change_percentage_24h': -1.10519, 'circulating_supply': 406462834.22799, 'total_supply': 446465917.213367, 'max_supply': 720000000.0, 'ath': 144.96, 'ath_change_percentage': -81.48341, 'ath_date': '2021-11-21T14:18:56.538Z', 'atl': 2.8, 'atl_change_percentage': 858.27418, 'atl_date': '2020-12-31T13:15:21.540Z', 'roi': None, 'last_updated': '2024-10-08T00:54:04.331Z'}, {'id': 'shiba-inu', 'symbol': 'shib', 'name': 'Shiba Inu', 'image': 'https://coin-images.coingecko.com/coins/images/11939/large/shiba.png?1696511800', 'current_price': 1.759e-05, 'market_cap': 10365613559, 'market_cap_rank': 14, 'fully_diluted_valuation': 17590632069, 'total_volume': 791497323, 'high_24h': 1.873e-05, 'low_24h': 1.746e-05, 'price_change_24h': -9.50643038621e-07, 'price_change_percentage_24h': -5.12756, 'market_cap_change_24h': -564457717.0176163, 'market_cap_change_percentage_24h': -5.16426, 'circulating_supply': 589258560734548.0, 'total_supply': 999982343215162.0, 'max_supply': None, 'ath': 8.616e-05, 'ath_change_percentage': -79.56469, 'ath_date': '2021-10-28T03:54:55.568Z', 'atl': 5.6366e-11, 'atl_change_percentage': 31236278.95797, 'atl_date': '2020-11-28T11:26:25.838Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.049Z'}, {'id': 'wrapped-steth', 'symbol': 'wsteth', 'name': 'Wrapped stETH', 'image': 'https://coin-images.coingecko.com/coins/images/18834/large/wstETH.png?1696518295', 'current_price': 2870.49, 'market_cap': 10083391703, 'market_cap_rank': 15, 'fully_diluted_valuation': 10083391703, 'total_volume': 57354973, 'high_24h': 2967.08, 'low_24h': 2856.16, 'price_change_24h': -65.57570573917383, 'price_change_percentage_24h': -2.23345, 'market_cap_change_24h': -183962777.72447205, 'market_cap_change_percentage_24h': -1.79173, 'circulating_supply': 3515218.76797872, 'total_supply': 3515218.76797872, 'max_supply': None, 'ath': 7256.02, 'ath_change_percentage': -60.4339, 'ath_date': '2022-05-13T15:09:54.509Z', 'atl': 558.54, 'atl_change_percentage': 414.00503, 'atl_date': '2022-05-13T01:36:45.053Z', 'roi': None, 'last_updated': '2024-10-08T00:54:12.928Z'}, {'id': 'wrapped-bitcoin', 'symbol': 'wbtc', 'name': 'Wrapped Bitcoin', 'image': 'https://coin-images.coingecko.com/coins/images/7598/large/wrapped_bitcoin_wbtc.png?1696507857', 'current_price': 62339, 'market_cap': 9370148390, 'market_cap_rank': 16, 'fully_diluted_valuation': 9370148390, 'total_volume': 173837802, 'high_24h': 64104, 'low_24h': 62051, 'price_change_24h': -1114.1613043298566, 'price_change_percentage_24h': -1.75588, 'market_cap_change_24h': -212142102.48479843, 'market_cap_change_percentage_24h': -2.2139, 'circulating_supply': 150281.33314858, 'total_supply': 150281.33314858, 'max_supply': 150281.33314858, 'ath': 73505, 'ath_change_percentage': -15.19126, 'ath_date': '2024-03-14T07:10:23.403Z', 'atl': 3139.17, 'atl_change_percentage': 1885.84407, 'atl_date': '2019-04-02T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:12.624Z'}, {'id': 'weth', 'symbol': 'weth', 'name': 'WETH', 'image': 'https://coin-images.coingecko.com/coins/images/2518/large/weth.png?1696503332', 'current_price': 2434.35, 'market_cap': 7231757765, 'market_cap_rank': 17, 'fully_diluted_valuation': 7231757765, 'total_volume': 421839130, 'high_24h': 2514.46, 'low_24h': 2412.75, 'price_change_24h': -52.1605917937668, 'price_change_percentage_24h': -2.09774, 'market_cap_change_24h': -167754303.50990772, 'market_cap_change_percentage_24h': -2.2671, 'circulating_supply': 2973433.46470016, 'total_supply': 2973433.46470016, 'max_supply': None, 'ath': 4799.89, 'ath_change_percentage': -49.27022, 'ath_date': '2021-11-09T00:00:00.000Z', 'atl': 82.1, 'atl_change_percentage': 2865.74184, 'atl_date': '2018-12-15T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:04.743Z'}, {'id': 'chainlink', 'symbol': 'link', 'name': 'Chainlink', 'image': 'https://coin-images.coingecko.com/coins/images/877/large/chainlink-new-logo.png?1696502009', 'current_price': 11.22, 'market_cap': 7034017951, 'market_cap_rank': 18, 'fully_diluted_valuation': 11221214442, 'total_volume': 429742466, 'high_24h': 11.74, 'low_24h': 11.11, 'price_change_24h': -0.20415057957979954, 'price_change_percentage_24h': -1.78697, 'market_cap_change_24h': -127373629.90058899, 'market_cap_change_percentage_24h': -1.77862, 'circulating_supply': 626849971.3083414, 'total_supply': 1000000000.0, 'max_supply': 1000000000.0, 'ath': 52.7, 'ath_change_percentage': -78.69639, 'ath_date': '2021-05-10T00:13:57.214Z', 'atl': 0.148183, 'atl_change_percentage': 7475.94449, 'atl_date': '2017-11-29T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:05.960Z'}, {'id': 'bitcoin-cash', 'symbol': 'bch', 'name': 'Bitcoin Cash', 'image': 'https://coin-images.coingecko.com/coins/images/780/large/bitcoin-cash-circle.png?1696501932', 'current_price': 325.66, 'market_cap': 6438369641, 'market_cap_rank': 19, 'fully_diluted_valuation': 6838542455, 'total_volume': 245664732, 'high_24h': 332.33, 'low_24h': 321.68, 'price_change_24h': -2.1671119914790893, 'price_change_percentage_24h': -0.66106, 'market_cap_change_24h': -44835556.588739395, 'market_cap_change_percentage_24h': -0.69156, 'circulating_supply': 19771137.3966508, 'total_supply': 21000000.0, 'max_supply': 21000000.0, 'ath': 3785.82, 'ath_change_percentage': -91.40438, 'ath_date': '2017-12-20T00:00:00.000Z', 'atl': 76.93, 'atl_change_percentage': 322.97489, 'atl_date': '2018-12-16T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.631Z'}, {'id': 'polkadot', 'symbol': 'dot', 'name': 'Polkadot', 'image': 'https://coin-images.coingecko.com/coins/images/12171/large/polkadot.png?1696512008', 'current_price': 4.15, 'market_cap': 5904726419, 'market_cap_rank': 20, 'fully_diluted_valuation': 6240862690, 'total_volume': 179496820, 'high_24h': 4.27, 'low_24h': 4.13, 'price_change_24h': -0.061744048750748355, 'price_change_percentage_24h': -1.46434, 'market_cap_change_24h': -89644363.9776411, 'market_cap_change_percentage_24h': -1.49548, 'circulating_supply': 1421990197.81355, 'total_supply': 1502939330.71355, 'max_supply': None, 'ath': 54.98, 'ath_change_percentage': -92.44717, 'ath_date': '2021-11-04T14:10:09.301Z', 'atl': 2.7, 'atl_change_percentage': 53.94571, 'atl_date': '2020-08-20T05:48:11.359Z', 'roi': None, 'last_updated': '2024-10-08T00:54:04.800Z'}, {'id': 'dai', 'symbol': 'dai', 'name': 'Dai', 'image': 'https://coin-images.coingecko.com/coins/images/9956/large/Badge_Dai.png?1696509996', 'current_price': 0.999811, 'market_cap': 5881292215, 'market_cap_rank': 21, 'fully_diluted_valuation': 5881798975, 'total_volume': 63258183, 'high_24h': 1.003, 'low_24h': 0.994743, 'price_change_24h': -0.000764124895492824, 'price_change_percentage_24h': -0.07637, 'market_cap_change_24h': 3577027, 'market_cap_change_percentage_24h': 0.06086, 'circulating_supply': 5882619286.90039, 'total_supply': 5883126161.262, 'max_supply': None, 'ath': 1.22, 'ath_change_percentage': -17.94271, 'ath_date': '2020-03-13T03:02:50.373Z', 'atl': 0.88196, 'atl_change_percentage': 13.40894, 'atl_date': '2023-03-11T07:50:50.514Z', 'roi': None, 'last_updated': '2024-10-08T00:54:06.710Z'}, {'id': 'sui', 'symbol': 'sui', 'name': 'Sui', 'image': 'https://coin-images.coingecko.com/coins/images/26375/large/sui-ocean-square.png?1727791290', 'current_price': 2.06, 'market_cap': 5685791571, 'market_cap_rank': 22, 'fully_diluted_valuation': 20572061868, 'total_volume': 2077001293, 'high_24h': 2.14, 'low_24h': 1.87, 'price_change_24h': 0.154302, 'price_change_percentage_24h': 8.10887, 'market_cap_change_24h': 415591222, 'market_cap_change_percentage_24h': 7.88568, 'circulating_supply': 2763841372.60889, 'total_supply': 10000000000.0, 'max_supply': 10000000000.0, 'ath': 2.17, 'ath_change_percentage': -5.01098, 'ath_date': '2024-03-27T17:46:02.608Z', 'atl': 0.364846, 'atl_change_percentage': 465.24479, 'atl_date': '2023-10-19T10:40:30.078Z', 'roi': None, 'last_updated': '2024-10-08T00:54:04.265Z'}, {'id': 'near', 'symbol': 'near', 'name': 'NEAR Protocol', 'image': 'https://coin-images.coingecko.com/coins/images/10365/large/near.jpg?1696510367', 'current_price': 5.11, 'market_cap': 5660830269, 'market_cap_rank': 23, 'fully_diluted_valuation': 6049736931, 'total_volume': 576314968, 'high_24h': 5.31, 'low_24h': 5.01, 'price_change_24h': 0.095049, 'price_change_percentage_24h': 1.89681, 'market_cap_change_24h': 113909251, 'market_cap_change_percentage_24h': 2.05356, 'circulating_supply': 1107181322.94878, 'total_supply': 1183246170.6779, 'max_supply': None, 'ath': 20.44, 'ath_change_percentage': -75.01369, 'ath_date': '2022-01-16T22:09:45.873Z', 'atl': 0.526762, 'atl_change_percentage': 869.45631, 'atl_date': '2020-11-04T16:09:15.137Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.280Z'}, {'id': 'leo-token', 'symbol': 'leo', 'name': 'LEO Token', 'image': 'https://coin-images.coingecko.com/coins/images/8418/large/leo-token.png?1696508607', 'current_price': 6.01, 'market_cap': 5560203959, 'market_cap_rank': 24, 'fully_diluted_valuation': 5920434513, 'total_volume': 801407, 'high_24h': 6.05, 'low_24h': 5.93, 'price_change_24h': -0.027411011021039933, 'price_change_percentage_24h': -0.45384, 'market_cap_change_24h': -24784317.658293724, 'market_cap_change_percentage_24h': -0.44377, 'circulating_supply': 925292320.9, 'total_supply': 985239504.0, 'max_supply': None, 'ath': 8.14, 'ath_change_percentage': -25.84331, 'ath_date': '2022-02-08T17:40:10.285Z', 'atl': 0.799859, 'atl_change_percentage': 654.24544, 'atl_date': '2019-12-24T15:14:35.376Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.729Z'}, {'id': 'uniswap', 'symbol': 'uni', 'name': 'Uniswap', 'image': 'https://coin-images.coingecko.com/coins/images/12504/large/uniswap-logo.png?1720676669', 'current_price': 7.24, 'market_cap': 5464326017, 'market_cap_rank': 25, 'fully_diluted_valuation': 7249360122, 'total_volume': 231484762, 'high_24h': 7.46, 'low_24h': 7.17, 'price_change_24h': 0.0370806, 'price_change_percentage_24h': 0.5151, 'market_cap_change_24h': 40188389, 'market_cap_change_percentage_24h': 0.74092, 'circulating_supply': 753766667.0, 'total_supply': 1000000000.0, 'max_supply': 1000000000.0, 'ath': 44.92, 'ath_change_percentage': -83.84698, 'ath_date': '2021-05-03T05:25:04.822Z', 'atl': 1.03, 'atl_change_percentage': 604.31268, 'atl_date': '2020-09-17T01:20:38.214Z', 'roi': None, 'last_updated': '2024-10-08T00:54:12.072Z'}, {'id': 'litecoin', 'symbol': 'ltc', 'name': 'Litecoin', 'image': 'https://coin-images.coingecko.com/coins/images/2/large/litecoin.png?1696501400', 'current_price': 65.04, 'market_cap': 4881816975, 'market_cap_rank': 26, 'fully_diluted_valuation': 5463392544, 'total_volume': 332840383, 'high_24h': 67.87, 'low_24h': 64.44, 'price_change_24h': -2.5680815804710733, 'price_change_percentage_24h': -3.79825, 'market_cap_change_24h': -192122729.2591772, 'market_cap_change_percentage_24h': -3.78646, 'circulating_supply': 75058239.4834713, 'total_supply': 84000000.0, 'max_supply': 84000000.0, 'ath': 410.26, 'ath_change_percentage': -84.11656, 'ath_date': '2021-05-10T03:13:07.904Z', 'atl': 1.15, 'atl_change_percentage': 5572.09067, 'atl_date': '2015-01-14T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:09.994Z'}, {'id': 'bittensor', 'symbol': 'tao', 'name': 'Bittensor', 'image': 'https://coin-images.coingecko.com/coins/images/28452/large/ARUsPeNQ_400x400.jpeg?1696527447', 'current_price': 617.07, 'market_cap': 4554478084, 'market_cap_rank': 27, 'fully_diluted_valuation': 12958100764, 'total_volume': 305713998, 'high_24h': 671.85, 'low_24h': 601.5, 'price_change_24h': -54.06723446710225, 'price_change_percentage_24h': -8.05608, 'market_cap_change_24h': -334381577.4141588, 'market_cap_change_percentage_24h': -6.83966, 'circulating_supply': 7381023.0, 'total_supply': 21000000.0, 'max_supply': 21000000.0, 'ath': 757.6, 'ath_change_percentage': -18.06424, 'ath_date': '2024-03-07T18:45:36.466Z', 'atl': 30.83, 'atl_change_percentage': 1913.46097, 'atl_date': '2023-05-14T08:57:53.732Z', 'roi': None, 'last_updated': '2024-10-08T00:54:05.434Z'}, {'id': 'aptos', 'symbol': 'apt', 'name': 'Aptos', 'image': 'https://coin-images.coingecko.com/coins/images/26455/large/aptos_round.png?1696525528', 'current_price': 8.91, 'market_cap': 4482021558, 'market_cap_rank': 28, 'fully_diluted_valuation': 9975514711, 'total_volume': 559303098, 'high_24h': 9.29, 'low_24h': 8.46, 'price_change_24h': -0.05442955762588575, 'price_change_percentage_24h': -0.6072, 'market_cap_change_24h': -22231350.747354507, 'market_cap_change_percentage_24h': -0.49356, 'circulating_supply': 503104224.342875, 'total_supply': 1119745526.9904, 'max_supply': None, 'ath': 19.92, 'ath_change_percentage': -55.28954, 'ath_date': '2023-01-26T14:25:17.390Z', 'atl': 3.08, 'atl_change_percentage': 189.19136, 'atl_date': '2022-12-29T21:35:14.796Z', 'roi': None, 'last_updated': '2024-10-08T00:54:04.642Z'}, {'id': 'pepe', 'symbol': 'pepe', 'name': 'Pepe', 'image': 'https://coin-images.coingecko.com/coins/images/29850/large/pepe-token.jpeg?1696528776', 'current_price': 9.89e-06, 'market_cap': 4160848085, 'market_cap_rank': 29, 'fully_diluted_valuation': 4160848085, 'total_volume': 2685682892, 'high_24h': 1.075e-05, 'low_24h': 9.79e-06, 'price_change_24h': -5.62021500895e-07, 'price_change_percentage_24h': -5.37525, 'market_cap_change_24h': -237379054.54402733, 'market_cap_change_percentage_24h': -5.39715, 'circulating_supply': 420690000000000.0, 'total_supply': 420690000000000.0, 'max_supply': 420690000000000.0, 'ath': 1.717e-05, 'ath_change_percentage': -42.2268, 'ath_date': '2024-05-27T08:30:07.709Z', 'atl': 5.5142e-08, 'atl_change_percentage': 17885.15417, 'atl_date': '2023-04-18T02:14:41.591Z', 'roi': None, 'last_updated': '2024-10-08T00:54:02.340Z'}, {'id': 'wrapped-eeth', 'symbol': 'weeth', 'name': 'Wrapped eETH', 'image': 'https://coin-images.coingecko.com/coins/images/33033/large/weETH.png?1701438396', 'current_price': 2554.88, 'market_cap': 4038780433, 'market_cap_rank': 30, 'fully_diluted_valuation': 4038780433, 'total_volume': 30073791, 'high_24h': 2635.83, 'low_24h': 2524.51, 'price_change_24h': -56.44659523926384, 'price_change_percentage_24h': -2.16161, 'market_cap_change_24h': -54462725.86518383, 'market_cap_change_percentage_24h': -1.33055, 'circulating_supply': 1580768.11822637, 'total_supply': 1580768.11822637, 'max_supply': None, 'ath': 4196.87, 'ath_change_percentage': -39.06049, 'ath_date': '2024-03-13T08:29:59.938Z', 'atl': 2231.18, 'atl_change_percentage': 14.62807, 'atl_date': '2024-01-08T03:35:28.624Z', 'roi': None, 'last_updated': '2024-10-08T00:54:12.420Z'}, {'id': 'fetch-ai', 'symbol': 'fet', 'name': 'Artificial Superintelligence Alliance', 'image': 'https://coin-images.coingecko.com/coins/images/5681/large/ASI.png?1719827289', 'current_price': 1.49, 'market_cap': 3876658812, 'market_cap_rank': 31, 'fully_diluted_valuation': 4039354437, 'total_volume': 571576675, 'high_24h': 1.55, 'low_24h': 1.46, 'price_change_24h': -0.02254401432244868, 'price_change_percentage_24h': -1.49514, 'market_cap_change_24h': -57053180.25035906, 'market_cap_change_percentage_24h': -1.45036, 'circulating_supply': 2609959126.672, 'total_supply': 2719493896.672, 'max_supply': 2719493896.672, 'ath': 3.45, 'ath_change_percentage': -57.02791, 'ath_date': '2024-03-28T17:21:18.050Z', 'atl': 0.00816959, 'atl_change_percentage': 18068.83315, 'atl_date': '2020-03-13T02:24:18.347Z', 'roi': {'times': 16.131207571263495, 'currency': 'usd', 'percentage': 1613.1207571263494}, 'last_updated': '2024-10-08T00:54:07.419Z'}, {'id': 'internet-computer', 'symbol': 'icp', 'name': 'Internet Computer', 'image': 'https://coin-images.coingecko.com/coins/images/14495/large/Internet_Computer_logo.png?1696514180', 'current_price': 8.13, 'market_cap': 3837481675, 'market_cap_rank': 32, 'fully_diluted_valuation': 4261162155, 'total_volume': 149290724, 'high_24h': 8.64, 'low_24h': 8.04, 'price_change_24h': -0.4425720487875928, 'price_change_percentage_24h': -5.16321, 'market_cap_change_24h': -205014513.86187172, 'market_cap_change_percentage_24h': -5.07148, 'circulating_supply': 471937376.870267, 'total_supply': 524042030.677131, 'max_supply': None, 'ath': 700.65, 'ath_change_percentage': -98.84109, 'ath_date': '2021-05-10T16:05:53.653Z', 'atl': 2.87, 'atl_change_percentage': 183.414, 'atl_date': '2023-09-22T00:29:57.369Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.888Z'}, {'id': 'kaspa', 'symbol': 'kas', 'name': 'Kaspa', 'image': 'https://coin-images.coingecko.com/coins/images/25751/large/kaspa-icon-exchanges.png?1696524837', 'current_price': 0.137693, 'market_cap': 3422402884, 'market_cap_rank': 33, 'fully_diluted_valuation': 3422566689, 'total_volume': 49406266, 'high_24h': 0.147201, 'low_24h': 0.135657, 'price_change_24h': -0.006053383405024011, 'price_change_percentage_24h': -4.21116, 'market_cap_change_24h': -175264701.85171556, 'market_cap_change_percentage_24h': -4.87162, 'circulating_supply': 24864690527.4324, 'total_supply': 24865880619.228, 'max_supply': 28704026601.0, 'ath': 0.207411, 'ath_change_percentage': -33.49467, 'ath_date': '2024-08-01T00:40:47.164Z', 'atl': 0.00017105, 'atl_change_percentage': 80542.92002, 'atl_date': '2022-05-26T14:42:59.316Z', 'roi': None, 'last_updated': '2024-10-08T00:54:09.197Z'}, {'id': 'polygon-ecosystem-token', 'symbol': 'pol', 'name': 'POL (ex-MATIC)', 'image': 'https://coin-images.coingecko.com/coins/images/32440/large/polygon.png?1698233684', 'current_price': 0.376926, 'market_cap': 2844275996, 'market_cap_rank': 34, 'fully_diluted_valuation': 3876780009, 'total_volume': 59628192, 'high_24h': 0.389753, 'low_24h': 0.373173, 'price_change_24h': -0.01072502812598436, 'price_change_percentage_24h': -2.76667, 'market_cap_change_24h': -89083731.41555977, 'market_cap_change_percentage_24h': -3.03692, 'circulating_supply': 7536487650.506331, 'total_supply': 10272317000.3188, 'max_supply': None, 'ath': 1.29, 'ath_change_percentage': -70.69669, 'ath_date': '2024-03-13T18:55:06.692Z', 'atl': 0.344976, 'atl_change_percentage': 9.36936, 'atl_date': '2024-08-05T06:45:46.141Z', 'roi': None, 'last_updated': '2024-10-08T00:53:49.166Z'}, {'id': 'ethereum-classic', 'symbol': 'etc', 'name': 'Ethereum Classic', 'image': 'https://coin-images.coingecko.com/coins/images/453/large/ethereum-classic-logo.png?1696501717', 'current_price': 18.7, 'market_cap': 2787253411, 'market_cap_rank': 35, 'fully_diluted_valuation': 3940186706, 'total_volume': 115562256, 'high_24h': 19.1, 'low_24h': 18.54, 'price_change_24h': -0.2404127891239547, 'price_change_percentage_24h': -1.26933, 'market_cap_change_24h': -34460136.58007479, 'market_cap_change_percentage_24h': -1.22125, 'circulating_supply': 149047326.285244, 'total_supply': 210700000.0, 'max_supply': 210700000.0, 'ath': 167.09, 'ath_change_percentage': -88.80786, 'ath_date': '2021-05-06T18:34:22.133Z', 'atl': 0.615038, 'atl_change_percentage': 2940.53103, 'atl_date': '2016-07-25T00:00:00.000Z', 'roi': {'times': 40.55485369483518, 'currency': 'usd', 'percentage': 4055.485369483518}, 'last_updated': '2024-10-08T00:54:08.346Z'}, {'id': 'stellar', 'symbol': 'xlm', 'name': 'Stellar', 'image': 'https://coin-images.coingecko.com/coins/images/100/large/Stellar_symbol_black_RGB.png?1696501482', 'current_price': 0.091505, 'market_cap': 2718689271, 'market_cap_rank': 36, 'fully_diluted_valuation': 4575830767, 'total_volume': 55794210, 'high_24h': 0.093297, 'low_24h': 0.090686, 'price_change_24h': -0.001792341838620773, 'price_change_percentage_24h': -1.92111, 'market_cap_change_24h': -51736126.038962364, 'market_cap_change_percentage_24h': -1.86744, 'circulating_supply': 29708118282.3443, 'total_supply': 50001786931.8027, 'max_supply': 50001786931.8027, 'ath': 0.875563, 'ath_change_percentage': -89.54997, 'ath_date': '2018-01-03T00:00:00.000Z', 'atl': 0.00047612, 'atl_change_percentage': 19116.98588, 'atl_date': '2015-03-05T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.358Z'}, {'id': 'monero', 'symbol': 'xmr', 'name': 'Monero', 'image': 'https://coin-images.coingecko.com/coins/images/69/large/monero_logo.png?1696501460', 'current_price': 144.06, 'market_cap': 2658495109, 'market_cap_rank': 37, 'fully_diluted_valuation': 2658495109, 'total_volume': 49881391, 'high_24h': 148.72, 'low_24h': 143.98, 'price_change_24h': -4.402376663747191, 'price_change_percentage_24h': -2.96526, 'market_cap_change_24h': -79142990.84689522, 'market_cap_change_percentage_24h': -2.89092, 'circulating_supply': 18446744.0737096, 'total_supply': 18446744.0737096, 'max_supply': None, 'ath': 542.33, 'ath_change_percentage': -73.40115, 'ath_date': '2018-01-09T00:00:00.000Z', 'atl': 0.216177, 'atl_change_percentage': 66628.90964, 'atl_date': '2015-01-14T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:12.179Z'}, {'id': 'blockstack', 'symbol': 'stx', 'name': 'Stacks', 'image': 'https://coin-images.coingecko.com/coins/images/2069/large/Stacks_Logo_png.png?1709979332', 'current_price': 1.77, 'market_cap': 2636405645, 'market_cap_rank': 38, 'fully_diluted_valuation': 2636405645, 'total_volume': 148650701, 'high_24h': 1.88, 'low_24h': 1.76, 'price_change_24h': -0.10597245605099315, 'price_change_percentage_24h': -5.65908, 'market_cap_change_24h': -157654404.66367483, 'market_cap_change_percentage_24h': -5.64248, 'circulating_supply': 1492329684.93845, 'total_supply': 1492329684.93845, 'max_supply': 1818000000.0, 'ath': 3.86, 'ath_change_percentage': -54.22821, 'ath_date': '2024-04-01T12:34:58.342Z', 'atl': 0.04559639, 'atl_change_percentage': 3779.18767, 'atl_date': '2020-03-13T02:29:26.415Z', 'roi': {'times': 13.721979508556592, 'currency': 'usd', 'percentage': 1372.1979508556592}, 'last_updated': '2024-10-08T00:54:05.953Z'}, {'id': 'first-digital-usd', 'symbol': 'fdusd', 'name': 'First Digital USD', 'image': 'https://coin-images.coingecko.com/coins/images/31079/large/firstfigital.jpeg?1696529912', 'current_price': 0.999501, 'market_cap': 2614879689, 'market_cap_rank': 39, 'fully_diluted_valuation': 2614879689, 'total_volume': 5277645305, 'high_24h': 1.009, 'low_24h': 0.989271, 'price_change_24h': -0.002162259148526635, 'price_change_percentage_24h': -0.21587, 'market_cap_change_24h': -2509489.620900631, 'market_cap_change_percentage_24h': -0.09588, 'circulating_supply': 2616235583.37, 'total_supply': 2616235583.37, 'max_supply': None, 'ath': 1.089, 'ath_change_percentage': -8.09515, 'ath_date': '2024-05-20T19:42:15.377Z', 'atl': 0.942129, 'atl_change_percentage': 6.18864, 'atl_date': '2023-08-17T21:55:41.478Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.584Z'}, {'id': 'dogwifcoin', 'symbol': 'wif', 'name': 'dogwifhat', 'image': 'https://coin-images.coingecko.com/coins/images/33566/large/dogwifhat.jpg?1702499428', 'current_price': 2.56, 'market_cap': 2553931562, 'market_cap_rank': 40, 'fully_diluted_valuation': 2553931562, 'total_volume': 1577781813, 'high_24h': 2.77, 'low_24h': 2.52, 'price_change_24h': -0.07085253704175454, 'price_change_percentage_24h': -2.69691, 'market_cap_change_24h': -69753420.44635868, 'market_cap_change_percentage_24h': -2.65861, 'circulating_supply': 998926392.0, 'total_supply': 998926392.0, 'max_supply': 998926392.0, 'ath': 4.83, 'ath_change_percentage': -47.0231, 'ath_date': '2024-03-31T09:34:58.366Z', 'atl': 0.00155464, 'atl_change_percentage': 164335.54227, 'atl_date': '2023-12-13T07:13:50.873Z', 'roi': None, 'last_updated': '2024-10-08T00:54:06.464Z'}, {'id': 'okb', 'symbol': 'okb', 'name': 'OKB', 'image': 'https://coin-images.coingecko.com/coins/images/4463/large/WeChat_Image_20220118095654.png?1696505053', 'current_price': 41.75, 'market_cap': 2504875206, 'market_cap_rank': 41, 'fully_diluted_valuation': 9850742591, 'total_volume': 3021964, 'high_24h': 42.23, 'low_24h': 41.49, 'price_change_24h': 0.133862, 'price_change_percentage_24h': 0.32167, 'market_cap_change_24h': 9021972, 'market_cap_change_percentage_24h': 0.36148, 'circulating_supply': 60000000.0, 'total_supply': 235957685.3, 'max_supply': 300000000.0, 'ath': 73.8, 'ath_change_percentage': -43.40078, 'ath_date': '2024-03-14T00:30:12.502Z', 'atl': 0.580608, 'atl_change_percentage': 7094.54711, 'atl_date': '2019-01-14T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.193Z'}, {'id': 'ethena-usde', 'symbol': 'usde', 'name': 'Ethena USDe', 'image': 'https://coin-images.coingecko.com/coins/images/33613/large/USDE.png?1716355685', 'current_price': 0.998868, 'market_cap': 2475501311, 'market_cap_rank': 42, 'fully_diluted_valuation': 2475501311, 'total_volume': 42239822, 'high_24h': 1.001, 'low_24h': 0.99399, 'price_change_24h': -0.000324026932799759, 'price_change_percentage_24h': -0.03243, 'market_cap_change_24h': -735411.9714083672, 'market_cap_change_percentage_24h': -0.0297, 'circulating_supply': 2478118745.28326, 'total_supply': 2478118745.28326, 'max_supply': None, 'ath': 1.032, 'ath_change_percentage': -3.18123, 'ath_date': '2023-12-20T15:38:34.596Z', 'atl': 0.929486, 'atl_change_percentage': 7.50557, 'atl_date': '2024-10-04T07:57:15.809Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.938Z'}, {'id': 'immutable-x', 'symbol': 'imx', 'name': 'Immutable', 'image': 'https://coin-images.coingecko.com/coins/images/17233/large/immutableX-symbol-BLK-RGB.png?1696516787', 'current_price': 1.49, 'market_cap': 2399110937, 'market_cap_rank': 43, 'fully_diluted_valuation': 2988988709, 'total_volume': 98646439, 'high_24h': 1.58, 'low_24h': 1.48, 'price_change_24h': -0.04880323823546994, 'price_change_percentage_24h': -3.16226, 'market_cap_change_24h': -77613561.05947065, 'market_cap_change_percentage_24h': -3.13372, 'circulating_supply': 1605299431.3898141, 'total_supply': 2000000000.0, 'max_supply': 2000000000.0, 'ath': 9.52, 'ath_change_percentage': -84.26299, 'ath_date': '2021-11-26T01:03:01.536Z', 'atl': 0.378055, 'atl_change_percentage': 296.27954, 'atl_date': '2022-12-31T07:36:37.649Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.984Z'}, {'id': 'filecoin', 'symbol': 'fil', 'name': 'Filecoin', 'image': 'https://coin-images.coingecko.com/coins/images/12817/large/filecoin.png?1696512609', 'current_price': 3.74, 'market_cap': 2212048994, 'market_cap_rank': 44, 'fully_diluted_valuation': 7333073696, 'total_volume': 220501731, 'high_24h': 3.81, 'low_24h': 3.69, 'price_change_24h': 0.00388424, 'price_change_percentage_24h': 0.10391, 'market_cap_change_24h': 3262126, 'market_cap_change_percentage_24h': 0.14769, 'circulating_supply': 591219863.0, 'total_supply': 1959928934.0, 'max_supply': 1959928934.0, 'ath': 236.84, 'ath_change_percentage': -98.42174, 'ath_date': '2021-04-01T13:29:41.564Z', 'atl': 2.64, 'atl_change_percentage': 41.53757, 'atl_date': '2022-12-16T22:45:28.552Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.874Z'}, {'id': 'aave', 'symbol': 'aave', 'name': 'Aave', 'image': 'https://coin-images.coingecko.com/coins/images/12645/large/aave-token-round.png?1720472354', 'current_price': 146.68, 'market_cap': 2192912572, 'market_cap_rank': 45, 'fully_diluted_valuation': 2346687419, 'total_volume': 317227189, 'high_24h': 154.37, 'low_24h': 145.63, 'price_change_24h': -5.858279094344567, 'price_change_percentage_24h': -3.84065, 'market_cap_change_24h': -87181126.71949911, 'market_cap_change_percentage_24h': -3.82358, 'circulating_supply': 14951544.4115039, 'total_supply': 16000000.0, 'max_supply': 16000000.0, 'ath': 661.69, 'ath_change_percentage': -77.76392, 'ath_date': '2021-05-18T21:19:59.514Z', 'atl': 26.02, 'atl_change_percentage': 465.39646, 'atl_date': '2020-11-05T09:20:11.928Z', 'roi': None, 'last_updated': '2024-10-08T00:54:13.358Z'}, {'id': 'crypto-com-chain', 'symbol': 'cro', 'name': 'Cronos', 'image': 'https://coin-images.coingecko.com/coins/images/7310/large/cro_token_logo.png?1696507599', 'current_price': 0.078844, 'market_cap': 2129405535, 'market_cap_rank': 46, 'fully_diluted_valuation': 2364898273, 'total_volume': 5713307, 'high_24h': 0.081471, 'low_24h': 0.078325, 'price_change_24h': -0.002135162614234229, 'price_change_percentage_24h': -2.63669, 'market_cap_change_24h': -60340163.39772034, 'market_cap_change_percentage_24h': -2.75558, 'circulating_supply': 27012648607.4253, 'total_supply': 30000000000.0, 'max_supply': None, 'ath': 0.965407, 'ath_change_percentage': -91.82827, 'ath_date': '2021-11-24T15:53:54.855Z', 'atl': 0.0121196, 'atl_change_percentage': 550.93336, 'atl_date': '2019-02-08T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.102Z'}, {'id': 'optimism', 'symbol': 'op', 'name': 'Optimism', 'image': 'https://coin-images.coingecko.com/coins/images/25244/large/Optimism.png?1696524385', 'current_price': 1.67, 'market_cap': 2101136824, 'market_cap_rank': 47, 'fully_diluted_valuation': 7190284537, 'total_volume': 218428886, 'high_24h': 1.73, 'low_24h': 1.64, 'price_change_24h': -0.03476149712388876, 'price_change_percentage_24h': -2.03669, 'market_cap_change_24h': -39040505.690792084, 'market_cap_change_percentage_24h': -1.82417, 'circulating_supply': 1255070491.0, 'total_supply': 4294967296.0, 'max_supply': 4294967296.0, 'ath': 4.84, 'ath_change_percentage': -65.39861, 'ath_date': '2024-03-06T08:35:50.817Z', 'atl': 0.402159, 'atl_change_percentage': 316.79983, 'atl_date': '2022-06-18T20:54:52.178Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.229Z'}, {'id': 'render-token', 'symbol': 'render', 'name': 'Render', 'image': 'https://coin-images.coingecko.com/coins/images/11636/large/rndr.png?1696511529', 'current_price': 5.32, 'market_cap': 2086965284, 'market_cap_rank': 48, 'fully_diluted_valuation': 2830162803, 'total_volume': 189902209, 'high_24h': 5.66, 'low_24h': 5.25, 'price_change_24h': -0.23595866218748007, 'price_change_percentage_24h': -4.24782, 'market_cap_change_24h': -90450600.43532825, 'market_cap_change_percentage_24h': -4.15403, 'circulating_supply': 392459381.455492, 'total_supply': 532219654.735492, 'max_supply': None, 'ath': 13.53, 'ath_change_percentage': -60.69465, 'ath_date': '2024-03-17T16:30:24.636Z', 'atl': 0.03665669, 'atl_change_percentage': 14412.65249, 'atl_date': '2020-06-16T13:22:25.900Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.739Z'}, {'id': 'injective-protocol', 'symbol': 'inj', 'name': 'Injective', 'image': 'https://coin-images.coingecko.com/coins/images/12882/large/Secondary_Symbol.png?1696512670', 'current_price': 20.63, 'market_cap': 2020107435, 'market_cap_rank': 49, 'fully_diluted_valuation': 2067087743, 'total_volume': 162234744, 'high_24h': 21.68, 'low_24h': 20.41, 'price_change_24h': -0.5420461942348993, 'price_change_percentage_24h': -2.56032, 'market_cap_change_24h': -38736183.09372449, 'market_cap_change_percentage_24h': -1.88145, 'circulating_supply': 97727222.33, 'total_supply': 100000000.0, 'max_supply': None, 'ath': 52.62, 'ath_change_percentage': -60.76848, 'ath_date': '2024-03-14T15:06:22.124Z', 'atl': 0.657401, 'atl_change_percentage': 3040.22836, 'atl_date': '2020-11-03T16:19:30.576Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.301Z'}, {'id': 'arbitrum', 'symbol': 'arb', 'name': 'Arbitrum', 'image': 'https://coin-images.coingecko.com/coins/images/16547/large/arb.jpg?1721358242', 'current_price': 0.55172, 'market_cap': 1995610038, 'market_cap_rank': 50, 'fully_diluted_valuation': 5517172560, 'total_volume': 305793045, 'high_24h': 0.574299, 'low_24h': 0.549433, 'price_change_24h': -0.01640000679415732, 'price_change_percentage_24h': -2.88672, 'market_cap_change_24h': -58973496.59108758, 'market_cap_change_percentage_24h': -2.87034, 'circulating_supply': 3617088312.0, 'total_supply': 10000000000.0, 'max_supply': 10000000000.0, 'ath': 2.39, 'ath_change_percentage': -76.88637, 'ath_date': '2024-01-12T12:29:59.231Z', 'atl': 0.431578, 'atl_change_percentage': 28.01435, 'atl_date': '2024-08-05T11:35:54.024Z', 'roi': None, 'last_updated': '2024-10-08T00:54:12.965Z'}, {'id': 'hedera-hashgraph', 'symbol': 'hbar', 'name': 'Hedera', 'image': 'https://coin-images.coingecko.com/coins/images/3688/large/hbar.png?1696504364', 'current_price': 0.052714, 'market_cap': 1984767944, 'market_cap_rank': 51, 'fully_diluted_valuation': 2635626384, 'total_volume': 58643085, 'high_24h': 0.055465, 'low_24h': 0.052442, 'price_change_24h': -0.002289720403595014, 'price_change_percentage_24h': -4.16286, 'market_cap_change_24h': -87166884.84962511, 'market_cap_change_percentage_24h': -4.20703, 'circulating_supply': 37652680130.7546, 'total_supply': 50000000000.0, 'max_supply': 50000000000.0, 'ath': 0.569229, 'ath_change_percentage': -90.73263, 'ath_date': '2021-09-15T10:40:28.318Z', 'atl': 0.00986111, 'atl_change_percentage': 434.95523, 'atl_date': '2020-01-02T17:30:24.852Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.816Z'}, {'id': 'mantle', 'symbol': 'mnt', 'name': 'Mantle', 'image': 'https://coin-images.coingecko.com/coins/images/30980/large/token-logo.png?1696529819', 'current_price': 0.594723, 'market_cap': 1942595143, 'market_cap_rank': 52, 'fully_diluted_valuation': 3698255282, 'total_volume': 83557233, 'high_24h': 0.617403, 'low_24h': 0.590237, 'price_change_24h': -0.020384026293793656, 'price_change_percentage_24h': -3.3139, 'market_cap_change_24h': -65637588.91737676, 'market_cap_change_percentage_24h': -3.26843, 'circulating_supply': 3266841707.83684, 'total_supply': 6219316794.99, 'max_supply': 6219316794.99, 'ath': 1.54, 'ath_change_percentage': -61.31909, 'ath_date': '2024-04-08T09:45:25.482Z', 'atl': 0.307978, 'atl_change_percentage': 93.25706, 'atl_date': '2023-10-18T02:50:46.942Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.666Z'}, {'id': 'fantom', 'symbol': 'ftm', 'name': 'Fantom', 'image': 'https://coin-images.coingecko.com/coins/images/4001/large/Fantom_round.png?1696504642', 'current_price': 0.680296, 'market_cap': 1904364725, 'market_cap_rank': 53, 'fully_diluted_valuation': 2156613951, 'total_volume': 442766255, 'high_24h': 0.692213, 'low_24h': 0.643663, 'price_change_24h': 0.02108022, 'price_change_percentage_24h': 3.19777, 'market_cap_change_24h': 51058008, 'market_cap_change_percentage_24h': 2.75497, 'circulating_supply': 2803634835.52659, 'total_supply': 3175000000.0, 'max_supply': 3175000000.0, 'ath': 3.46, 'ath_change_percentage': -80.42443, 'ath_date': '2021-10-28T05:19:39.845Z', 'atl': 0.00190227, 'atl_change_percentage': 35490.93498, 'atl_date': '2020-03-13T02:25:38.280Z', 'roi': {'times': 21.676526443109893, 'currency': 'usd', 'percentage': 2167.652644310989}, 'last_updated': '2024-10-08T00:54:08.083Z'}, {'id': 'vechain', 'symbol': 'vet', 'name': 'VeChain', 'image': 'https://coin-images.coingecko.com/coins/images/1167/large/VET_Token_Icon.png?1710013505', 'current_price': 0.02305745, 'market_cap': 1866097633, 'market_cap_rank': 54, 'fully_diluted_valuation': 1981310122, 'total_volume': 33029041, 'high_24h': 0.02413624, 'low_24h': 0.02287468, 'price_change_24h': -0.000579121553516551, 'price_change_percentage_24h': -2.45011, 'market_cap_change_24h': -48530189.73806381, 'market_cap_change_percentage_24h': -2.53471, 'circulating_supply': 80985041177.0, 'total_supply': 85985041177.0, 'max_supply': 86712634466.0, 'ath': 0.280991, 'ath_change_percentage': -91.78701, 'ath_date': '2021-04-19T01:08:21.675Z', 'atl': 0.00191713, 'atl_change_percentage': 1103.76506, 'atl_date': '2020-03-13T02:29:59.652Z', 'roi': {'times': 2.315297764204298, 'currency': 'eth', 'percentage': 231.52977642042976}, 'last_updated': '2024-10-08T00:54:04.449Z'}, {'id': 'cosmos', 'symbol': 'atom', 'name': 'Cosmos Hub', 'image': 'https://coin-images.coingecko.com/coins/images/1481/large/cosmos_hub.png?1696502525', 'current_price': 4.44, 'market_cap': 1735226482, 'market_cap_rank': 55, 'fully_diluted_valuation': 1736299828, 'total_volume': 157094956, 'high_24h': 4.78, 'low_24h': 4.42, 'price_change_24h': -0.2745205030004252, 'price_change_percentage_24h': -5.82303, 'market_cap_change_24h': -106653058.32591224, 'market_cap_change_percentage_24h': -5.79045, 'circulating_supply': 390688369.813272, 'total_supply': 390930035.085365, 'max_supply': None, 'ath': 44.45, 'ath_change_percentage': -90.00078, 'ath_date': '2022-01-17T00:34:41.497Z', 'atl': 1.16, 'atl_change_percentage': 283.12318, 'atl_date': '2020-03-13T02:27:44.591Z', 'roi': {'times': 43.39870942426602, 'currency': 'usd', 'percentage': 4339.870942426602}, 'last_updated': '2024-10-08T00:54:06.660Z'}, {'id': 'thorchain', 'symbol': 'rune', 'name': 'THORChain', 'image': 'https://coin-images.coingecko.com/coins/images/6595/large/Rune200x200.png?1696506946', 'current_price': 5.09, 'market_cap': 1713439020, 'market_cap_rank': 56, 'fully_diluted_valuation': 2107376729, 'total_volume': 436029070, 'high_24h': 5.32, 'low_24h': 4.99, 'price_change_24h': 0.092116, 'price_change_percentage_24h': 1.8445, 'market_cap_change_24h': 30384714, 'market_cap_change_percentage_24h': 1.80533, 'circulating_supply': 336760873.0, 'total_supply': 414185751.0, 'max_supply': 500000000.0, 'ath': 20.87, 'ath_change_percentage': -75.60622, 'ath_date': '2021-05-19T00:30:09.436Z', 'atl': 0.00851264, 'atl_change_percentage': 59698.01566, 'atl_date': '2019-09-28T00:00:00.000Z', 'roi': {'times': 132.84719235436123, 'currency': 'usd', 'percentage': 13284.719235436123}, 'last_updated': '2024-10-08T00:54:11.506Z'}, {'id': 'whitebit', 'symbol': 'wbt', 'name': 'WhiteBIT Coin', 'image': 'https://coin-images.coingecko.com/coins/images/27045/large/wbt_token.png?1696526096', 'current_price': 11.61, 'market_cap': 1672767757, 'market_cap_rank': 57, 'fully_diluted_valuation': 3986192957, 'total_volume': 5269580, 'high_24h': 11.74, 'low_24h': 11.45, 'price_change_24h': 0.054923, 'price_change_percentage_24h': 0.47536, 'market_cap_change_24h': 8356197, 'market_cap_change_percentage_24h': 0.50205, 'circulating_supply': 144118517.10815412, 'total_supply': 343433340.0, 'max_supply': 400000000.0, 'ath': 14.64, 'ath_change_percentage': -20.66611, 'ath_date': '2022-10-28T12:32:18.119Z', 'atl': 3.06, 'atl_change_percentage': 279.46514, 'atl_date': '2023-02-13T19:01:21.899Z', 'roi': None, 'last_updated': '2024-10-08T00:54:12.576Z'}, {'id': 'the-graph', 'symbol': 'grt', 'name': 'The Graph', 'image': 'https://coin-images.coingecko.com/coins/images/13397/large/Graph_Token.png?1696513159', 'current_price': 0.16643, 'market_cap': 1590930683, 'market_cap_rank': 58, 'fully_diluted_valuation': 1797445718, 'total_volume': 74994229, 'high_24h': 0.175174, 'low_24h': 0.164311, 'price_change_24h': -0.002237407739360153, 'price_change_percentage_24h': -1.32652, 'market_cap_change_24h': -19839233.915035248, 'market_cap_change_percentage_24h': -1.23166, 'circulating_supply': 9548531509.16547, 'total_supply': 10788004319.0, 'max_supply': 10788004319.0, 'ath': 2.84, 'ath_change_percentage': -94.13502, 'ath_date': '2021-02-12T07:28:45.775Z', 'atl': 0.052051, 'atl_change_percentage': 220.17379, 'atl_date': '2022-11-22T10:05:03.503Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.707Z'}, {'id': 'sei-network', 'symbol': 'sei', 'name': 'Sei', 'image': 'https://coin-images.coingecko.com/coins/images/28205/large/Sei_Logo_-_Transparent.png?1696527207', 'current_price': 0.436364, 'market_cap': 1537777839, 'market_cap_rank': 59, 'fully_diluted_valuation': 4362662218, 'total_volume': 385107557, 'high_24h': 0.45975, 'low_24h': 0.425647, 'price_change_24h': -0.012570966005509167, 'price_change_percentage_24h': -2.80018, 'market_cap_change_24h': -44875622.576061964, 'market_cap_change_percentage_24h': -2.83547, 'circulating_supply': 3524861111.0, 'total_supply': 10000000000.0, 'max_supply': None, 'ath': 1.14, 'ath_change_percentage': -61.71689, 'ath_date': '2024-03-16T02:30:23.904Z', 'atl': 0.095364, 'atl_change_percentage': 357.53176, 'atl_date': '2023-10-19T08:05:30.655Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.382Z'}, {'id': 'bitget-token', 'symbol': 'bgb', 'name': 'Bitget Token', 'image': 'https://coin-images.coingecko.com/coins/images/11610/large/icon_colour.png?1696511504', 'current_price': 1.075, 'market_cap': 1505112149, 'market_cap_rank': 60, 'fully_diluted_valuation': 2150158677, 'total_volume': 4809367, 'high_24h': 1.14, 'low_24h': 0.641602, 'price_change_24h': -0.029675512038628504, 'price_change_percentage_24h': -2.68607, 'market_cap_change_24h': -41599600.1858089, 'market_cap_change_percentage_24h': -2.68955, 'circulating_supply': 1400001000.0, 'total_supply': 2000000000.0, 'max_supply': 2000000000.0, 'ath': 1.48, 'ath_change_percentage': -27.18174, 'ath_date': '2024-06-01T03:50:55.951Z', 'atl': 0.0142795, 'atl_change_percentage': 7443.46905, 'atl_date': '2020-06-25T04:17:05.895Z', 'roi': None, 'last_updated': '2024-10-08T00:54:05.579Z'}, {'id': 'bonk', 'symbol': 'bonk', 'name': 'Bonk', 'image': 'https://coin-images.coingecko.com/coins/images/28600/large/bonk.jpg?1696527587', 'current_price': 2.134e-05, 'market_cap': 1482766707, 'market_cap_rank': 61, 'fully_diluted_valuation': 1996093347, 'total_volume': 418331264, 'high_24h': 2.327e-05, 'low_24h': 2.116e-05, 'price_change_24h': -1.444364956781e-06, 'price_change_percentage_24h': -6.33855, 'market_cap_change_24h': -100689387.01500964, 'market_cap_change_percentage_24h': -6.35884, 'circulating_supply': 69474461693558.484, 'total_supply': 93526183276778.0, 'max_supply': 93526183276778.0, 'ath': 4.547e-05, 'ath_change_percentage': -53.06035, 'ath_date': '2024-03-04T17:05:29.594Z', 'atl': 8.6142e-08, 'atl_change_percentage': 24677.40406, 'atl_date': '2022-12-29T22:48:46.755Z', 'roi': None, 'last_updated': '2024-10-08T00:54:06.906Z'}, {'id': 'binance-peg-weth', 'symbol': 'weth', 'name': 'Binance-Peg WETH', 'image': 'https://coin-images.coingecko.com/coins/images/39580/large/weth.png?1723006716', 'current_price': 2434.55, 'market_cap': 1472855920, 'market_cap_rank': 62, 'fully_diluted_valuation': 1472855920, 'total_volume': 52219432, 'high_24h': 2512.03, 'low_24h': 2423.7, 'price_change_24h': -52.02834219076749, 'price_change_percentage_24h': -2.09236, 'market_cap_change_24h': -32065616.701003075, 'market_cap_change_percentage_24h': -2.13072, 'circulating_supply': 604999.999959841, 'total_supply': 604999.999959841, 'max_supply': None, 'ath': 2808.56, 'ath_change_percentage': -13.40018, 'ath_date': '2024-08-24T16:52:46.091Z', 'atl': 2171.8, 'atl_change_percentage': 11.99047, 'atl_date': '2024-09-06T21:02:52.910Z', 'roi': None, 'last_updated': '2024-10-08T00:54:04.960Z'}, {'id': 'floki', 'symbol': 'floki', 'name': 'FLOKI', 'image': 'https://coin-images.coingecko.com/coins/images/16746/large/PNG_image.png?1696516318', 'current_price': 0.00013831, 'market_cap': 1340262428, 'market_cap_rank': 63, 'fully_diluted_valuation': 1383218927, 'total_volume': 371506712, 'high_24h': 0.00014615, 'low_24h': 0.00013686, 'price_change_24h': -5.493776205175e-06, 'price_change_percentage_24h': -3.82021, 'market_cap_change_24h': -53932127.53149462, 'market_cap_change_percentage_24h': -3.86834, 'circulating_supply': 9689445405951.0, 'total_supply': 10000000000000.0, 'max_supply': 10000000000000.0, 'ath': 0.00034495, 'ath_change_percentage': -59.87297, 'ath_date': '2024-06-05T07:25:59.137Z', 'atl': 8.428e-08, 'atl_change_percentage': 164136.20747, 'atl_date': '2021-07-06T01:11:20.438Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.994Z'}, {'id': 'rocket-pool-eth', 'symbol': 'reth', 'name': 'Rocket Pool ETH', 'image': 'https://coin-images.coingecko.com/coins/images/20764/large/reth.png?1696520159', 'current_price': 2719.44, 'market_cap': 1328506348, 'market_cap_rank': 64, 'fully_diluted_valuation': 1328506348, 'total_volume': 5214534, 'high_24h': 2808.84, 'low_24h': 2696.73, 'price_change_24h': -64.3257142784546, 'price_change_percentage_24h': -2.31075, 'market_cap_change_24h': -30626102.881741285, 'market_cap_change_percentage_24h': -2.25336, 'circulating_supply': 488193.944227492, 'total_supply': 488193.944227492, 'max_supply': None, 'ath': 4814.31, 'ath_change_percentage': -43.50101, 'ath_date': '2021-12-01T08:03:50.749Z', 'atl': 887.26, 'atl_change_percentage': 206.56468, 'atl_date': '2022-06-18T20:55:45.957Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.671Z'}, {'id': 'theta-token', 'symbol': 'theta', 'name': 'Theta Network', 'image': 'https://coin-images.coingecko.com/coins/images/2538/large/theta-token-logo.png?1696503349', 'current_price': 1.31, 'market_cap': 1305909326, 'market_cap_rank': 65, 'fully_diluted_valuation': 1305909326, 'total_volume': 19702810, 'high_24h': 1.37, 'low_24h': 1.3, 'price_change_24h': -0.05468473696731757, 'price_change_percentage_24h': -4.02037, 'market_cap_change_24h': -52673402.857219934, 'market_cap_change_percentage_24h': -3.87708, 'circulating_supply': 1000000000.0, 'total_supply': 1000000000.0, 'max_supply': 1000000000.0, 'ath': 15.72, 'ath_change_percentage': -91.68442, 'ath_date': '2021-04-16T13:15:11.190Z', 'atl': 0.04039979, 'atl_change_percentage': 3135.62621, 'atl_date': '2020-03-13T02:24:16.483Z', 'roi': {'times': 7.7033697367491865, 'currency': 'usd', 'percentage': 770.3369736749187}, 'last_updated': '2024-10-08T00:54:11.319Z'}, {'id': 'popcat', 'symbol': 'popcat', 'name': 'Popcat', 'image': 'https://coin-images.coingecko.com/coins/images/33760/large/image.jpg?1702964227', 'current_price': 1.28, 'market_cap': 1254180237, 'market_cap_rank': 66, 'fully_diluted_valuation': 1254180237, 'total_volume': 166807604, 'high_24h': 1.47, 'low_24h': 1.27, 'price_change_24h': -0.1536987502902194, 'price_change_percentage_24h': -10.72276, 'market_cap_change_24h': -149282754.86362696, 'market_cap_change_percentage_24h': -10.63674, 'circulating_supply': 979978669.96, 'total_supply': 979978669.96, 'max_supply': 979978694.0, 'ath': 1.47, 'ath_change_percentage': -13.18589, 'ath_date': '2024-10-07T08:39:47.267Z', 'atl': 0.00379728, 'atl_change_percentage': 33597.19423, 'atl_date': '2024-01-05T15:34:24.842Z', 'roi': None, 'last_updated': '2024-10-08T00:53:24.463Z'}, {'id': 'arweave', 'symbol': 'ar', 'name': 'Arweave', 'image': 'https://coin-images.coingecko.com/coins/images/4343/large/oRt6SiEN_400x400.jpg?1696504946', 'current_price': 19.03, 'market_cap': 1245444184, 'market_cap_rank': 67, 'fully_diluted_valuation': 1245444184, 'total_volume': 93459497, 'high_24h': 20.35, 'low_24h': 18.83, 'price_change_24h': -1.102395992722606, 'price_change_percentage_24h': -5.47499, 'market_cap_change_24h': -72086752.11991191, 'market_cap_change_percentage_24h': -5.47135, 'circulating_supply': 65454185.5381511, 'total_supply': 65454185.5381511, 'max_supply': 66000000.0, 'ath': 89.24, 'ath_change_percentage': -78.66006, 'ath_date': '2021-11-05T04:14:42.689Z', 'atl': 0.298788, 'atl_change_percentage': 6273.32719, 'atl_date': '2020-01-31T06:47:36.543Z', 'roi': {'times': 24.719889503069542, 'currency': 'usd', 'percentage': 2471.988950306954}, 'last_updated': '2024-10-08T00:54:04.701Z'}, {'id': 'maker', 'symbol': 'mkr', 'name': 'Maker', 'image': 'https://coin-images.coingecko.com/coins/images/1364/large/Mark_Maker.png?1696502423', 'current_price': 1406.89, 'market_cap': 1228618819, 'market_cap_rank': 68, 'fully_diluted_valuation': 1271387721, 'total_volume': 141868687, 'high_24h': 1484.9, 'low_24h': 1388.68, 'price_change_24h': -65.40749774075653, 'price_change_percentage_24h': -4.44254, 'market_cap_change_24h': -63091679.83813453, 'market_cap_change_percentage_24h': -4.88435, 'circulating_supply': 874256.939689837, 'total_supply': 904690.308012115, 'max_supply': 1005577.0, 'ath': 6292.31, 'ath_change_percentage': -77.6293, 'ath_date': '2021-05-03T21:54:29.333Z', 'atl': 168.36, 'atl_change_percentage': 736.09842, 'atl_date': '2020-03-16T20:52:36.527Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.987Z'}, {'id': 'mantle-staked-ether', 'symbol': 'meth', 'name': 'Mantle Staked Ether', 'image': 'https://coin-images.coingecko.com/coins/images/33345/large/symbol_transparent_bg.png?1701697066', 'current_price': 2540.6, 'market_cap': 1203906007, 'market_cap_rank': 69, 'fully_diluted_valuation': 1203906007, 'total_volume': 46913247, 'high_24h': 2625.66, 'low_24h': 2510.44, 'price_change_24h': -54.040076595622395, 'price_change_percentage_24h': -2.08276, 'market_cap_change_24h': -28273549.475244284, 'market_cap_change_percentage_24h': -2.2946, 'circulating_supply': 474094.100298458, 'total_supply': 474094.100298458, 'max_supply': None, 'ath': 4729.53, 'ath_change_percentage': -46.19013, 'ath_date': '2024-03-27T05:26:27.333Z', 'atl': 2142.02, 'atl_change_percentage': 18.81108, 'atl_date': '2023-12-18T10:41:32.686Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.632Z'}, {'id': 'mantra-dao', 'symbol': 'om', 'name': 'MANTRA', 'image': 'https://coin-images.coingecko.com/coins/images/12151/large/OM_Token.png?1696511991', 'current_price': 1.4, 'market_cap': 1192208346, 'market_cap_rank': 70, 'fully_diluted_valuation': 1246554201, 'total_volume': 55717062, 'high_24h': 1.47, 'low_24h': 1.38, 'price_change_24h': -0.008372729468661968, 'price_change_percentage_24h': -0.59314, 'market_cap_change_24h': 993214, 'market_cap_change_percentage_24h': 0.08338, 'circulating_supply': 850136119.15, 'total_supply': 888888888.0, 'max_supply': 888888888.0, 'ath': 1.47, 'ath_change_percentage': -4.68623, 'ath_date': '2024-10-07T01:10:47.695Z', 'atl': 0.01726188, 'atl_change_percentage': 8009.94779, 'atl_date': '2023-10-12T17:25:09.068Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.389Z'}, {'id': 'pyth-network', 'symbol': 'pyth', 'name': 'Pyth Network', 'image': 'https://coin-images.coingecko.com/coins/images/31924/large/pyth.png?1701245725', 'current_price': 0.327718, 'market_cap': 1187587206, 'market_cap_rank': 71, 'fully_diluted_valuation': 3276112770, 'total_volume': 71153373, 'high_24h': 0.347405, 'low_24h': 0.323434, 'price_change_24h': -0.014656647843092951, 'price_change_percentage_24h': -4.28087, 'market_cap_change_24h': -53646265.49825287, 'market_cap_change_percentage_24h': -4.32201, 'circulating_supply': 3624988786.43857, 'total_supply': 10000000000.0, 'max_supply': 10000000000.0, 'ath': 1.2, 'ath_change_percentage': -72.6067, 'ath_date': '2024-03-16T07:01:15.357Z', 'atl': 0.22352, 'atl_change_percentage': 46.61974, 'atl_date': '2024-08-05T11:30:50.894Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.687Z'}, {'id': 'helium', 'symbol': 'hnt', 'name': 'Helium', 'image': 'https://coin-images.coingecko.com/coins/images/4284/large/Helium_HNT.png?1696504894', 'current_price': 6.89, 'market_cap': 1175763601, 'market_cap_rank': 72, 'fully_diluted_valuation': 1535181838, 'total_volume': 21363150, 'high_24h': 7.5, 'low_24h': 6.8, 'price_change_24h': -0.5567727122519592, 'price_change_percentage_24h': -7.47186, 'market_cap_change_24h': -96629481.61365056, 'market_cap_change_percentage_24h': -7.59431, 'circulating_supply': 170791027.272522, 'total_supply': 223000000.0, 'max_supply': 223000000.0, 'ath': 54.88, 'ath_change_percentage': -87.4452, 'ath_date': '2021-11-12T23:08:25.301Z', 'atl': 0.113248, 'atl_change_percentage': 5983.74805, 'atl_date': '2020-04-18T00:19:10.902Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.888Z'}, {'id': 'solv-btc', 'symbol': 'solvbtc', 'name': 'Solv Protocol SolvBTC', 'image': 'https://coin-images.coingecko.com/coins/images/36800/large/solvBTC.png?1719810684', 'current_price': 62563, 'market_cap': 1166983495, 'market_cap_rank': 73, 'fully_diluted_valuation': 1166983495, 'total_volume': 4234286, 'high_24h': 64322, 'low_24h': 62260, 'price_change_24h': -972.2084470276677, 'price_change_percentage_24h': -1.5302, 'market_cap_change_24h': -18407428.421874046, 'market_cap_change_percentage_24h': -1.55286, 'circulating_supply': 18649.035274833, 'total_supply': 18649.035274833, 'max_supply': 21000000.0, 'ath': 70898, 'ath_change_percentage': -11.66845, 'ath_date': '2024-06-05T16:15:24.629Z', 'atl': 49058, 'atl_change_percentage': 27.6557, 'atl_date': '2024-08-05T06:28:45.311Z', 'roi': None, 'last_updated': '2024-10-08T00:53:36.145Z'}, {'id': 'celestia', 'symbol': 'tia', 'name': 'Celestia', 'image': 'https://coin-images.coingecko.com/coins/images/31967/large/tia.jpg?1696530772', 'current_price': 5.39, 'market_cap': 1160718301, 'market_cap_rank': 74, 'fully_diluted_valuation': 5799974465, 'total_volume': 223647640, 'high_24h': 5.72, 'low_24h': 5.33, 'price_change_24h': -0.03489822745982085, 'price_change_percentage_24h': -0.64286, 'market_cap_change_24h': -4896667.941010714, 'market_cap_change_percentage_24h': -0.42009, 'circulating_supply': 214906541.448367, 'total_supply': 1073863013.69837, 'max_supply': None, 'ath': 20.85, 'ath_change_percentage': -74.13297, 'ath_date': '2024-02-10T14:30:02.495Z', 'atl': 2.08, 'atl_change_percentage': 158.86734, 'atl_date': '2023-10-31T15:14:31.951Z', 'roi': None, 'last_updated': '2024-10-08T00:54:05.796Z'}, {'id': 'gatechain-token', 'symbol': 'gt', 'name': 'Gate', 'image': 'https://coin-images.coingecko.com/coins/images/8183/large/gate.png?1696508395', 'current_price': 8.86, 'market_cap': 1133702266, 'market_cap_rank': 75, 'fully_diluted_valuation': 2655869390, 'total_volume': 3255449, 'high_24h': 9.0, 'low_24h': 8.73, 'price_change_24h': 0.123653, 'price_change_percentage_24h': 1.4158, 'market_cap_change_24h': 15515512, 'market_cap_change_percentage_24h': 1.38756, 'circulating_supply': 128060017.17436442, 'total_supply': 300000000.0, 'max_supply': None, 'ath': 12.94, 'ath_change_percentage': -31.4915, 'ath_date': '2021-05-12T11:39:16.531Z', 'atl': 0.25754, 'atl_change_percentage': 3342.6755, 'atl_date': '2020-03-13T02:18:02.481Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.598Z'}, {'id': 'jupiter-exchange-solana', 'symbol': 'jup', 'name': 'Jupiter', 'image': 'https://coin-images.coingecko.com/coins/images/34188/large/jup.png?1704266489', 'current_price': 0.772541, 'market_cap': 1043006766, 'market_cap_rank': 76, 'fully_diluted_valuation': 7725976046, 'total_volume': 151655748, 'high_24h': 0.813931, 'low_24h': 0.767147, 'price_change_24h': -0.02900773798361378, 'price_change_percentage_24h': -3.61896, 'market_cap_change_24h': -38808691.17161238, 'market_cap_change_percentage_24h': -3.58737, 'circulating_supply': 1350000000.0, 'total_supply': 10000000000.0, 'max_supply': 10000000000.0, 'ath': 2.0, 'ath_change_percentage': -61.2806, 'ath_date': '2024-01-31T15:02:47.304Z', 'atl': 0.457464, 'atl_change_percentage': 69.27956, 'atl_date': '2024-02-21T18:31:05.083Z', 'roi': None, 'last_updated': '2024-10-08T00:54:09.568Z'}, {'id': 'algorand', 'symbol': 'algo', 'name': 'Algorand', 'image': 'https://coin-images.coingecko.com/coins/images/4380/large/download.png?1696504978', 'current_price': 0.125351, 'market_cap': 1039769409, 'market_cap_rank': 77, 'fully_diluted_valuation': 1039769414, 'total_volume': 57158385, 'high_24h': 0.129693, 'low_24h': 0.124391, 'price_change_24h': -0.002723524453178167, 'price_change_percentage_24h': -2.12651, 'market_cap_change_24h': -20685980.82075572, 'market_cap_change_percentage_24h': -1.95067, 'circulating_supply': 8292746539.53288, 'total_supply': 8292746578.70868, 'max_supply': 10000000000.0, 'ath': 3.56, 'ath_change_percentage': -96.48322, 'ath_date': '2019-06-20T14:51:19.480Z', 'atl': 0.087513, 'atl_change_percentage': 43.10684, 'atl_date': '2023-09-11T19:42:08.247Z', 'roi': {'times': -0.9477703186223201, 'currency': 'usd', 'percentage': -94.77703186223201}, 'last_updated': '2024-10-08T00:54:06.162Z'}, {'id': 'matic-network', 'symbol': 'matic', 'name': 'Polygon', 'image': 'https://coin-images.coingecko.com/coins/images/4713/large/polygon.png?1698233745', 'current_price': 0.377139, 'market_cap': 1031513152, 'market_cap_rank': 78, 'fully_diluted_valuation': 3770427519, 'total_volume': 10428459, 'high_24h': 0.389223, 'low_24h': 0.375421, 'price_change_24h': -0.005050292179460625, 'price_change_percentage_24h': -1.32141, 'market_cap_change_24h': -1119194.28873837, 'market_cap_change_percentage_24h': -0.10838, 'circulating_supply': 2735798915.4468446, 'total_supply': 10000000000.0, 'max_supply': 10000000000.0, 'ath': 2.92, 'ath_change_percentage': -87.0595, 'ath_date': '2021-12-27T02:08:34.307Z', 'atl': 0.00314376, 'atl_change_percentage': 11904.06254, 'atl_date': '2019-05-10T00:00:00.000Z', 'roi': {'times': 142.3989391138943, 'currency': 'usd', 'percentage': 14239.89391138943}, 'last_updated': '2024-10-08T00:54:10.159Z'}, {'id': 'ondo-finance', 'symbol': 'ondo', 'name': 'Ondo', 'image': 'https://coin-images.coingecko.com/coins/images/26580/large/ONDO.png?1696525656', 'current_price': 0.711239, 'market_cap': 1020937000, 'market_cap_rank': 79, 'fully_diluted_valuation': 7106589499, 'total_volume': 233426961, 'high_24h': 0.767711, 'low_24h': 0.705277, 'price_change_24h': -0.0452137306854673, 'price_change_percentage_24h': -5.97708, 'market_cap_change_24h': -67890921.7707479, 'market_cap_change_percentage_24h': -6.23523, 'circulating_supply': 1436606124.1074963, 'total_supply': 10000000000.0, 'max_supply': 10000000000.0, 'ath': 1.48, 'ath_change_percentage': -51.93391, 'ath_date': '2024-06-03T08:29:55.744Z', 'atl': 0.082171, 'atl_change_percentage': 765.00038, 'atl_date': '2024-01-18T12:14:30.524Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.869Z'}, {'id': 'worldcoin-wld', 'symbol': 'wld', 'name': 'Worldcoin', 'image': 'https://coin-images.coingecko.com/coins/images/31069/large/worldcoin.jpeg?1696529903', 'current_price': 1.95, 'market_cap': 994535999, 'market_cap_rank': 80, 'fully_diluted_valuation': 19530175731, 'total_volume': 540964835, 'high_24h': 2.06, 'low_24h': 1.91, 'price_change_24h': -0.008271319197278306, 'price_change_percentage_24h': -0.42173, 'market_cap_change_24h': 4400790, 'market_cap_change_percentage_24h': 0.44446, 'circulating_supply': 509230440.396566, 'total_supply': 10000000000.0, 'max_supply': 10000000000.0, 'ath': 11.74, 'ath_change_percentage': -83.29774, 'ath_date': '2024-03-10T00:10:42.330Z', 'atl': 0.973104, 'atl_change_percentage': 101.51475, 'atl_date': '2023-09-13T07:36:44.064Z', 'roi': None, 'last_updated': '2024-10-08T00:53:49.245Z'}, {'id': 'quant-network', 'symbol': 'qnt', 'name': 'Quant', 'image': 'https://coin-images.coingecko.com/coins/images/3370/large/5ZOu7brX_400x400.jpg?1696504070', 'current_price': 67.87, 'market_cap': 986909088, 'market_cap_rank': 81, 'fully_diluted_valuation': 991544793, 'total_volume': 18537323, 'high_24h': 71.06, 'low_24h': 67.1, 'price_change_24h': -2.311314272384635, 'price_change_percentage_24h': -3.2935, 'market_cap_change_24h': -34028927.77297032, 'market_cap_change_percentage_24h': -3.3331, 'circulating_supply': 14544176.164091088, 'total_supply': 14612493.0, 'max_supply': 14612493.0, 'ath': 427.42, 'ath_change_percentage': -84.11226, 'ath_date': '2021-09-11T09:15:00.668Z', 'atl': 0.215773, 'atl_change_percentage': 31371.79506, 'atl_date': '2018-08-23T00:00:00.000Z', 'roi': {'times': 10.988597073984174, 'currency': 'eth', 'percentage': 1098.8597073984174}, 'last_updated': '2024-10-08T00:54:10.405Z'}, {'id': 'lido-dao', 'symbol': 'ldo', 'name': 'Lido DAO', 'image': 'https://coin-images.coingecko.com/coins/images/13573/large/Lido_DAO.png?1696513326', 'current_price': 1.079, 'market_cap': 965929778, 'market_cap_rank': 82, 'fully_diluted_valuation': 1078912610, 'total_volume': 116984366, 'high_24h': 1.14, 'low_24h': 1.07, 'price_change_24h': -0.02206092767483625, 'price_change_percentage_24h': -2.0029, 'market_cap_change_24h': -19209916.68047166, 'market_cap_change_percentage_24h': -1.94997, 'circulating_supply': 895280831.178504, 'total_supply': 1000000000.0, 'max_supply': 1000000000.0, 'ath': 7.3, 'ath_change_percentage': -85.19627, 'ath_date': '2021-08-20T08:35:20.158Z', 'atl': 0.40615, 'atl_change_percentage': 166.18197, 'atl_date': '2022-06-18T20:55:12.035Z', 'roi': None, 'last_updated': '2024-10-08T00:54:09.774Z'}, {'id': 'kucoin-shares', 'symbol': 'kcs', 'name': 'KuCoin', 'image': 'https://coin-images.coingecko.com/coins/images/1047/large/sa9z79.png?1696502152', 'current_price': 7.95, 'market_cap': 956761609, 'market_cap_rank': 83, 'fully_diluted_valuation': 1135548069, 'total_volume': 105920, 'high_24h': 8.13, 'low_24h': 7.88, 'price_change_24h': -0.01620126524070553, 'price_change_percentage_24h': -0.20344, 'market_cap_change_24h': -1863355.6644928455, 'market_cap_change_percentage_24h': -0.19438, 'circulating_supply': 120406971.14608, 'total_supply': 142906971.14608, 'max_supply': None, 'ath': 28.83, 'ath_change_percentage': -72.50285, 'ath_date': '2021-12-01T15:09:35.541Z', 'atl': 0.342863, 'atl_change_percentage': 2212.11774, 'atl_date': '2019-02-07T00:00:00.000Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.466Z'}, {'id': 'jasmycoin', 'symbol': 'jasmy', 'name': 'JasmyCoin', 'image': 'https://coin-images.coingecko.com/coins/images/13876/large/JASMY200x200.jpg?1696513620', 'current_price': 0.01934656, 'market_cap': 936477923, 'market_cap_rank': 84, 'fully_diluted_valuation': 967036269, 'total_volume': 96934539, 'high_24h': 0.02058099, 'low_24h': 0.01921798, 'price_change_24h': -0.000913663078247352, 'price_change_percentage_24h': -4.50964, 'market_cap_change_24h': -43929303.850437164, 'market_cap_change_percentage_24h': -4.48072, 'circulating_supply': 48419999999.3058, 'total_supply': 50000000000.0, 'max_supply': 50000000000.0, 'ath': 4.79, 'ath_change_percentage': -99.59613, 'ath_date': '2021-02-16T03:53:32.207Z', 'atl': 0.00275026, 'atl_change_percentage': 603.41823, 'atl_date': '2022-12-29T20:41:09.113Z', 'roi': None, 'last_updated': '2024-10-08T00:54:09.140Z'}, {'id': 'bitcoin-cash-sv', 'symbol': 'bsv', 'name': 'Bitcoin SV', 'image': 'https://coin-images.coingecko.com/coins/images/6799/large/BSV.png?1696507128', 'current_price': 45.98, 'market_cap': 908863334, 'market_cap_rank': 85, 'fully_diluted_valuation': 965558272, 'total_volume': 14295577, 'high_24h': 47.42, 'low_24h': 45.54, 'price_change_24h': -1.3404163753512677, 'price_change_percentage_24h': -2.83266, 'market_cap_change_24h': -26325157.159350753, 'market_cap_change_percentage_24h': -2.81496, 'circulating_supply': 19766937.5, 'total_supply': 21000000.0, 'max_supply': 21000000.0, 'ath': 489.75, 'ath_change_percentage': -90.62735, 'ath_date': '2021-04-16T17:09:04.630Z', 'atl': 21.43, 'atl_change_percentage': 114.14966, 'atl_date': '2023-06-10T04:32:12.266Z', 'roi': None, 'last_updated': '2024-10-08T00:54:05.350Z'}, {'id': 'conflux-token', 'symbol': 'cfx', 'name': 'Conflux', 'image': 'https://coin-images.coingecko.com/coins/images/13079/large/3vuYMbjN.png?1696512867', 'current_price': 0.198997, 'market_cap': 897133314, 'market_cap_rank': 87, 'fully_diluted_valuation': 1099701725, 'total_volume': 136203952, 'high_24h': 0.197642, 'low_24h': 0.181839, 'price_change_24h': 0.01636931, 'price_change_percentage_24h': 8.96323, 'market_cap_change_24h': 70613398, 'market_cap_change_percentage_24h': 8.54346, 'circulating_supply': 4526069775.50803, 'total_supply': 5548034679.73236, 'max_supply': None, 'ath': 1.7, 'ath_change_percentage': -88.39547, 'ath_date': '2021-03-27T03:43:35.178Z', 'atl': 0.02199898, 'atl_change_percentage': 797.20122, 'atl_date': '2022-12-30T08:16:30.345Z', 'roi': None, 'last_updated': '2024-10-08T00:53:33.560Z'}, {'id': 'bittorrent', 'symbol': 'btt', 'name': 'BitTorrent', 'image': 'https://coin-images.coingecko.com/coins/images/22457/large/btt_logo.png?1696521780', 'current_price': 9.23941e-07, 'market_cap': 894567068, 'market_cap_rank': 86, 'fully_diluted_valuation': 914665287, 'total_volume': 26689838, 'high_24h': 9.43491e-07, 'low_24h': 9.21746e-07, 'price_change_24h': -1.8515579683e-08, 'price_change_percentage_24h': -1.96461, 'market_cap_change_24h': -16353484.723588705, 'market_cap_change_percentage_24h': -1.79527, 'circulating_supply': 968246428571000.0, 'total_supply': 990000000000000.0, 'max_supply': 990000000000000.0, 'ath': 3.43e-06, 'ath_change_percentage': -73.05988, 'ath_date': '2022-01-21T04:00:31.909Z', 'atl': 3.65368e-07, 'atl_change_percentage': 153.00723, 'atl_date': '2023-10-13T05:10:41.241Z', 'roi': None, 'last_updated': '2024-10-08T00:54:05.559Z'}, {'id': 'based-brett', 'symbol': 'brett', 'name': 'Brett', 'image': 'https://coin-images.coingecko.com/coins/images/35529/large/1000050750.png?1709031995', 'current_price': 0.088115, 'market_cap': 873218316, 'market_cap_rank': 88, 'fully_diluted_valuation': 873218316, 'total_volume': 58537246, 'high_24h': 0.092752, 'low_24h': 0.085926, 'price_change_24h': 0.00054038, 'price_change_percentage_24h': 0.61706, 'market_cap_change_24h': 5528824, 'market_cap_change_percentage_24h': 0.63719, 'circulating_supply': 9910164240.66912, 'total_supply': 9910164240.66912, 'max_supply': 9999998988.0, 'ath': 0.193328, 'ath_change_percentage': -54.36865, 'ath_date': '2024-06-09T12:55:51.835Z', 'atl': 0.00084753, 'atl_change_percentage': 10308.88083, 'atl_date': '2024-02-29T08:40:24.951Z', 'roi': None, 'last_updated': '2024-10-08T00:54:05.104Z'}, {'id': 'coredaoorg', 'symbol': 'core', 'name': 'Core', 'image': 'https://coin-images.coingecko.com/coins/images/28938/large/file_589.jpg?1701868471', 'current_price': 0.937112, 'market_cap': 858110315, 'market_cap_rank': 89, 'fully_diluted_valuation': 1971753958, 'total_volume': 31657462, 'high_24h': 0.967114, 'low_24h': 0.932028, 'price_change_24h': -0.01422813882470342, 'price_change_percentage_24h': -1.49559, 'market_cap_change_24h': -10961374.97734642, 'market_cap_change_percentage_24h': -1.26127, 'circulating_supply': 913923187.211838, 'total_supply': 2100000000.0, 'max_supply': 2100000000.0, 'ath': 6.14, 'ath_change_percentage': -84.69421, 'ath_date': '2023-02-08T12:55:39.828Z', 'atl': 0.334237, 'atl_change_percentage': 181.33666, 'atl_date': '2023-11-03T01:20:55.441Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.590Z'}, {'id': 'fasttoken', 'symbol': 'ftn', 'name': 'Fasttoken', 'image': 'https://coin-images.coingecko.com/coins/images/28478/large/lightenicon_200x200.png?1696527472', 'current_price': 2.6, 'market_cap': 856233446, 'market_cap_rank': 90, 'fully_diluted_valuation': 2290629881, 'total_volume': 215412570, 'high_24h': 2.62, 'low_24h': 2.51, 'price_change_24h': 0.062821, 'price_change_percentage_24h': 2.47271, 'market_cap_change_24h': 21132723, 'market_cap_change_percentage_24h': 2.53056, 'circulating_supply': 328942461.97373456, 'total_supply': 880000000.0, 'max_supply': 1000000000.0, 'ath': 2.62, 'ath_change_percentage': -0.4686, 'ath_date': '2024-10-07T19:07:59.036Z', 'atl': 0.398142, 'atl_change_percentage': 553.93665, 'atl_date': '2023-01-21T10:15:39.503Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.935Z'}, {'id': 'gala', 'symbol': 'gala', 'name': 'GALA', 'image': 'https://coin-images.coingecko.com/coins/images/12493/large/GALA_token_image_-_200PNG.png?1709725869', 'current_price': 0.02149496, 'market_cap': 848757809, 'market_cap_rank': 91, 'fully_diluted_valuation': 848757654, 'total_volume': 161097632, 'high_24h': 0.02224224, 'low_24h': 0.02072146, 'price_change_24h': 0.00021638, 'price_change_percentage_24h': 1.01687, 'market_cap_change_24h': 9005263, 'market_cap_change_percentage_24h': 1.07237, 'circulating_supply': 39481499168.913, 'total_supply': 39481491922.3056, 'max_supply': 50000000000.0, 'ath': 0.824837, 'ath_change_percentage': -97.39249, 'ath_date': '2021-11-26T01:03:48.731Z', 'atl': 0.00013475, 'atl_change_percentage': 15861.25537, 'atl_date': '2020-12-28T08:46:48.367Z', 'roi': None, 'last_updated': '2024-10-08T00:54:09.522Z'}, {'id': 'ether-fi-staked-eth', 'symbol': 'eeth', 'name': 'ether.fi Staked ETH', 'image': 'https://coin-images.coingecko.com/coins/images/33049/large/ether.fi_eETH.png?1700473063', 'current_price': 2424.85, 'market_cap': 846755524, 'market_cap_rank': 92, 'fully_diluted_valuation': 4871591578, 'total_volume': 106209, 'high_24h': 2508.25, 'low_24h': 2415.47, 'price_change_24h': -41.29035543839427, 'price_change_percentage_24h': -1.67429, 'market_cap_change_24h': -18371264.200617433, 'market_cap_change_percentage_24h': -2.12353, 'circulating_supply': 349139.81169378624, 'total_supply': 2008686.70827969, 'max_supply': None, 'ath': 5307.23, 'ath_change_percentage': -54.27135, 'ath_date': '2024-08-06T16:16:30.007Z', 'atl': 2155.76, 'atl_change_percentage': 12.57876, 'atl_date': '2024-02-08T11:26:30.368Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.140Z'}, {'id': 'wormhole', 'symbol': 'w', 'name': 'Wormhole', 'image': 'https://coin-images.coingecko.com/coins/images/35087/large/womrhole_logo_full_color_rgb_2000px_72ppi_fb766ac85a.png?1708688954', 'current_price': 0.324805, 'market_cap': 836255956, 'market_cap_rank': 93, 'fully_diluted_valuation': 3243975804, 'total_volume': 219924753, 'high_24h': 0.35574, 'low_24h': 0.323118, 'price_change_24h': -0.02045301877589417, 'price_change_percentage_24h': -5.92398, 'market_cap_change_24h': -51338984.688058615, 'market_cap_change_percentage_24h': -5.78406, 'circulating_supply': 2577873594.0, 'total_supply': 10000000000.0, 'max_supply': 10000000000.0, 'ath': 1.66, 'ath_change_percentage': -80.38595, 'ath_date': '2024-04-03T11:46:35.308Z', 'atl': 0.16434, 'atl_change_percentage': 97.84984, 'atl_date': '2024-08-05T06:26:45.610Z', 'roi': None, 'last_updated': '2024-10-08T00:53:48.826Z'}, {'id': 'flow', 'symbol': 'flow', 'name': 'Flow', 'image': 'https://coin-images.coingecko.com/coins/images/13446/large/5f6294c0c7a8cda55cb1c936_Flow_Wordmark.png?1696513210', 'current_price': 0.540995, 'market_cap': 831299188, 'market_cap_rank': 94, 'fully_diluted_valuation': 831299188, 'total_volume': 58056753, 'high_24h': 0.561971, 'low_24h': 0.53686, 'price_change_24h': -0.01656192450478211, 'price_change_percentage_24h': -2.97044, 'market_cap_change_24h': -24892946.25119114, 'market_cap_change_percentage_24h': -2.9074, 'circulating_supply': 1536086728.20855, 'total_supply': 1536086728.20855, 'max_supply': None, 'ath': 42.4, 'ath_change_percentage': -98.72532, 'ath_date': '2021-04-05T13:49:10.098Z', 'atl': 0.391969, 'atl_change_percentage': 37.87491, 'atl_date': '2023-09-11T19:41:06.528Z', 'roi': None, 'last_updated': '2024-10-08T00:54:08.411Z'}, {'id': 'notcoin', 'symbol': 'not', 'name': 'Notcoin', 'image': 'https://coin-images.coingecko.com/coins/images/33453/large/rFmThDiD_400x400.jpg?1701876350', 'current_price': 0.00801109, 'market_cap': 822138290, 'market_cap_rank': 95, 'fully_diluted_valuation': 822138290, 'total_volume': 264338630, 'high_24h': 0.00839936, 'low_24h': 0.00772693, 'price_change_24h': 0.00011801, 'price_change_percentage_24h': 1.49515, 'market_cap_change_24h': 13636202, 'market_cap_change_percentage_24h': 1.6866, 'circulating_supply': 102474422538.643, 'total_supply': 102474422538.643, 'max_supply': None, 'ath': 0.02836145, 'ath_change_percentage': -71.61676, 'ath_date': '2024-06-02T18:00:38.587Z', 'atl': 0.00461057, 'atl_change_percentage': 74.59678, 'atl_date': '2024-05-24T07:12:14.147Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.088Z'}, {'id': 'beam-2', 'symbol': 'beam', 'name': 'Beam', 'image': 'https://coin-images.coingecko.com/coins/images/32417/large/chain-logo.png?1698114384', 'current_price': 0.01552574, 'market_cap': 801595531, 'market_cap_rank': 96, 'fully_diluted_valuation': 968773026, 'total_volume': 39916618, 'high_24h': 0.01674658, 'low_24h': 0.01538882, 'price_change_24h': -0.000940684073936141, 'price_change_percentage_24h': -5.71274, 'market_cap_change_24h': -47924263.662335515, 'market_cap_change_percentage_24h': -5.64134, 'circulating_supply': 51660007768.0, 'total_supply': 62434008330.0, 'max_supply': 62434008330.0, 'ath': 0.04416304, 'ath_change_percentage': -64.79328, 'ath_date': '2024-03-10T10:40:22.381Z', 'atl': 0.0043383, 'atl_change_percentage': 258.39735, 'atl_date': '2023-10-29T08:20:14.064Z', 'roi': None, 'last_updated': '2024-10-08T00:54:13.203Z'}, {'id': 'renzo-restaked-eth', 'symbol': 'ezeth', 'name': 'Renzo Restaked ETH', 'image': 'https://coin-images.coingecko.com/coins/images/34753/large/Ezeth_logo_circle.png?1713496404', 'current_price': 2482.56, 'market_cap': 794760666, 'market_cap_rank': 97, 'fully_diluted_valuation': 794760666, 'total_volume': 11266087, 'high_24h': 2562.78, 'low_24h': 2455.06, 'price_change_24h': -51.8338887688119, 'price_change_percentage_24h': -2.04522, 'market_cap_change_24h': -22560571.011731625, 'market_cap_change_percentage_24h': -2.76031, 'circulating_supply': 320078.067454777, 'total_supply': 320078.067454777, 'max_supply': None, 'ath': 4106.74, 'ath_change_percentage': -39.47159, 'ath_date': '2024-03-12T00:44:29.429Z', 'atl': 2198.04, 'atl_change_percentage': 13.08921, 'atl_date': '2024-01-26T08:09:59.848Z', 'roi': None, 'last_updated': '2024-10-08T00:54:10.315Z'}, {'id': 'ethena', 'symbol': 'ena', 'name': 'Ethena', 'image': 'https://coin-images.coingecko.com/coins/images/36530/large/ethena.png?1711701436', 'current_price': 0.287191, 'market_cap': 789001209, 'market_cap_rank': 98, 'fully_diluted_valuation': 4308539023, 'total_volume': 209536876, 'high_24h': 0.311956, 'low_24h': 0.283762, 'price_change_24h': -0.013023047337344384, 'price_change_percentage_24h': -4.33793, 'market_cap_change_24h': -35648060.95506203, 'market_cap_change_percentage_24h': -4.32281, 'circulating_supply': 2746875000.0, 'total_supply': 15000000000.0, 'max_supply': None, 'ath': 1.52, 'ath_change_percentage': -81.04188, 'ath_date': '2024-04-11T13:15:15.057Z', 'atl': 0.195078, 'atl_change_percentage': 47.4937, 'atl_date': '2024-09-06T21:04:48.253Z', 'roi': None, 'last_updated': '2024-10-08T00:54:04.226Z'}, {'id': 'klay-token', 'symbol': 'klay', 'name': 'Klaytn', 'image': 'https://coin-images.coingecko.com/coins/images/9672/large/klaytn.png?1696509742', 'current_price': 0.132756, 'market_cap': 770707685, 'market_cap_rank': 99, 'fully_diluted_valuation': 774944601, 'total_volume': 15981912, 'high_24h': 0.13638, 'low_24h': 0.131797, 'price_change_24h': -0.002995926554336559, 'price_change_percentage_24h': -2.20692, 'market_cap_change_24h': -19069806.84438181, 'market_cap_change_percentage_24h': -2.41458, 'circulating_supply': 5806693656.94806, 'total_supply': 5838615577.49937, 'max_supply': None, 'ath': 4.34, 'ath_change_percentage': -96.93899, 'ath_date': '2021-03-30T03:44:28.828Z', 'atl': 0.06044, 'atl_change_percentage': 119.7475, 'atl_date': '2020-04-29T08:19:34.574Z', 'roi': None, 'last_updated': '2024-10-08T00:54:11.218Z'}, {'id': 'aerodrome-finance', 'symbol': 'aero', 'name': 'Aerodrome Finance', 'image': 'https://coin-images.coingecko.com/coins/images/31745/large/token.png?1696530564', 'current_price': 1.2, 'market_cap': 769656411, 'market_cap_rank': 100, 'fully_diluted_valuation': 1557648682, 'total_volume': 55612141, 'high_24h': 1.27, 'low_24h': 1.19, 'price_change_24h': -0.0154197910139926, 'price_change_percentage_24h': -1.26784, 'market_cap_change_24h': -9478210.726666927, 'market_cap_change_percentage_24h': -1.2165, 'circulating_supply': 641571846.36209, 'total_supply': 1298428138.12827, 'max_supply': None, 'ath': 2.31, 'ath_change_percentage': -48.0214, 'ath_date': '2024-04-12T03:44:52.080Z', 'atl': 1.861e-05, 'atl_change_percentage': 6440257.63163, 'atl_date': '2023-10-17T01:23:50.860Z', 'roi': None, 'last_updated': '2024-10-08T00:54:07.760Z'}]\n\n\n\n# Loop through the data and print the name and current price\nfor coin in data:\n    name = coin['name']\n    price = coin['current_price']\n    print(f\"Coin: {name}, Price: ${price}\")\n\nCoin: Bitcoin, Price: $62490\nCoin: Ethereum, Price: $2434.1\nCoin: Tether, Price: $0.999711\nCoin: BNB, Price: $568.59\nCoin: Solana, Price: $144.62\nCoin: USDC, Price: $0.99982\nCoin: XRP, Price: $0.531761\nCoin: Lido Staked Ether, Price: $2433.54\nCoin: Dogecoin, Price: $0.109138\nCoin: TRON, Price: $0.156189\nCoin: Toncoin, Price: $5.23\nCoin: Cardano, Price: $0.35311\nCoin: Avalanche, Price: $26.86\nCoin: Shiba Inu, Price: $1.759e-05\nCoin: Wrapped stETH, Price: $2870.49\nCoin: Wrapped Bitcoin, Price: $62339\nCoin: WETH, Price: $2434.35\nCoin: Chainlink, Price: $11.22\nCoin: Bitcoin Cash, Price: $325.66\nCoin: Polkadot, Price: $4.15\nCoin: Dai, Price: $0.999811\nCoin: Sui, Price: $2.06\nCoin: NEAR Protocol, Price: $5.11\nCoin: LEO Token, Price: $6.01\nCoin: Uniswap, Price: $7.24\nCoin: Litecoin, Price: $65.04\nCoin: Bittensor, Price: $617.07\nCoin: Aptos, Price: $8.91\nCoin: Pepe, Price: $9.89e-06\nCoin: Wrapped eETH, Price: $2554.88\nCoin: Artificial Superintelligence Alliance, Price: $1.49\nCoin: Internet Computer, Price: $8.13\nCoin: Kaspa, Price: $0.137693\nCoin: POL (ex-MATIC), Price: $0.376926\nCoin: Ethereum Classic, Price: $18.7\nCoin: Stellar, Price: $0.091505\nCoin: Monero, Price: $144.06\nCoin: Stacks, Price: $1.77\nCoin: First Digital USD, Price: $0.999501\nCoin: dogwifhat, Price: $2.56\nCoin: OKB, Price: $41.75\nCoin: Ethena USDe, Price: $0.998868\nCoin: Immutable, Price: $1.49\nCoin: Filecoin, Price: $3.74\nCoin: Aave, Price: $146.68\nCoin: Cronos, Price: $0.078844\nCoin: Optimism, Price: $1.67\nCoin: Render, Price: $5.32\nCoin: Injective, Price: $20.63\nCoin: Arbitrum, Price: $0.55172\nCoin: Hedera, Price: $0.052714\nCoin: Mantle, Price: $0.594723\nCoin: Fantom, Price: $0.680296\nCoin: VeChain, Price: $0.02305745\nCoin: Cosmos Hub, Price: $4.44\nCoin: THORChain, Price: $5.09\nCoin: WhiteBIT Coin, Price: $11.61\nCoin: The Graph, Price: $0.16643\nCoin: Sei, Price: $0.436364\nCoin: Bitget Token, Price: $1.075\nCoin: Bonk, Price: $2.134e-05\nCoin: Binance-Peg WETH, Price: $2434.55\nCoin: FLOKI, Price: $0.00013831\nCoin: Rocket Pool ETH, Price: $2719.44\nCoin: Theta Network, Price: $1.31\nCoin: Popcat, Price: $1.28\nCoin: Arweave, Price: $19.03\nCoin: Maker, Price: $1406.89\nCoin: Mantle Staked Ether, Price: $2540.6\nCoin: MANTRA, Price: $1.4\nCoin: Pyth Network, Price: $0.327718\nCoin: Helium, Price: $6.89\nCoin: Solv Protocol SolvBTC, Price: $62563\nCoin: Celestia, Price: $5.39\nCoin: Gate, Price: $8.86\nCoin: Jupiter, Price: $0.772541\nCoin: Algorand, Price: $0.125351\nCoin: Polygon, Price: $0.377139\nCoin: Ondo, Price: $0.711239\nCoin: Worldcoin, Price: $1.95\nCoin: Quant, Price: $67.87\nCoin: Lido DAO, Price: $1.079\nCoin: KuCoin, Price: $7.95\nCoin: JasmyCoin, Price: $0.01934656\nCoin: Bitcoin SV, Price: $45.98\nCoin: Conflux, Price: $0.198997\nCoin: BitTorrent, Price: $9.23941e-07\nCoin: Brett, Price: $0.088115\nCoin: Core, Price: $0.937112\nCoin: Fasttoken, Price: $2.6\nCoin: GALA, Price: $0.02149496\nCoin: ether.fi Staked ETH, Price: $2424.85\nCoin: Wormhole, Price: $0.324805\nCoin: Flow, Price: $0.540995\nCoin: Notcoin, Price: $0.00801109\nCoin: Beam, Price: $0.01552574\nCoin: Renzo Restaked ETH, Price: $2482.56\nCoin: Ethena, Price: $0.287191\nCoin: Klaytn, Price: $0.132756\nCoin: Aerodrome Finance, Price: $1.2",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "Reading data.html#independent-study",
    "href": "Reading data.html#independent-study",
    "title": "5  Reading Data",
    "section": "5.6 Independent Study",
    "text": "5.6 Independent Study\n\n5.6.1 Practice exercise 1: Reading .csv data\nRead the file Top 10 Albums By Year.csv. This file contains the top 10 albums for each year from 1990 to 2021. Each row corresponds to a unique album.\n\n5.6.1.1 \nPrint the first 5 rows of the data.\n\n\n5.6.1.2 \nHow many rows and columns are there in the data?\n\n\n5.6.1.3 \nPrint the summary statistics of the data, and answer the following questions:\n\nWhat proportion of albums have 15 or lesser tracks? Mention a range for the proportion.\nWhat is the mean length of a track (in minutes)?\n\n\n\n5.6.1.4 \nFind the album having the highest worldwide sales per year, and its artist.\n\n\n5.6.1.5 \nSubset the data to include only Hip-Hop albums. How many Hip_Hop albums are there?\n\n\n5.6.1.6 \nWhich album amongst hip-hop has the higest mean sales per year per track, and who is its artist?\n\n\n\n5.6.2 Practice exercise 2: Reading .txt data\nRead the file bestseller_books.txt. It contains top 50 best-selling books on amazon from 2009 to 2019. Identify the delimiter without opening the file with Notepad or a text-editing software. How many rows and columns are there in the dataset?\nSolution:\n\n#The delimiter seems to be ';' based on the output of the above code\nbestseller_books = pd.read_csv('../Data/bestseller_books.txt',sep=';')\nbestseller_books.head()\n\n\n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n0\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n1\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n2\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n3\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n4\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\n#The file read with ';' as the delimited is correct\nprint(\"The file has\",bestseller_books.shape[0],\"rows and\",bestseller_books.shape[1],\"columns\")\n\nThe file has 550 rows and 9 columns\n\n\nAlternatively, you can use the argument sep = None, and engine = 'python'. The default engine is C. However, the ‘python’ engine has a ‘sniffer’ tool which may identify the delimiter automatically.\n\nbestseller_books = pd.read_csv('../data/bestseller_books.txt',sep=None, engine = 'python')\nbestseller_books.head()\n\n\n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n0\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n1\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n2\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n3\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n4\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\n\n5.6.3 Practice exercise 3: Reading HTML data\nRead the table(s) consisting of attendance of spectators in FIFA worlds cup from this page. Read only those table(s) that have the word ‘attendance’ in them. How many rows and columns are there in the table(s)?\n\ndfs = pd.read_html('https://en.wikipedia.org/wiki/FIFA_World_Cup',\n                       match='reaching')\nprint(len(dfs))\ndata = dfs[0]\nprint(\"Number of rows =\",data.shape[0], \"and number of columns=\",data.shape[1])\ndata.head()\n\n1\nNumber of rows = 25 and number of columns= 6\n\n\n\n\n\n\n\n\n\nTeam\nTitles\nRunners-up\nThird place\nFourth place\nTop 4 total\n\n\n\n\n0\nBrazil\n5 (1958, 1962, 1970, 1994, 2002)\n2 (1950 *, 1998)\n2 (1938, 1978)\n2 (1974, 2014 *)\n11\n\n\n1\nGermany1\n4 (1954, 1974 *, 1990, 2014)\n4 (1966, 1982, 1986, 2002)\n4 (1934, 1970, 2006 *, 2010)\n1 (1958)\n13\n\n\n2\nItaly\n4 (1934 *, 1938, 1982, 2006)\n2 (1970, 1994)\n1 (1990 *)\n1 (1978)\n8\n\n\n3\nArgentina\n3 (1978 *, 1986, 2022)\n3 (1930, 1990, 2014)\nNaN\nNaN\n6\n\n\n4\nFrance\n2 (1998 *, 2018)\n2 (2006, 2022)\n2 (1958, 1986)\n1 (1982)\n7\n\n\n\n\n\n\n\n\n\n5.6.4 Practice exercise 4: Reading JSON data\nRead the movies dataset from here. How many rows and columns are there in the data?\n\nmovies_data = pd.read_json('https://raw.githubusercontent.com/vega/vega-datasets/master/data/movies.json')\nprint(\"Number of rows =\",movies_data.shape[0], \"and number of columns=\",movies_data.shape[1])\n\nNumber of rows = 3201 and number of columns= 16",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading Data</span>"
    ]
  },
  {
    "objectID": "NumPy.html",
    "href": "NumPy.html",
    "title": "6  NumPy",
    "section": "",
    "text": "6.1 Learning Objectives\nNumPy is a foundational library in Python, providing support for large, multi-dimensional arrays and matrices, along with a variety of mathematical functions. It’s a critical tool in data science and machine learning because it enables efficient numerical computations, data manipulation, and linear algebra operations. Many machine learning algorithms rely on these operations to process data and perform complex calculations quickly. Moreover, popular libraries like Pandas, SciPy, and TensorFlow are built on top of NumPy, making it essential to understand for implementing and optimizing machine learning models.\nBy the end of this lecture, students should be able to:",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#learning-objectives",
    "href": "NumPy.html#learning-objectives",
    "title": "6  NumPy",
    "section": "",
    "text": "Understand the basic structure and functionarlity of NumPy\nCreate and manipulate NumPy arrays.\nPerform mathematical and statistical operations on arrays.\nUtilize advanced concepts such as slicing, broadcasting, and vectorization.\nApply NumPy operations to real-world data science problems.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#getting-started",
    "href": "NumPy.html#getting-started",
    "title": "6  NumPy",
    "section": "6.2 Getting Started",
    "text": "6.2 Getting Started\n\n\nCode\n\nimport numpy as np\nimport pandas as pd\n\n\nIf you encounter a ‘ModuleNotFoundError’, please ensure that the module is installed in your current environment before attempting to import it\n\n\nCode\n#Using the NumPy function array() to define a NumPy array\nnumpy_array = np.array([[1,2],[3,4]])\nnumpy_array\n\n\narray([[1, 2],\n       [3, 4]])\n\n\n\n\nCode\ntype(numpy_array)\n\n\nnumpy.ndarray\n\n\nNumpy arrays can have any number of dimensions and different lengths along each dimension. We can inspect the length along each dimension using the .shape property of an array.\n\n\n\nCode\nnumpy_array.ndim\n\n\n2",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#data-types-in-numpy",
    "href": "NumPy.html#data-types-in-numpy",
    "title": "6  NumPy",
    "section": "6.3 Data Types in NumPy",
    "text": "6.3 Data Types in NumPy\nUnlike lists and tuples, NumPy arrays are designed to store elements of the same type, enabling more efficient memory usage and faster computations. The data type of the elements in a NumPy array can be accessed using the .dtype attribute\n\n\nCode\nnumpy_array.dtype\n\n\ndtype('int64')\n\n\nNumPy supports a wide range of data types, each with a specific memory size. Here is a list of common NumPy data types and the memory they consume:\nNote that: these data type correspond directly to C data types, since NumPy uses C for the core computational operations. This C implementation allows NumPy to perform array operations much faster and more efficiently than native Python data structures.\n\n\n\nData Type\nMemory Size\n\n\n\n\nnp.int8\n1 byte\n\n\nnp.int16\n2 bytes\n\n\nnp.int32\n4 bytes\n\n\nnp.int64\n8 bytes\n\n\nnp.uint8\n1 byte\n\n\nnp.uint16\n2 bytes\n\n\nnp.uint32\n4 bytes\n\n\nnp.uint64\n8 bytes\n\n\nnp.float16\n2 bytes\n\n\nnp.float32\n4 bytes\n\n\nnp.float64\n8 bytes\n\n\nnp.complex64\n8 bytes\n\n\nnp.complex128\n16 bytes\n\n\nnp.bool_\n1 byte\n\n\nnp.string_\n1 byte per character\n\n\nnp.unicode_\n4 bytes per character\n\n\nnp.object_\nVariable\n\n\nnp.datetime64\n8 bytes\n\n\nnp.timedelta64\n8 bytes\n\n\n\n\n6.3.1 Upcasting\nWhen creating a NumPy array with elements of different data types, NumPy automatically attempts to upcast the elements to a compatible data type that can accommodate all of them. This process is known as type coercion or type promotion. The rules for upcasting follow a hierarchy of data types to ensure no data is lost.\nBelow are two common types of upcasting with examples:\nNumeric Upcasting: If you mix integers and floats, NumPy will convert the entire array to floats.\n\n\nCode\narr = np.array([1, 2.5, 3])\nprint(arr.dtype)  \n\n\nfloat64\n\n\nString Upcasting: If you mix numbers and strings, NumPy will upcast all elements to strings.\n\n\nCode\narr = np.array([1, 'hello', 3.5])\nprint(arr.dtype)\n\n\n&lt;U32\n\n\n&lt;U21 is a NumPy data type that stands for a Unicode string with a maximum length of 21 characters.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#why-do-we-need-numpy-arrays",
    "href": "NumPy.html#why-do-we-need-numpy-arrays",
    "title": "6  NumPy",
    "section": "6.4 Why do we need NumPy arrays?",
    "text": "6.4 Why do we need NumPy arrays?\nNumPy arrays can store data similarly to lists and tuples, and the same computations can be performed across these structures. However, NumPy is preferred because it is significantly more efficient, particularly when handling large datasets, due to its optimized memory usage and computational speed.\n\n6.4.1 Numpy arrays are memory efficient: Homogeneity and Contiguous Memory Storage\nA NumPy array is a collection of elements of the same data type, stored in contiguous memory locations. In contrast, data structures like lists can hold elements of different data types, stored in non-contiguous memory locations. This homogeneity and contiguous storage allow NumPy arrays to be densely packed, leading to lower memory consumption. The following example demonstrates how NumPy arrays are more memory-efficient compared to other data structures.\n\n\nCode\nimport sys\n\n# Create a NumPy array, Python list, and tuple with the same elements\narray = np.arange(1000)\npy_list = list(range(1000))\npy_tuple = tuple(range(1000))\n\n# Calculate memory usage\narray_memory = array.nbytes\nlist_memory = sys.getsizeof(py_list) + sum(sys.getsizeof(item) for item in py_list)\ntuple_memory = sys.getsizeof(py_tuple) + sum(sys.getsizeof(item) for item in py_tuple)\n\n# Display the memory usage\nmemory_usage = {\n    \"NumPy Array (in bytes)\": array_memory,\n    \"Python List (in bytes)\": list_memory,\n    \"Python Tuple (in bytes)\": tuple_memory\n}\n\nmemory_usage\n\n\n{'NumPy Array (in bytes)': 8000,\n 'Python List (in bytes)': 36056,\n 'Python Tuple (in bytes)': 36040}\n\n\n\n\nCode\n# each element in the array is a 64-bit integer\narray.dtype\n\n\ndtype('int64')\n\n\n\n\n6.4.2 NumPy arrays are fast\nWith NumPy arrays, mathematical computations can be performed faster, as compared to other data structures, due to the following reasons:\n\nAs the NumPy array is densely packed with homogenous data, it helps retrieve the data faster as well, thereby making computations faster.\nWith NumPy, vectorized computations can replace the relatively more expensive python for loops. The NumPy package breaks down the vectorized computations into multiple fragments and then processes all the fragments parallelly. However, with a for loop, computations will be one at a time.\nThe NumPy package integrates C, and C++ codes in Python. These programming languages have very little execution time as compared to Python.\n\nWe’ll see the faster speed on NumPy computations in the example below.\nExample: This example shows that computations using NumPy arrays are typically much faster than computations with other data structures.\nQ: Multiply whole numbers up to 1 million by an integer, say 2. Compare the time taken for the computation if the numbers are stored in a NumPy array vs a list.\nUse the numpy function arange() to define a one-dimensional NumPy array.\n\n\nCode\n#Examples showing NumPy arrays are more efficient for numerical computation\nimport time as tm\n\n# List comprehension to multiply each element by 2\nstart_time = tm.time()\nlist_ex = list(range(1000000))  # List containing whole numbers up to 1 million\na = [x * 2 for x in list_ex]    # Multiply each element by 2\nprint(\"Time taken to multiply numbers in a list = \", tm.time() - start_time)\n\n# Tuple - converting to list for multiplication, then back to tuple\nstart_time = tm.time()\ntuple_ex = tuple(range(1000000))  # Tuple containing whole numbers up to 1 million\na = tuple(x * 2 for x in tuple_ex)  # Multiply each element by 2\nprint(\"Time taken to multiply numbers in a tuple = \", tm.time() - start_time)\n\n# NumPy array element-wise multiplication\nstart_time = tm.time()\nnumpy_ex = np.arange(1000000)  # NumPy array containing whole numbers up to 1 million\na = numpy_ex * 2               # Multiply each element by 2\nprint(\"Time taken to multiply numbers in a NumPy array = \", tm.time() - start_time)\n\n\nTime taken to multiply numbers in a list =  0.049832820892333984\nTime taken to multiply numbers in a tuple =  0.08172726631164551\nTime taken to multiply numbers in a NumPy array =  0.010966300964355469",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#basics-of-numpy-arrays",
    "href": "NumPy.html#basics-of-numpy-arrays",
    "title": "6  NumPy",
    "section": "6.5 Basics of NumPy Arrays",
    "text": "6.5 Basics of NumPy Arrays\n\n6.5.1 Creating NumPy Arrays\n\n6.5.1.1 Creating Arrays for specific use cases:\nYou can create a NumPy array using various methods, such as:\n\nnp.array(): Creates an array from a list or iterable.\nnp.zeros(), np.ones(): Creates arrays filled with zeros or ones.\nnp.arange(), np.linespace(): Creates arrays with evenly spaced values.\nnp.random module: Generates arrays with random values.\n\nEach method is designed for different use cases. I encourage you to explore and experiment with these functions to see the types of arrays they produce and how they can be used in different scenarios.\n\n\n6.5.1.2 Loading data from file into a NumPy array\n\nnumpy.loadtxt(): Reading simple numerical text files\nnp.genfromtxt(): Reading more complex text files\ndf.to_numpy(): Reading tabular data into pandas dataframe, then using `to_numpy()’ convert it to NumPy array\n\n\n\n\n6.5.2 Array Attributes:\nLet us define a NumPy array in order to access its attributes:\n\n\nCode\nnumpy_ex = np.array([[1,2,3],[4,5,6]])\nnumpy_ex\n\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nThe attributes of numpy_ex can be seen by typing numpy_ex followed by a ., and then pressing the tab key.\nSome of the basic attributes of a NumPy array are the following:\n\n6.5.2.1 ndim\nShows the number of dimensions (or axes) of the array.\n\n\nCode\nnumpy_ex.ndim\n\n\n2\n\n\n\n\n6.5.2.2 shape\nThis is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, the shape will be (n,m). The length of the shape tuple is therefore the rank, or the number of dimensions, ndim.\n\n\nCode\nnumpy_ex.shape\n\n\n(2, 3)\n\n\n\n\n6.5.2.3 size\nThis is the total number of elements of the array, which is the product of the elements of shape.\n\n\nCode\nnumpy_ex.size\n\n\n6\n\n\n\n\n6.5.2.4 dtype\nThis is an object describing the type of the elements in the array. One can create or specify dtype’s using standard Python types. NumPy provides many, for example bool_, character, int_, int8, int16, int32, int64, float_, float8, float16, float32, float64, complex_, complex64, object_.\n\n\nCode\nnumpy_ex.dtype\n\n\ndtype('int64')",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#array-indexing-and-slicing",
    "href": "NumPy.html#array-indexing-and-slicing",
    "title": "6  NumPy",
    "section": "6.6 Array Indexing and Slicing",
    "text": "6.6 Array Indexing and Slicing\n\n6.6.1 Array Indexing\nSimilar to Python lists, NumPy uses zero-based indexing, meaning the first element of an array is accessed using index 0. You can use positive or negative indices to access elements\n\n\nCode\narray = np.array([10, 20, 30, 40, 50])\n\nprint(array[0])  \nprint(array[4]) \nprint(array[-1])  \nprint(array[-3])  \n\n\n10\n50\n50\n30\n\n\nIn multi-dimensional arrays, indices are separated by commas. The first index refers to the row, and the second index refers to the column in a 2D array.\n\n\nCode\n# 2D array (3 rows, 3 columns)\narray_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(array_2d)\nprint(array_2d[0, 1])  \nprint(array_2d[1, -1]) \nprint(array_2d[-1, -1])  \n\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n2\n6\n9\n\n\nYou can use boolean arrays to filter or select elements based on a condition\n\n\nCode\narray = np.array([10, 20, 30, 40, 50])\nmask = array &gt; 30  # Boolean mask for elements greater than 30\nprint(array[mask])  # Output: [40 50]\n\n\n[40 50]\n\n\n\n\n6.6.2 Array Slicing\nSlicing is used to extract a sub-array from an existing array.\nThe Syntax for slicing is `array[start:stop:step]\n\n\nCode\narray = np.array([10, 20, 30, 40, 50])\n\nprint(array[1:4])  \nprint(array[:3])  \nprint(array[2:])  \nprint(array[::2])  \nprint(array[::-1]) \n\n\n[20 30 40]\n[10 20 30]\n[30 40 50]\n[10 30 50]\n[50 40 30 20 10]\n\n\nFor slicing in Multi-Dimensional Arrays, use commas to separate slicing for different dimensions\n\n\nCode\narray_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Extract a sub-array: elements from the first two rows and the first two columns\nsub_array = array_2d[:2, :2]\nprint(sub_array)  \n\n# Extract all rows for the second column\ncol = array_2d[:, 1]\nprint(col) \n\n# Extract the last two rows and last two columns\nsub_array = array_2d[-2:, -2:]\nprint(sub_array)  \n\n\n[[1 2]\n [4 5]]\n[2 5 8]\n[[5 6]\n [8 9]]\n\n\nThe step parameter can be used to select elements at regular intervals.\n\n\nCode\narray = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nprint(array[1:8:2]) \nprint(array[::-2])  \n\n\n[1 3 5 7]\n[9 7 5 3 1]\n\n\n\n\n6.6.3 Modify Sub-Arrays through Slicing\nSlices are views of the original array, not copies. Modifying a slice will change the original array.\n\n\nCode\narray = np.array([10, 20, 30, 40, 50])\narray[1:4] = 100  # Replace elements from index 1 to 3 with 100\nprint(array)  # Output: [ 10 100 100 100  50]\n\n\n[ 10 100 100 100  50]\n\n\n\n\n6.6.4 Combining Indexing and Slicing\nYou can combine indexing and slicing to extract specific elements or sub-arrays\n\n\nCode\n# Create a 3D array\narray_3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n\n# Select specific elements and slices\nprint(array_3d[0, :, 1])  # Output: [2 5] (second element from each row in the first sub-array)\nprint(array_3d[1, 1, :2])  # Output: [10 11] (first two elements in the last row of the second sub-array)\n\n\n[2 5]\n[10 11]\n\n\n\n\n6.6.5 np.where() and np.select()\nYou can also use np.where and np.select for array slicing and conditional selection.\n\n\nCode\narray_3d\n\n\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\n\nCode\n# Using np.where to create a mask and select elements greater than 5\ngreater_than_5 = np.where(array_3d &gt; 5, array_3d, 0)\nprint(\"Elements greater than 5:\\n\", greater_than_5)\n\n# Using np.select to categorize elements into three categories\nconditions = [array_3d &lt; 4, (array_3d &gt;= 4) & (array_3d &lt;= 8), array_3d &gt; 8]\nchoices = ['low', 'medium', 'high']\ncategorized_array = np.select(conditions, choices, default='unknown')\nprint(\"\\nCategorized array:\\n\", categorized_array)\n\n\nElements greater than 5:\n [[[ 0  0  0]\n  [ 0  0  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\n\nCategorized array:\n [[['low' 'low' 'low']\n  ['medium' 'medium' 'medium']]\n\n [['medium' 'medium' 'high']\n  ['high' 'high' 'high']]]\n\n\nThis example shows how np.where and np.select can be used to filter, manipulate, and categorize elements within a 3D array based on specific conditions.\n\n\n6.6.6 np.argmin() and np.argmax()\nYou can use np.argmin and np.argmax to quickly find the index of the minimum or maximum value in an array along a specified axis. You’ll see their usage in the practice example below",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#array-operations",
    "href": "NumPy.html#array-operations",
    "title": "6  NumPy",
    "section": "6.7 Array Operations",
    "text": "6.7 Array Operations\n\n6.7.1 Arithmetic operations\nNumpy arrays support arithmetic operators like +, -, *, etc. We can perform an arithmetic operation on an array either with a single number (also called scalar) or with another array of the same shape. However, we cannot perform an arithmetic operation on an array with an array of a different shape.\nBelow are some examples of arithmetic operations on arrays.\n\n\nCode\n#Defining two arrays of the same shape\narr1 = np.array([[1, 2, 3, 4], \n                 [5, 6, 7, 8], \n                 [9, 1, 2, 3]])\narr2 = np.array([[11, 12, 13, 14], \n                 [15, 16, 17, 18], \n                 [19, 11, 12, 13]])\n\n\n\n\nCode\n#Element-wise summation of arrays\narr1 + arr2\n\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 12, 14, 16]])\n\n\n\n\nCode\n# Element-wise subtraction\narr2 - arr1\n\n\narray([[10, 10, 10, 10],\n       [10, 10, 10, 10],\n       [10, 10, 10, 10]])\n\n\n\n\nCode\n# Adding a scalar to an array adds the scalar to each element of the array\narr1 + 3\n\n\narray([[ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12,  4,  5,  6]])\n\n\n\n\nCode\n# Dividing an array by a scalar divides all elements of the array by the scalar\narr1 / 2\n\n\narray([[0.5, 1. , 1.5, 2. ],\n       [2.5, 3. , 3.5, 4. ],\n       [4.5, 0.5, 1. , 1.5]])\n\n\n\n\nCode\n# Element-wise multiplication\narr1 * arr2\n\n\narray([[ 11,  24,  39,  56],\n       [ 75,  96, 119, 144],\n       [171,  11,  24,  39]])\n\n\n\n\nCode\n# Modulus operator with scalar\narr1 % 4\n\n\narray([[1, 2, 3, 0],\n       [1, 2, 3, 0],\n       [1, 1, 2, 3]])\n\n\n\n\n6.7.2 Comparison and Logical Operation\nNumpy arrays support comparison operations like ==, !=, &gt; etc. The result is an array of booleans.\n\n\nCode\narr1 = np.array([[1, 2, 3], [3, 4, 5]])\narr2 = np.array([[2, 2, 3], [1, 2, 5]])\n\n\n\n\nCode\narr1 == arr2\n\n\narray([[False,  True,  True],\n       [False, False,  True]])\n\n\n\n\nCode\narr1 != arr2\n\n\narray([[ True, False, False],\n       [ True,  True, False]])\n\n\n\n\nCode\narr1 &gt;= arr2\n\n\narray([[False,  True,  True],\n       [ True,  True,  True]])\n\n\n\n\nCode\narr1 &lt; arr2\n\n\narray([[ True, False, False],\n       [False, False, False]])\n\n\nArray comparison is frequently used to count the number of equal elements in two arrays using the sum method. Remember that True evaluates to 1 and False evaluates to 0 when booleans are used in arithmetic operations.\n\n\nCode\n(arr1 == arr2).sum()\n\n\nnp.int64(3)\n\n\n\n\n6.7.3 Aggregate Functions: np.sum(), np.mean(), np.min(), np.max()\n\n6.7.3.1 Overall Aggregate Calculations:\n\nnp.sum(array): Calculates the sum of all elements in the array.\nnp.mean(array): Calculates the mean of all elements in the array.\nnp.min(array): Finds the minimum value in the entire array.\nnp.max(array): Finds the maximum value in the entire array.\n\n\n\n6.7.3.2 Row-Wise Calculations (axis=1):\n\nnp.sum(array, axis=1): Computes the sum of elements in each row.\nnp.mean(array, axis=1): Computes the mean of elements in each row.\nnp.min(array, axis=1): Finds the minimum value in each row.\nnp.max(array, axis=1): Finds the maximum value in each row.\n\n\n\n6.7.3.3 Column-Wise Calculations (axis=0):\n\nnp.sum(array, axis=0): Computes the sum of elements in each column.\nnp.mean(array, axis=0): Computes the mean of elements in each column.\nnp.min(array, axis=0): Finds the minimum value in each column.\nnp.max(array, axis=0): Finds the maximum value in each column.\n\n\n\nCode\n# Create a 3x4 array of integers\narray = np.array([[4, 7, 1, 3],\n                  [5, 8, 2, 6],\n                  [9, 3, 5, 2]])\n\n# Display the original array\nprint(\"Original Array:\\n\", array)\n\n# Calculate the sum, mean, minimum, and maximum for the entire array\ntotal_sum = np.sum(array)\nmean_value = np.mean(array)\nmin_value = np.min(array)\nmax_value = np.max(array)\n\nprint(f\"\\nSum of all elements: {total_sum}\")  \nprint(f\"Mean of all elements: {mean_value}\")  \nprint(f\"Minimum value in the array: {min_value}\")  \nprint(f\"Maximum value in the array: {max_value}\")  \n\n# Calculate the sum, mean, minimum, and maximum along each row (axis=1)\nrow_sum = np.sum(array, axis=1)\nrow_mean = np.mean(array, axis=1)\nrow_min = np.min(array, axis=1)\nrow_max = np.max(array, axis=1)\n\nprint(\"\\nSum along each row:\", row_sum)  \nprint(\"Mean along each row:\", row_mean)  \nprint(\"Minimum value along each row:\", row_min)  \nprint(\"Maximum value along each row:\", row_max)  \n\n# Calculate the sum, mean, minimum, and maximum along each column (axis=0)\ncol_sum = np.sum(array, axis=0)\ncol_mean = np.mean(array, axis=0)\ncol_min = np.min(array, axis=0)\ncol_max = np.max(array, axis=0)\n\nprint(\"\\nSum along each column:\", col_sum)  \nprint(\"Mean along each column:\", col_mean)  \nprint(\"Minimum value along each column:\", col_min)  \nprint(\"Maximum value along each column:\", col_max)  \n\n\nOriginal Array:\n [[4 7 1 3]\n [5 8 2 6]\n [9 3 5 2]]\n\nSum of all elements: 55\nMean of all elements: 4.583333333333333\nMinimum value in the array: 1\nMaximum value in the array: 9\n\nSum along each row: [15 21 19]\nMean along each row: [3.75 5.25 4.75]\nMinimum value along each row: [1 2 2]\nMaximum value along each row: [7 8 9]\n\nSum along each column: [18 18  8 11]\nMean along each column: [6.         6.         2.66666667 3.66666667]\nMinimum value along each column: [4 3 1 2]\nMaximum value along each column: [9 8 5 6]",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#array-reshaping",
    "href": "NumPy.html#array-reshaping",
    "title": "6  NumPy",
    "section": "6.8 Array Reshaping",
    "text": "6.8 Array Reshaping\nCertain functions and machine learning models require input data in a specific shape or format. For instance, operations like matrix multiplication and broadcasting depend on the alignment of array dimensions. Many deep learning models expect input data to be in a 4D array format (batch size, height, width, channels). Reshaping enables us to convert data into the necessary shape, ensuring compatibility without altering the underlying values.\nBelow are some methods to reshape NumPy arrays:\n\n6.8.1 reshape()\nThe reshape method in NumPy allows you to change the shape of an existing array without changing its data.\n\n\nCode\narr = np.array([1, 2, 3, 4, 5, 6])\nreshaped_arr = arr.reshape(2, 3)\nprint(reshaped_arr)\n\n\n[[1 2 3]\n [4 5 6]]\n\n\nUsing-1 for automatic dimension inference\n\n\nCode\narr = np.arange(12)\nreshaped_arr = arr.reshape(3, -1)\nprint(reshaped_arr)\n\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n\n\nHere, -1 means “calculate this dimension based on the remaining dimensions and the total size of the array”. So, (3, -1) becomes (3, 4).\n\n\n6.8.2 flatten() and ravel()\nThe flatten method returns a copy of the array collapsed into one dimension. It is useful when you need to perform operations that require 1D input or need to pass the array data as a linear sequence. order specifies the order in which elements are read, options include but not limited to: * C (default): Row-major (C-style). * F: Column-major (Fortran-style).\nIn contrast, ravel() returns a flattened view of the original array whenever possible, without creating a copy.\n\n\nCode\narr = np.array([[1, 2, 3], [4, 5, 6]])\nflattened_arr = arr.flatten()\nprint(flattened_arr)\n\n\n[1 2 3 4 5 6]\n\n\n\n\nCode\n# using ravel() to flatten the array\nflattened_arr = arr.ravel()\nprint(flattened_arr)\n\n\n[1 2 3 4 5 6]\n\n\n\n\n6.8.3 resize()\nChanges the shape and size of an array in place. Unlike reshape(), it can modify the array and fill additional elements with zeros if necessary.\n\n\nCode\narr = np.array([1, 2, 3, 4])\narr.resize(2, 3) \nprint(arr)\n\n\n[[1 2 3]\n [4 0 0]]\n\n\n\n\n6.8.4 tranpose() or T\nBoth can be used to transpose the NumPy array. This is often used to make matrices (2-dimensional arrays) compatible for multiplication.\n\n\nCode\narr = np.array([[1, 2, 3], [4, 5, 6]])\ntransposed_arr = arr.transpose()  # Output: [[1 4], [2 5], [3 6]]\nprint(transposed_arr)\n\nT_arr = arr.T  # Output: [[1 4], [2 5], [3 6]]\nprint(T_arr)\n\n\n[[1 4]\n [2 5]\n [3 6]]\n[[1 4]\n [2 5]\n [3 6]]",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#arrays-concaternating",
    "href": "NumPy.html#arrays-concaternating",
    "title": "6  NumPy",
    "section": "6.9 Arrays Concaternating",
    "text": "6.9 Arrays Concaternating\nNumPy provides several functions to concatenate arrays along different axes.\n\n6.9.1 np.concatenate()\nArrays can be concatenated along an axis with NumPy’s concatenate function. The axis argument specifies the dimension for concatenation. The arrays should have the same number of dimensions, and the same length along each axis except the axis used for concatenation.\nThe examples below show concatenation of arrays.\n\n\nCode\narr1 = np.array([[1, 2, 3], [3, 4, 5]])\narr2 = np.array([[2, 2, 3], [1, 2, 5]])\nprint(\"Array 1:\\n\",arr1)\nprint(\"Array 2:\\n\",arr2)\n\n\nArray 1:\n [[1 2 3]\n [3 4 5]]\nArray 2:\n [[2 2 3]\n [1 2 5]]\n\n\n\n\nCode\n#Concatenating the arrays along the default axis: axis=0\nnp.concatenate((arr1,arr2))\n\n\narray([[1, 2, 3],\n       [3, 4, 5],\n       [2, 2, 3],\n       [1, 2, 5]])\n\n\n\n\nCode\n#Concatenating the arrays along axis = 1\nnp.concatenate((arr1,arr2),axis=1)\n\n\narray([[1, 2, 3, 2, 2, 3],\n       [3, 4, 5, 1, 2, 5]])\n\n\nHere’s a visual explanation of np.concatenate along axis=1 (can you guess what axis=0 results in?):\n\nLet us concatenate the array below (arr3) with arr1, along axis = 0.\n\n\nCode\narr3 = np.array([2, 2, 3])\n\n\n\n\nCode\nnp.concatenate((arr1,arr3),axis=0)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[52], line 1\n----&gt; 1 np.concatenate((arr1,arr3),axis=0)\n\nValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n\n\n\nNote the above error, which indicates that arr3 has only one dimension. Let us check the shape of arr3.\n\n\nCode\narr3.shape\n\n\n(3,)\n\n\nWe can reshape arr3 to a shape of (1,3) to make it compatible for concatenation with arr1 along axis = 0.\n\n\nCode\narr3_reshaped = arr3.reshape(1,3)\narr3_reshaped\n\n\narray([[2, 2, 3]])\n\n\nNow we can concatenate the reshaped arr3 with arr1 along axis = 0.\n\n\nCode\nnp.concatenate((arr1,arr3_reshaped),axis=0)\n\n\narray([[1, 2, 3],\n       [3, 4, 5],\n       [2, 2, 3]])\n\n\n\n\n6.9.2 np.vstack() and np.hstack()\n\nnp.vstack(): Stacks arrays vertically (along rows).\nnp.hstack(): Stacks arrays horizontally (along columns).\n\n\n\nCode\n# Vertical stacking\nvstack = np.vstack((arr1, arr2))\nprint(\"\\nVertical Stack:\\n\", vstack)\n\n# Horizontal stacking\nhstack = np.hstack((arr1, arr2))\nprint(\"\\nHorizontal Stack:\\n\", hstack)\n\n\n\nVertical Stack:\n [[1 2 3]\n [3 4 5]\n [2 2 3]\n [1 2 5]]\n\nHorizontal Stack:\n [[1 2 3 2 2 3]\n [3 4 5 1 2 5]]",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#vectorization-in-numpy",
    "href": "NumPy.html#vectorization-in-numpy",
    "title": "6  NumPy",
    "section": "6.10 Vectorization in NumPy",
    "text": "6.10 Vectorization in NumPy\n\n6.10.1 Introduction to Vectorization\nVectorization is the process of applying operations to entire arrays or matrices simultaneously rather than iterating over individual elements using loops. This approach allows NumPy to utilize highly optimized C libraries for faster computation.\n\nBenefits of Vectorization:\n\nPerformance: Significantly faster than using for-loops in Python.\nConciseness: Shorter and more readable code.\nEfficiency: Reduces the overhead of Python loops and leverages lower-level optimizations.\n\n\n\n6.10.2 Understanding Vectorized Operations with Examples\n\n\nCode\n# Create two arrays\narr1 = np.array([1, 2, 3, 4, 5])\narr2 = np.array([10, 20, 30, 40, 50])\n\n# Element-wise addition (vectorized operation)\nsum_arr = arr1 + arr2\nprint(\"Sum Array:\", sum_arr)\n\n\nSum Array: [11 22 33 44 55]\n\n\n\n\n6.10.3 NumPy Vectorization Vs Python for Loop\nnp.dot is a vectorized operation in NumPy that performs matrix multiplication or dot product between arrays. It efficiently computes the element-wise multiplications and then sums them up. To better understand its efficiency, let’s first implement the dot product using for loops, and then compare its performance with np.dot to see the benefits of vectorization.\n\n\nCode\nimport time\n\n# Function to calculate dot product using for loops\ndef dot_product_loops(arr1, arr2):\n    result = 0\n    for i in range(len(arr1)):\n        result += arr1[i] * arr2[i]\n    return result\n\n# Create sample arrays\narr1 = np.random.rand(1000000)\narr2 = np.random.rand(1000000)\n\n# Measure time for the loop-based implementation\nstart_time = time.time()\nloop_result = dot_product_loops(arr1, arr2)\nloop_time = time.time() - start_time\n\n# Measure time for np.dot\nstart_time = time.time()\nnumpy_result = np.dot(arr1, arr2)\nnumpy_time = time.time() - start_time\n\n# Display results\nprint(f\"Loop-based implementation result: {loop_result:.5f}, Time: {loop_time:.5f} seconds\")\nprint(f\"NumPy np.dot result: {numpy_result:.5f}, Time: {numpy_time:.5f} seconds\")\n\n\nLoop-based implementation result: 250068.46992, Time: 0.19136 seconds\nNumPy np.dot result: 250068.46992, Time: 0.00199 seconds\n\n\nThe np.dot function significantly outperforms the manual loop-based implementation because it leverages NumPy’s vectorized operations, which is written in highly efficient C code. This example highlights why vectorized operations like np.dot are preferred for large-scale numerical computations in NumPy.\n\n\n6.10.4 Broadcasting and Its Role in Vectorization\nBroadcasting allows NumPy to perform operations between arrays of different shapes by automatically expanding their dimensions. This is essential for vectorized operations involving arrays with varying shapes.\nLet’s look at an example to see how it works\n\n\nCode\narr2 = np.array([[1, 2, 3, 4], \n                 [5, 6, 7, 8], \n                 [9, 1, 2, 3]])\n\n\n\n\nCode\narr4 = np.array([4, 5, 6, 7])\n\n\n\n\nCode\narr2 + arr4\n\n\narray([[ 5,  7,  9, 11],\n       [ 9, 11, 13, 15],\n       [13,  6,  8, 10]])\n\n\nWhen the expression arr2 + arr4 is evaluated, arr4 (which has the shape (4,)) is replicated three times to match the shape (3, 4) of arr2. Numpy performs the replication without actually creating three copies of the smaller dimension array, thus improving performance and using lower memory.\n\nBroadcasting only works if one of the arrays can be replicated to match the other array’s shape.\n\nDimensions of size 1 will broadcast (as if the value was repeated).\nOtherwise, the dimension must have the same shape.\n\n\n\nCode\narr5 = np.array([7, 8])\n\n\n\n\nCode\narr2 + arr5\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[62], line 1\n----&gt; 1 arr2 + arr5\n\nValueError: operands could not be broadcast together with shapes (3,4) (2,) \n\n\n\nIn the above example, even if arr5 is replicated three times, it will not match the shape of arr2. Hence arr2 + arr5 cannot be evaluated successfully. See the broadcasting documentation to learn more about it.\n\n\n6.10.5 Matrix Multiplication in NumPy with Vectorization\nMatrix multiplication is one of the most common and computationally intensive operations in numerical computing and deep learning. NumPy offers efficient and highly optimized methods for performing matrix multiplication, which leverage vectorization to handle large matrices quickly and accurately.\nNote that: NumPy matrix operations follow the standard rules of linear algebra, so it’s important to ensure that the shapes of the matrices are compatible. If they are not, consider reshaping the matrices before performing multiplication\nThere are two commonly methods for matrix multiplication\n\n6.10.5.1 Method 1: Matrix Multiplication Using np.dot()\n\n\nCode\n# Define two 2D arrays (matrices)\nmatrix1 = np.array([[1, 2, 3], \n                    [4, 5, 6]])\nmatrix2 = np.array([[7, 8], \n                    [9, 10], \n                    [11, 12]])\n\n# Matrix multiplication using np.dot\nresult_dot = np.dot(matrix1, matrix2)\nprint(\"Matrix Multiplication using np.dot:\\n\", result_dot)\n\n# Another way to perform np.dot for matrix multiplication\nresult_dot2 = matrix1.dot(matrix2)\nprint(\"\\nMatrix Multiplication using dot method:\\n\", result_dot2)\n\n\nMatrix Multiplication using np.dot:\n [[ 58  64]\n [139 154]]\n\nMatrix Multiplication using dot method:\n [[ 58  64]\n [139 154]]\n\n\n\n\n6.10.5.2 Method 2: Matrix Multiplication using np.matmul() pr @\n\n\nCode\n# Matrix multiplication using np.matmul or @ operator\nresult_matmul = np.matmul(matrix1, matrix2)\nresult_operator = matrix1 @ matrix2\nprint(\"\\nMatrix Multiplication using np.matmul:\\n\", result_matmul)\nprint(\"\\nMatrix Multiplication using @ operator:\\n\", result_operator)\n\n\n\nMatrix Multiplication using np.matmul:\n [[ 58  64]\n [139 154]]\n\nMatrix Multiplication using @ operator:\n [[ 58  64]\n [139 154]]\n\n\nNote that * operator in numpy is element-wise multiplication\n\n\nCode\n# using * operator for element-wise multiplication, please uncomment the code below to run\n# element_wise = matrix1 * matrix2\n# print(\"\\nElement-wise Multiplication:\\n\", element_wise)\n\n\n\n\nCode\n# reshape the array for element-wise multiplication\nmatrix2_reshaped = matrix2.reshape(2, 3)\nelement_wise = matrix1 * matrix2_reshaped\nprint(\"\\nElement-wise Multiplication after reshaping:\\n\", element_wise)\n\n\n\nElement-wise Multiplication after reshaping:\n [[ 7 16 27]\n [40 55 72]]",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#converting-between-numpy-arrays-and-pandas-dataframes",
    "href": "NumPy.html#converting-between-numpy-arrays-and-pandas-dataframes",
    "title": "6  NumPy",
    "section": "6.11 Converting Between NumPy Arrays and pandas DataFrames",
    "text": "6.11 Converting Between NumPy Arrays and pandas DataFrames\n\n6.11.1 Why Conversion Matters\nWhile both pandas and NumPy are integral to data science, they serve different purposes:\n\npandas:\n\nFoucses on data manipulation and analysis\nProvided intuitive tools to work with tabular data(e.g., labeled rows and columns).\n\nNumPy:\n\nOptimized for numerical computations.\nOffers mathematical functions like matrix multiplication, matrix decomposition, and eigenvalue computations, which pandas does not directly support.\n\n\n\n\n6.11.2 Converting a NumPy Array to a pandas DataFrame\nYou have numerical data stored in a NumPy array and want to apply pandas functionalities like labeling rows/columns or analyzing tabular data.\n\n\nCode\n# Create a NumPy array\ndata = np.array([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\n\n# Convert to a DataFrame\ndf = pd.DataFrame(data, columns=['A', 'B', 'C'])\n\n# Display the DataFrame\nprint(df)\n\n\n    A   B   C\n0  10  20  30\n1  40  50  60\n2  70  80  90\n\n\nKey Points\n\nUse the pd.DataFrame() constructor to create a DataFrame from a NumPy array.\nYou can specify:\n\nColumn names using the columns parameter.\nRow labels using the index parameter.\n\nIf columns or index are not specified, pandas assigns default labels:\n\nColumns: 0, 1, 2, ...\nRows: 0, 1, 2, ...\n\n\n\n\n6.11.3 Converting a pandas DataFrame to a NumPy Array\nYou have a pandas DataFrame and want to perform NumPy operations for optimized numerical processing.\n\n\nCode\n# Convert DataFrame to NumPy array\narray = df.to_numpy()\n\n# Display the NumPy array\nprint(array)\n\n\n[[10 20 30]\n [40 50 60]\n [70 80 90]]\n\n\nKey Points * Use .to_numpy() for conversion. * .values is an older method but still works (not recommended for newer code).\n\n\n6.11.4 Preserving Index and Column Information\nWhen converting a DataFrame to an array and back, ensure row/column labels are retained.\n\n\nCode\n# Convert DataFrame to NumPy array\narray_with_labels = df.to_numpy()\n\n# Convert back to a DataFrame\ndf_restored = pd.DataFrame(array_with_labels, columns=df.columns, index=df.index)\n\n# Display the restored DataFrame\nprint(df_restored)\n\n\n    A   B   C\n0  10  20  30\n1  40  50  60\n2  70  80  90\n\n\nNote that you need to manually pass columns and index back when re-creating the DataFrame.\n\n\n6.11.5 Practice User Cases\n\n6.11.5.1 Case 1: Adding labels to a NumPy Array\n\n\nCode\n# NumPy array without labels\ndata = np.random.rand(4, 3)\n\n# Add labels by converting to a DataFrame\ndf_with_labels = pd.DataFrame(data, columns=['Feature1', 'Feature2', 'Feature3'])\n\nprint(df_with_labels)\n\n\n   Feature1  Feature2  Feature3\n0  0.867644  0.478449  0.684584\n1  0.378309  0.860889  0.096055\n2  0.168311  0.981326  0.685720\n3  0.459235  0.340368  0.396904\n\n\n\n\n6.11.5.2 Case 2: Perform Numerical Computations\n\n\nCode\n# DataFrame for tabular manipulation\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n\n# Convert to NumPy for computations\narray = df.to_numpy()\n\n# Perform a NumPy operation\narray_squared = np.square(array)\n\n# Convert back to a DataFrame\ndf_squared = pd.DataFrame(array_squared, columns=df.columns)\n\nprint(df_squared)\n\n\n   A   B\n0  1  16\n1  4  25\n2  9  36\n\n\n\n\n\n6.11.6 Best Practices\n\nCheck Data Types: Ensure the data in your DataFrame is numerical before converting to NumPy for computations.\n\n\n\nCode\nprint(df.dtypes)\n\n\nA    int64\nB    int64\ndtype: object\n\n\n\nHandle Missing Values: Fill or drop missing values (NaN) in DataFrames before conversion to avoid errors in NumPy.\n\n\n\nCode\ndf = df.fillna(0)\n\n\n\nOptimize for Performance: Use NumPy for heavy numerical computations and pandas for data manipulation.\nConvert Back to pandas for Analysis: After computations, convert the NumPy array back to pandas if you need labeled data for further analysis or visualization.\n\n\n\n6.11.7 Comparison Table\n\n\n\n\n\n\n\n\nOperation\npandas DataFrame\nNumPy Array\n\n\n\n\nRow/Column Labels\nYes\nNo\n\n\nIndexing\nLabel-based (loc, iloc)\nInteger-based only\n\n\nMathematical Ops\nSlower (handles metadata)\nFaster\n\n\nMissing Data\nSupports (NaN)\nDoes not support (NaN)",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#random-number-generation-in-numpy",
    "href": "NumPy.html#random-number-generation-in-numpy",
    "title": "6  NumPy",
    "section": "6.12 Random Number Generation in NumPy",
    "text": "6.12 Random Number Generation in NumPy\nNumPy provides a variety of functions for generating random numbers through its numpy.random module. These functions can generate random numbers for different distributions and array shapes, making them essential for simulations, statistical sampling, and machine learning applications.\n\n6.12.1 Key Functions for Random Number Generation in NumPy\n\nnp.random.rand(): Generates random numbers from a uniform distribution between 0 and 1.\n\n\n\nCode\n# Generate a 2x3 array of random values between 0 and 1\nrand_array = np.random.rand(2, 3)\nprint(rand_array)\n\n\n[[0.59160333 0.1864713  0.70446818]\n [0.46476414 0.75172527 0.2225734 ]]\n\n\n\nnp.random.randn(): Generates random numbers from a standard normal distribution (mean = 0, standard deviation = 1). It is useful for simulating Gaussian distributed data\n\n\n\nCode\nnormal_array = np.random.randn(3, 3)\nprint(normal_array)\n\n\n[[ 0.96287539 -0.57932461 -0.25728454]\n [ 0.33379165  0.01331971 -0.19328112]\n [-0.79966965 -1.20325527  1.50081961]]\n\n\nNumPy’s random module can be used to generate arrays of random numbers from several different probability distributions. For example, a 3x5 array of uniformly distributed random numbers can be generated using the uniform function of the random module.\n\nnp.random.randint(): Generate random integers within a specific range, you can specify low and high values and the shape of the output array\n\n\n\nCode\n# Generate a 4x4 matrix of random integers between 10 and 20\nint_array = np.random.randint(10, 20, (4, 4))\nprint(int_array)\n\n\n[[13 17 15 18]\n [10 11 13 15]\n [13 12 12 13]\n [17 14 11 16]]\n\n\n\nnp.random.choice(): Random selects elements from an input array\n\n\n\nCode\n# Select 5 random elements from the array [1, 2, 3, 4, 5] with replacement\nchoice_array = np.random.choice([1, 2, 3, 4, 5], size=5, replace=True)\nprint(choice_array)\n\n\n[4 3 5 2 2]\n\n\n\nnp.random.uniform(): Generates random floating-point numbers between a specified range (low, high) that follow uniform distribution\n\n\n\nCode\n# Generate 6 random numbers from a normal distribution with mean=10 and std=2\ncustom_normal = np.random.normal(10, 2, size=6)\nprint(custom_normal)\n\n\n[ 8.96337061  9.80411528  7.1623284   9.58883996 11.82330829 10.53508984]\n\n\n\n\nCode\n# Generate a 1D array of 5 random numbers between -5 and 5\nuniform_array = np.random.uniform(-5, 5, size=5)\nprint(uniform_array)\n\n\n[-4.76424391 -2.90776829  4.56871733  2.3230159  -4.72083461]\n\n\n\nnp.random.normal(): Generates random numbers from a normal distribution with a specified mean and standard deviation.\n\n\n\nCode\nnp.random.uniform(size = (3,5))\n\n\narray([[0.66198756, 0.76365694, 0.79924355, 0.80723229, 0.0661366 ],\n       [0.5777333 , 0.49288606, 0.02914087, 0.61883579, 0.04236321],\n       [0.2049758 , 0.9819383 , 0.65983787, 0.87025466, 0.39354816]])\n\n\nFor a full list, please check the offcial website\nRandom numbers can also be generated by Python’s built-in random module. However, it generates one random number at a time, which makes it much slower than NumPy’s random module.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "NumPy.html#independent-practice",
    "href": "NumPy.html#independent-practice",
    "title": "6  NumPy",
    "section": "6.13 Independent Practice:",
    "text": "6.13 Independent Practice:\n\n6.13.1 Practice exercise 1\n\n6.13.1.1 \nRead the coordinates of the capital cities of the world from https://gist.github.com/ofou/df09a6834a8421b4f376c875194915c9 .\nTask 1: Use NumPy to print the name and coordinates of the capital city closest to the US capital - Washington DC.\nNote that:\n\nThe Country Name for US is given as United States of America in the data.\nThe ‘closeness’ of capital cities from the US capital is based on the Euclidean distance of their coordinates to those of the US capital.\n\nHints:\n\nUse the to_numpy() function of the Pandas DataFrame class to convert a DataFrame to a Numpy array\nUse broadcasting to compute the euclidean distance of capital cities from Washington DC.\nUse np.argmin to locate the index of the minimum value in an array\nExclude the capital itself to avoid trivial zero values in the results.\n\nSolution:\n\n\nCode\n\ncapital_cities = pd.read_csv('./datasets/country-capital-lat-long-population.csv')\ncoordinates_capital_cities = capital_cities[['Latitude', 'Longitude']].to_numpy()\nus_coordinates = capital_cities.loc[capital_cities['Country']=='United States of America',['Latitude','Longitude']].to_numpy()\n\n#Broadcasting\ndistance_from_DC = np.sqrt(np.sum((us_coordinates-coordinates_capital_cities)**2,axis=1))\n\n#Assigning a high value of distance to DC, otherwise it will itself be selected as being closest to DC\ndistance_from_DC[distance_from_DC==0]=9999\nclosest_capital_index = np.argmin(distance_from_DC)\nprint(\"Closest capital city is:\" ,capital_cities.loc[closest_capital_index,'Capital City'])\nprint(\"Coordinates of the closest capital city are:\",coordinates_capital_cities[closest_capital_index,:])\n\n\nClosest capital city is: Ottawa-Gatineau\nCoordinates of the closest capital city are: [ 45.4166 -75.698 ]\n\n\nTask 2: Use NumPy to:\n\nPrint the names of the countries of the top 10 capital cities closest to the US capital - Washington DC.\nCreate and print a NumPy array containing the coordinates of the top 10 cities.\n\nHint: Use the concatenate() function from the NumPy library to stack the coordinates of the top 10 cities.\n\n\nCode\ntop10_cities_coordinates = coordinates_capital_cities[closest_capital_index,:].reshape(1,2)\nprint(\"Top 10 countries closest to Washington DC are:\\n Canada\")\nfor i in range(9):\n    distance_from_DC[closest_capital_index]=9999\n    closest_capital_index = np.argmin(distance_from_DC)\n    print(capital_cities.loc[closest_capital_index,'Country'])\n    top10_cities_coordinates=np.concatenate((top10_cities_coordinates,coordinates_capital_cities[closest_capital_index,:].reshape(1,2)))\nprint(\"Coordinates of the top 10 cities closest to US are: \\n\",top10_cities_coordinates)\n\n\nTop 10 countries closest to Washington DC are:\n Canada\nCuba\nTurks and Caicos Islands\nCayman Islands\nHaiti\nJamaica\nDominican Republic\nSaint Pierre and Miquelon\nPuerto Rico\nUnited States Virgin Islands\nCoordinates of the top 10 cities closest to US are: \n [[ 32.2915 -64.778 ]\n [ 23.1195 -82.3785]\n [ 21.4612 -71.1419]\n [ 19.2866 -81.3744]\n [ 18.5392 -72.335 ]\n [ 17.997  -76.7936]\n [ 18.4896 -69.9018]\n [ 46.7738 -56.1815]\n [ 18.4663 -66.1057]\n [ 18.3419 -64.9307]]\n\n\n\n\n\n6.13.2 Practice exercise 2:\nThis exercise will show vectorized computations with NumPy. Vectorized computations help perform computations more efficiently, and also make the code concise.\nQ: Read the (1) quantities of roll, bun, cake and bread required by 3 people - Ben, Barbara & Beth, from food_quantity.csv, (2) price of these food items in two shops - Target and Kroger, from price.csv. Find out which shop should each person go to minimize their expenses.\n\n\nCode\n#Reading the datasets on food quantity and price\nimport pandas as pd\nfood_qty = pd.read_csv('../data/food_quantity.csv',index_col=0)\nprice = pd.read_csv('../data/price.csv',index_col=0)\n\n\n\n\nCode\nfood_qty\n\n\n\n\n\n\n\n\n\nroll\nbun\ncake\nbread\n\n\nPerson\n\n\n\n\n\n\n\n\nBen\n6\n5\n3\n1\n\n\nBarbara\n3\n6\n2\n2\n\n\nBeth\n3\n4\n3\n1\n\n\n\n\n\n\n\n\n\nCode\nprice\n\n\n\n\n\n\n\n\n\nTarget\nKroger\n\n\nItem\n\n\n\n\n\n\nroll\n1.5\n1.0\n\n\nbun\n2.0\n2.5\n\n\ncake\n5.0\n4.5\n\n\nbread\n16.0\n17.0\n\n\n\n\n\n\n\nFirst, let’s start from a simple problem. We’ll compute the expenses of Ben if he prefers to buy all food items from Target\n\n\nCode\n%%time\n# write a for loop to calculate the total cost of Ben's food if he shops at the Target store\ntotal_cost = 0 \nfor food in food_qty.columns:\n\n    total_cost += food_qty.loc['Ben',food]*price.loc[food,'Target']\ntotal_cost\n\n\nCPU times: total: 0 ns\nWall time: 0 ns\n\n\nnp.float64(50.0)\n\n\n\n\nCode\n%%time\n# using numpy \ntotal_cost = np.sum(food_qty.loc['Ben',]*price.loc[:,'Target'])\ntotal_cost\n\n\nCPU times: total: 0 ns\nWall time: 1 ms\n\n\nnp.float64(50.0)\n\n\nBen will spend $50 if he goes to Target\nNow, let’s add another layer of complication. We’ll compute Ben’s expenses for both stores - Target and Kroger\n\n\nCode\n%%time\n# using loops to calculate the total cost of food for Ben for all stores\ntotal_cost = {}\nfor store in price.columns:\n    total_cost[store] = 0\n    for food in food_qty.columns:\n        total_cost[store] += food_qty.loc['Ben',food]*price.loc[food,store]\ntotal_cost\n\n\nCPU times: total: 0 ns\nWall time: 1 ms\n\n\n{'Target': np.float64(50.0), 'Kroger': np.float64(49.0)}\n\n\n\n\nCode\n%%time\n# using numpy\ntotal_cost = np.dot(food_qty.loc['Ben',],price.loc[:,:])\ntotal_cost\n\n\nCPU times: total: 0 ns\nWall time: 0 ns\n\n\narray([50., 49.])\n\n\nBen will spend $50 if he goes to Target, and $49 if he goes to Kroger. Thus, he should choose Kroger.\nNow, let’s add the final layer of complication, and solve the problem. We’ll compute everyone’s expenses for both stores - Target and Kroger\n\n\nCode\n%%timeit\nstore_expense = pd.DataFrame(0.0, columns=price.columns, index = food_qty.index)\nfor person in store_expense.index:\n    for store in store_expense.columns:\n        for food in food_qty.columns:\n            store_expense.loc[person, store] += food_qty.loc[person, food]*price.loc[food, store]\n\nstore_expense\n\n\n1.51 ms ± 15.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\nCode\n%%timeit\n# using matrix multiplication in numpy\npd.DataFrame(np.dot(food_qty.values, price.values), columns=price.columns, index=food_qty.index)\n\n\n11 μs ± 102 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nBased on the above table, Ben should go to Kroger, Barbara to Target and Beth can go to either store.\nAs the complexity of operations increases, the number of nested for-loops tends to grow, making the code more cumbersome and difficult to manage. In contrast, leveraging NumPy arrays allows for concise and straightforward implementation, regardless of the complexity. Vectorized computations are not only cleaner but also significantly faster, offering a more efficient solution. To take advantage of this, you first need to convert a pandas DataFrame to a NumPy array for matrix multiplication. Once the computation is complete, you can convert the results back to a pandas DataFrame for further analysis or display\n\n\n6.13.3 Practice exercise 3\nUse matrix multiplication to find the average IMDB rating and average Rotten tomatoes rating for each genre - comedy, action, drama and horror. Use the data: movies_cleaned.csv. Which is the most preferred genre for IMDB users, and which is the least preferred genre for Rotten Tomatoes users?\nHint: 1. Create two matrices - one containing the IMDB and Rotten Tomatoes ratings, and the other containing the genre flags (comedy/action/drama/horror).\n\nMultiply the two matrices created in 1.\nDivide each row/column of the resulting matrix by a vector having the number of ratings in each genre to get the average rating for the genre.\n\nSolution:\n\n\nCode\ndata = pd.read_csv('../Data/movies_cleaned.csv')\ndata.head()\n\n\n\n\n\n\n\n\n\nTitle\nIMDB Rating\nRotten Tomatoes Rating\nRunning Time min\nRelease Date\nUS Gross\nWorldwide Gross\nProduction Budget\ncomedy\nAction\ndrama\nhorror\n\n\n\n\n0\nBroken Arrow\n5.8\n55\n108\nFeb 09 1996\n70645997\n148345997\n65000000\n0\n1\n0\n0\n\n\n1\nBrazil\n8.0\n98\n136\nDec 18 1985\n9929135\n9929135\n15000000\n1\n0\n0\n0\n\n\n2\nThe Cable Guy\n5.8\n52\n95\nJun 14 1996\n60240295\n102825796\n47000000\n1\n0\n0\n0\n\n\n3\nChain Reaction\n5.2\n13\n106\nAug 02 1996\n21226204\n60209334\n55000000\n0\n1\n0\n0\n\n\n4\nClash of the Titans\n5.9\n65\n108\nJun 12 1981\n30000000\n30000000\n15000000\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n\nCode\n# Getting ratings of all movies\ndrating = data[['IMDB Rating','Rotten Tomatoes Rating']]\ndrating_num = drating.to_numpy() #Converting the data to NumPy array\ndrating_num\n\n\narray([[ 5.8, 55. ],\n       [ 8. , 98. ],\n       [ 5.8, 52. ],\n       ...,\n       [ 7. , 65. ],\n       [ 5.7, 26. ],\n       [ 6.7, 82. ]])\n\n\n\n\nCode\n# Getting the matrix indicating the genre of all movies\ndgenre = data.iloc[:,8:12]\ndgenre_num = dgenre.to_numpy() #Converting the data to NumPy array\ndgenre_num\n\n\narray([[0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       ...,\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 1, 0, 0]])\n\n\nWe’ll first find the total IMDB and Rotten tomatoes ratings for all movies of each genre, and then divide them by the number of movies of the corresponding genre to find the average rating for the genre.\nFor finding the total IMDB and Rotten tomatoes ratings, we’ll multiply drating_num with dgenre_num. However, before multiplying, we’ll check if their shapes are compatible for matrix multiplication.\n\n\nCode\n#Shape of drating_num\ndrating_num.shape\n\n\n(980, 2)\n\n\n\n\nCode\n#Shape of dgenre_num\ndgenre_num.shape\n\n\n(980, 4)\n\n\nNote that the above shapes are not compatible for matrix multiplication. We’ll transpose dgenre_num to make the shapes compatible.\n\n\nCode\n#Total IMDB and Rotten tomatoes ratings for each genre\nratings_sum_genre = drating_num.T.dot(dgenre_num)\nratings_sum_genre\n\n\narray([[ 1785.6,  1673.1,  1630.3,   946.2],\n       [14119. , 13725. , 14535. ,  6533. ]])\n\n\n\n\nCode\n#Number of movies in the data will be stored in 'rows', and number of columns stored in 'cols'\nrows, cols = data.shape\n\n\n\n\nCode\n#Getting number of movies in each genre\nmovies_count_genre = dgenre_num.T.dot(np.ones(rows))\nmovies_count_genre\n\n\narray([302., 264., 239., 154.])\n\n\n\n\nCode\n#Finding the average IMDB and average Rotten tomatoes ratings for each genre\nratings_sum_genre/movies_count_genre\n\n\narray([[ 5.91258278,  6.3375    ,  6.82133891,  6.14415584],\n       [46.75165563, 51.98863636, 60.81589958, 42.42207792]])\n\n\n\n\nCode\npd.DataFrame(ratings_sum_genre/movies_count_genre,columns = ['comedy','Action','drama','horror'],\n             index = ['IMDB Rating','Rotten Tomatoes Rating'])\n\n\n\n\n\n\n\n\n\ncomedy\nAction\ndrama\nhorror\n\n\n\n\nIMDB Rating\n5.912583\n6.337500\n6.821339\n6.144156\n\n\nRotten Tomatoes Rating\n46.751656\n51.988636\n60.815900\n42.422078\n\n\n\n\n\n\n\nIMDB users prefer drama, and are amused the least by comedy movies, on an average. However, Rotten tomatoes critics would rather watch comedy than horror movies, on an average.\n\n\n6.13.4 Practice exercise 4:\nRandom number generation using NumPy\nSuppose 500 people eat at Food cart 1, and another 500 eat at Food cart 2, everyday.\nThe waiting time at Food cart 2 has a normal distribution with mean 8 minutes and standard deviation 3 minutes, while the waiting time at Food cart 1 has a uniform distribution with minimum 5 minutes and maximum 25 minutes.\nSimulate a dataset containing waiting times for 500 ppl for 30 days in each of the food joints. Assume that the waiting times are measured simultaneously at a certain time in both places, i.e., the observations are paired.\nOn how many days is the average waiting time at Food cart 2 higher than that at Food cart 1?\nWhat percentage of times the waiting time at Food cart 2 was higher than the waiting time at Food cart 1?\nTry both approaches: (1) Using loops to generate data, (2) numpy array to generate data. Compare the time taken in both approaches.\n\n\nCode\nimport time as tm\n\n\n\n\nCode\n#Method 1: Using loops\nstart_time = tm.time() #Current system time\n\n#Initializing waiting times for 500 ppl over 30 days\nwaiting_times_FoodCart1 = pd.DataFrame(0,index=range(500),columns=range(30)) #FoodCart1\nwaiting_times_FoodCart2 = pd.DataFrame(0,index=range(500),columns=range(30)) #FoodCart2\nimport random as rm\nfor i in range(500):  #Iterating over 500 ppl\n    for j in range(30): #Iterating over 30 days\n        waiting_times_FoodCart2.iloc[i,j] = rm.gauss(8,3) #Simulating waiting time in FoodCart2 for the ith person on jth day\n        waiting_times_FoodCart1.iloc[i,j] = rm.uniform(5,25) #Simulating waiting time in FoodCart1 for the ith person on jth day\ntime_diff = waiting_times_FoodCart2-waiting_times_FoodCart1\n\nprint(\"On \",sum(time_diff.mean()&gt;0),\" days, the average waiting time at FoodCart2 higher than that at FoodCart1\")\nprint(\"Percentage of times waiting time at FoodCart2 was greater than that at FoodCart1 = \",100*(time_diff&gt;0).sum().sum()/(30*500),\"%\")\nend_time = tm.time() #Current system time\nprint(\"Time taken = \", end_time-start_time)\n\n\nOn  0  days, the average waiting time at FoodCart2 higher than that at FoodCart1\nPercentage of times waiting time at FoodCart2 was greater than that at FoodCart1 =  16.226666666666667 %\nTime taken =  4.521248817443848\n\n\n\n\nCode\n#Method 2: Using NumPy arrays\nstart_time = tm.time()\nwaiting_time_FoodCart2 = np.random.normal(8,3,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in FoodCart2\nwaiting_time_FoodCart1 = np.random.uniform(5,25,size = (500,30)) #Simultaneously generating the waiting times of 500 ppl over 30 days in FoodCart1\ntime_diff = waiting_time_FoodCart2-waiting_time_FoodCart1\nprint(\"On \",(time_diff.mean()&gt;0).sum(),\" days, the average waiting time at FoodCart2 higher than that at FoodCart1\")\nprint(\"Percentage of times waiting time at FoodCart2 was greater than that at FoodCart1 = \",100*(time_diff&gt;0).sum()/15000,\"%\")\nend_time = tm.time()\nprint(\"Time taken = \", end_time-start_time)\n\n\nOn  0  days, the average waiting time at FoodCart2 higher than that at FoodCart1\nPercentage of times waiting time at FoodCart2 was greater than that at FoodCart1 =  16.52 %\nTime taken =  0.008000850677490234\n\n\nThe approach with NumPy is much faster than the one with loops.\n\n\n6.13.5 Practice exercise 5\nBootstrapping: Find the 95% confidence interval of mean profit for ‘Action’ movies, using Bootstrapping.\nBootstrapping is a non-parametric method for obtaining confidence interval. Use the algorithm below to find the confidence interval:\n\nFind the profit for each of the ‘Action’ movies. Suppose there are N such movies. We will have a Profit column with N values.\n\nRandomly sample N values with replacement from the Profit column\n\nFind the mean of the N values obtained in (b)\n\nRepeat steps (b) and (c) M=1000 times\n\nThe 95% Confidence interval is the range between the 2.5% and 97.5% percentile values of the 1000 means obtained in (c)\nUse the movies_cleaned.csv dataset.\n\nSolution:\n\n\nCode\n#Reading data\nmovies = pd.read_csv('./Datasets/movies_cleaned.csv')\n\n#Filtering action movies\nmovies_action = movies.loc[movies['Action']==1,:]\n\n#Computing profit of movies\nmovies_action.loc[:,'Profit'] = movies_action.loc[:,'Worldwide Gross'] - movies_action.loc[:,'Production Budget']\n\n#Subsetting the profit column\nprofit_vec = movies_action['Profit']\n\n#Creating a matrix of 1000 samples with replacement from the profit column\nbootstrap_samples=np.random.choice(profit_vec,size = (1000,len(profit_vec)))\n\n#Computing the mean of each of the 1000 samples\nbootstrap_sample_means = bootstrap_samples.mean(axis=1)\n\n#The confidence interval is the 2.5th and 97.5th percentile of the mean of the 1000 samples\nprint(\"Confidence interval = [$\"+str(np.round(np.percentile(bootstrap_sample_means,2.5)/1e6,2))+\" million, $\"+str(np.round(np.percentile(bootstrap_sample_means,97.5)/1e6,2))+\" million]\")\n\n\nConfidence interval = [$132.53 million, $182.69 million]",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "Pandas.html",
    "href": "Pandas.html",
    "title": "7  Pandas",
    "section": "",
    "text": "7.1 Introduction\nPandas is an essential tool in the data scientist or data analyst’s toolkit due to its ability to handle and transform data with ease and efficiency.\nBuilt on top of the NumPy package, Pandas extends the capabilities of NumPy by offering support for working with tabular or heterogeneous data. While NumPy is optimized for working with homogeneous numerical arrays, Pandas is specifically designed for handling structured, labeled data, commonly represented in two-dimensional tables known as DataFrames. There are some similarities between the two libraries. Like NumPy, Pandas provides basic mathematical functionalities such as addition, subtraction, conditional operations, and broadcasting. However, while NumPy focuses on multi-dimensional arrays, Pandas excels in offering the powerful 2D DataFrame object for data manipulation.\nData in Pandas is often used to feed statistical analyses in SciPy, visualization functions in Matplotlib, and machine learning algorithms in Scikit-learn.\nTypically, the Pandas library is used for:\nLet’s import the Pandas library to use its methods and functions.\nimport pandas as pd",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#introduction",
    "href": "Pandas.html#introduction",
    "title": "7  Pandas",
    "section": "",
    "text": "Data reading/writing from different sources (CSV, Excel, SQL, etc.).\nData manipulation, cleaning, and transformation.\nComputing data distribution and summary statistics\nGrouping, aggregation, and pivoting for advanced data analysis.\nMerging, concatenating, and reshaping data.\nHandling time series and datetime data.\nData visualization and plotting.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#pandas-data-structures---series-and-dataframe",
    "href": "Pandas.html#pandas-data-structures---series-and-dataframe",
    "title": "7  Pandas",
    "section": "7.2 Pandas data structures - Series and DataFrame",
    "text": "7.2 Pandas data structures - Series and DataFrame\nThere are two core components of the Pandas library - Series and DataFrame.\nA DataFrame is a two-dimensional object - comprising of tabular data organized in rows and columns, where individual columns can be of different value types (numeric / string / boolean etc.). A DataFrame has row labels (also called row indices) which refer to individual rows, and column labels (also called column names) that refer to individual columns. By default, the row indices are integers starting from zero. However, both the row indices and column names can be customized by the user.\nLet us read the spotify data - spotify_data.csv, using the Pandas function read_csv().\n\nspotify_data = pd.read_csv('../Data/spotify_data.csv')\nspotify_data.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n2\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n7\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n\n\n3\n16996777\nrap\nJuice WRLD\n96\nRobbery\n0\n240527\n1\n2021\n0.708\n...\n2\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\n\n\n4\n5988689\nrap\nRoddy Ricch\n88\nBig Stepper\n0\n175170\n0\n2021\n0.753\n...\n8\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\n\n\n\n\n5 rows × 21 columns\n\n\n\nThe object spotify_data is a pandas DataFrame:\n\ntype(spotify_data)\n\npandas.core.frame.DataFrame\n\n\nA Series is a one-dimensional array-like object in pandas that contains a sequence of values, where each value is associated with an index. All the values in a Series must have the same data type. Each column of a DataFrame is Series as shown in the example below.\n\n#Extracting song titles from the spotify_songs DataFrame\nspotify_songs = spotify_data['track_name']\nspotify_songs\n\n0                           All Girls Are The Same\n1                                     Lucid Dreams\n2                                  Hear Me Calling\n3                                          Robbery\n4                                      Big Stepper\n                            ...                   \n243185                                    Stardust\n243186             Knockin' A Jug - 78 rpm Version\n243187            When It's Sleepy Time Down South\n243188    On The Sunny Side Of The Street - Part 2\n243189                                    My Sweet\nName: track_name, Length: 243190, dtype: object\n\n\n\n#The object spotify_songs is a Series\ntype(spotify_songs)\n\npandas.core.series.Series\n\n\nA Series is essentially a column, and a DataFrame is a two-dimensional table made up of a collection of Series",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#creating-a-pandas-series-dataframe",
    "href": "Pandas.html#creating-a-pandas-series-dataframe",
    "title": "7  Pandas",
    "section": "7.3 Creating a Pandas Series / DataFrame",
    "text": "7.3 Creating a Pandas Series / DataFrame\n\nFrom a Python List or Dictionary\nBy Reading Data from a file\n\n\n7.3.1 Creating a Series/DataFrame from Python List or Dictionary\n\n7.3.1.1 Creating a series:\n\nFrom a Python List: You can create a pandas Series by passing a list of values.\n\n\n#Defining a Pandas Series\nseries_example = pd.Series(['these','are','english','words'])\nseries_example\n\n0      these\n1        are\n2    english\n3      words\ndtype: object\n\n\nNote that the default row indices are integers starting from 0. However, the index can be specified with the index argument if desired by the user:\n\n#Defining a Pandas Series with custom row labels\nseries_example = pd.Series(['these','are','english','words'], index = range(101,105))\nseries_example\n\n101      these\n102        are\n103    english\n104      words\ndtype: object\n\n\n\nFrom a Dictionary: A dictionary can also be used to create a Series, where keys become index labels.\n\n\n#Dictionary consisting of the GDP per capita of the US from 1960 to 2021 with some missing values\nGDP_per_capita_dict = {'1960':3007,'1961':3067,'1962':3244,'1963':3375,'1964':3574,'1965':3828,'1966':4146,'1967':4336,'1968':4696,'1970':5234,'1971':5609,'1972':6094,'1973':6726,'1974':7226,'1975':7801,'1976':8592,'1978':10565,'1979':11674, '1980':12575,'1981':13976,'1982':14434,'1983':15544,'1984':17121,'1985':18237,  '1986':19071,'1987':20039,'1988':21417,'1989':22857,'1990':23889,'1991':24342,  '1992':25419,'1993':26387,'1994':27695,'1995':28691,'1996':29968,'1997':31459,  '1998':32854,'2000':36330,'2001':37134,'2002':37998,'2003':39490,'2004':41725,  '2005':44123,'2006':46302,'2007':48050,'2008':48570,'2009':47195,'2010':48651,  '2011':50066,'2012':51784,'2013':53291,'2015':56763,'2016':57867,'2017':59915,'2018':62805, '2019':65095,'2020':63028,'2021':69288}\n\n\n#Example 2: Creating a Pandas Series from a Dictionary\nGDP_per_capita_series = pd.Series(GDP_per_capita_dict)\nGDP_per_capita_series.head()\n\n1960    3007\n1961    3067\n1962    3244\n1963    3375\n1964    3574\ndtype: int64\n\n\n\n\n7.3.1.2 Creating a DataFrame\n\nFrom a list of Python Dictionary: You can create a DataFrame where keys are column names and values are lists representing column data.\n\n\n#List of dictionary consisting of 52 playing cards of the deck\ndeck_list_of_dictionaries = [{'value':i, 'suit':c}\nfor c in ['spades', 'clubs', 'hearts', 'diamonds']\nfor i in range(2,15)]\n\n\n#Example 3: Creating a Pandas DataFrame from a List of dictionaries\ndeck_df = pd.DataFrame(deck_list_of_dictionaries)\ndeck_df.head()\n\n\n\n\n\n\n\n\nvalue\nsuit\n\n\n\n\n0\n2\nspades\n\n\n1\n3\nspades\n\n\n2\n4\nspades\n\n\n3\n5\nspades\n\n\n4\n6\nspades\n\n\n\n\n\n\n\n\nFrom a Python Dictionary: You can create a DataFrame where keys are column names and values are lists representing column data.\n\n\n#Example 4: Creating a Pandas DataFrame from a Dictionary\ndict_data = {'A': [1, 2, 3, 4, 5],\n        'B': [20, 10, 50, 40, 30],\n        'C': [100, 200, 300, 400, 500]}\n\ndict_df = pd.DataFrame(dict_data)\ndict_df\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n20\n100\n\n\n1\n2\n10\n200\n\n\n2\n3\n50\n300\n\n\n3\n4\n40\n400\n\n\n4\n5\n30\n500",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#creating-a-seriesdataframe-by-reading-data-from-a-file",
    "href": "Pandas.html#creating-a-seriesdataframe-by-reading-data-from-a-file",
    "title": "7  Pandas",
    "section": "7.4 Creating a Series/DataFrame by Reading Data from a File",
    "text": "7.4 Creating a Series/DataFrame by Reading Data from a File\nIn the real world, a Pandas DataFrame will typically be created by loading the datasets from existing storage such as SQL Database, CSV file, Excel file, text files, HTML files, etc., as we learned in the perious chapter of the book on Reading data.\n\nUsing read_csv(): This is one of the most common methods to read data from a CSV file into a pandas DataFrame.\nUsing read_excel(): You can also read data from Excel files.\nUsing read_json(): You can also read data from json files.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#attributes-and-methods-of-a-pandas-dataframe",
    "href": "Pandas.html#attributes-and-methods-of-a-pandas-dataframe",
    "title": "7  Pandas",
    "section": "7.5 Attributes and Methods of a Pandas DataFrame",
    "text": "7.5 Attributes and Methods of a Pandas DataFrame\nAll attributes and methods of a Pandas DataFrame object can be viewed with the python’s built-in dir() function.\n\n#List of attributes and methods of a Pandas DataFrame\n#This code is not executed as the list is too long\n# dir(spotify_data)\n\nAlthough we’ll see examples of attributes and methods of a Pandas DataFrame, please note that most of these attributes and methods are also applicable to the Pandas Series object.\n\n7.5.1 Attributes of a Pandas DataFrame\nSome of the attributes of the Pandas DataFrame class are the following.\n\n7.5.1.1 dtypes\nThis attribute is a Series consisting the datatypes of columns of a Pandas DataFrame.\n\nspotify_data.dtypes\n\nartist_followers       int64\ngenres                object\nartist_name           object\nartist_popularity      int64\ntrack_name            object\ntrack_popularity       int64\nduration_ms            int64\nexplicit               int64\nrelease_year           int64\ndanceability         float64\nenergy               float64\nkey                    int64\nloudness             float64\nmode                   int64\nspeechiness          float64\nacousticness         float64\ninstrumentalness     float64\nliveness             float64\nvalence              float64\ntempo                float64\ntime_signature         int64\ndtype: object\n\n\nThe table below describes the datatypes of columns in a Pandas DataFrame.\n\n\n\nPandas Type\nNative Python Type\nDescription\n\n\n\n\nobject\nstring\nThe most general dtype. This datatype is assigned to a column if the column has mixed types (numbers and strings)\n\n\nint64\nint\nThis datatype is for integers from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 or for integers having a maximum size of 64 bits\n\n\nfloat64\nfloat\nThis datatype is for real numbers. If a column contains integers and NaNs, Pandas will default to float64. This is because the missing values may be a real number\n\n\ndatetime64, timedelta[ns]\nN/A (but see the datetime module in Python’s standard library)\nValues meant to hold time data. This datatype is useful for time series analysis\n\n\n\n\n\n7.5.1.2 columns\nThis attribute consists of the column labels (or column names) of a Pandas DataFrame.\n\nspotify_data.columns\n\nIndex(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n       'track_name', 'track_popularity', 'duration_ms', 'explicit',\n       'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n       'valence', 'tempo', 'time_signature'],\n      dtype='object')\n\n\n\n\n7.5.1.3 index\nThis attribute consists of the row lables (or row indices) of a Pandas DataFrame.\n\nspotify_data.index\n\nRangeIndex(start=0, stop=243190, step=1)\n\n\n\n\n7.5.1.4 axes\nThis is a list of length two, where the first element is the row labels, and the second element is the columns labels. In other words, this attribute combines the information in the attributes - index and columns.\n\nspotify_data.axes\n\n[RangeIndex(start=0, stop=243190, step=1),\n Index(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n        'track_name', 'track_popularity', 'duration_ms', 'explicit',\n        'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n        'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n        'valence', 'tempo', 'time_signature'],\n       dtype='object')]\n\n\n\n\n7.5.1.5 ndim\nAs in NumPy, this attribute specifies the number of dimensions. However, unlike NumPy, a Pandas DataFrame has a fixed dimenstion of 2, and a Pandas Series has a fixed dimesion of 1.\n\nspotify_data.ndim\n\n2\n\n\n\n\n7.5.1.6 size\nThis attribute specifies the number of elements in a DataFrame. Its value is the product of the number of rows and columns.\n\nspotify_data.size\n\n5106990\n\n\n\n\n7.5.1.7 shape\nThis is a tuple consisting of the number of rows and columns in a Pandas DataFrame.\n\nspotify_data.shape\n\n(243190, 21)\n\n\n\n\n7.5.1.8 values\nThis provides a NumPy representation of a Pandas DataFrame.\n\nspotify_data.values\n\narray([[16996777, 'rap', 'Juice WRLD', ..., 0.203, 161.991, 4],\n       [16996777, 'rap', 'Juice WRLD', ..., 0.218, 83.903, 4],\n       [16996777, 'rap', 'Juice WRLD', ..., 0.499, 88.933, 4],\n       ...,\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.37, 105.093, 4],\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.576, 101.279, 4],\n       [2256652, 'jazz', 'Louis Armstrong', ..., 0.816, 105.84, 4]],\n      dtype=object)\n\n\n\n\n\n7.5.2 Methods of a Pandas DataFrame\nSome of the commonly used methods of the Pandas DataFrame class are the following.\n\n7.5.2.1 head()\nPrints the first n rows of a DataFrame.\n\nspotify_data.head(2)\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n1\n0.306\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0\n0.200\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n\n\n2 rows × 21 columns\n\n\n\n\n\n7.5.2.2 tail()\nPrints the last n rows of a DataFrame.\n\nspotify_data.tail(3)\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n243187\n2256652\njazz\nLouis Armstrong\n74\nWhen It's Sleepy Time Down South\n4\n200200\n0\n1923\n0.527\n...\n3\n-14.814\n1\n0.0793\n0.989\n0.00001\n0.1040\n0.370\n105.093\n4\n\n\n243188\n2256652\njazz\nLouis Armstrong\n74\nOn The Sunny Side Of The Street - Part 2\n4\n185973\n0\n1923\n0.559\n...\n0\n-9.804\n1\n0.0512\n0.989\n0.84700\n0.4480\n0.576\n101.279\n4\n\n\n243189\n2256652\njazz\nLouis Armstrong\n74\nMy Sweet\n4\n195960\n0\n1923\n0.741\n...\n3\n-10.406\n1\n0.0505\n0.927\n0.07880\n0.0633\n0.816\n105.840\n4\n\n\n\n\n3 rows × 21 columns\n\n\n\n\n\n7.5.2.3 describe()\nPrint summary statistics of a Pandas DataFrame, as seen in chapter 3 on Reading Data.\n\nspotify_data.describe()\n\n\n\n\n\n\n\n\nartist_followers\nartist_popularity\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\ncount\n2.431900e+05\n243190.000000\n243190.000000\n2.431900e+05\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n\n\nmean\n1.960931e+06\n65.342633\n36.080772\n2.263209e+05\n0.050039\n1992.475258\n0.568357\n0.580633\n5.240326\n-9.432548\n0.670928\n0.111984\n0.383938\n0.071169\n0.223756\n0.552302\n119.335060\n3.884177\n\n\nstd\n5.028746e+06\n10.289182\n16.476836\n9.973214e+04\n0.218026\n18.481463\n0.159444\n0.236631\n3.532546\n4.449731\n0.469877\n0.198068\n0.321142\n0.209555\n0.198076\n0.250017\n29.864219\n0.458082\n\n\nmin\n2.300000e+01\n51.000000\n0.000000\n3.344000e+03\n0.000000\n1923.000000\n0.000000\n0.000000\n0.000000\n-60.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n1.832620e+05\n57.000000\n25.000000\n1.776670e+05\n0.000000\n1980.000000\n0.462000\n0.405000\n2.000000\n-11.990000\n0.000000\n0.033200\n0.070000\n0.000000\n0.098100\n0.353000\n96.099250\n4.000000\n\n\n50%\n5.352520e+05\n64.000000\n36.000000\n2.188670e+05\n0.000000\n1994.000000\n0.579000\n0.591000\n5.000000\n-8.645000\n1.000000\n0.043100\n0.325000\n0.000011\n0.141000\n0.560000\n118.002000\n4.000000\n\n\n75%\n1.587332e+06\n72.000000\n48.000000\n2.645465e+05\n0.000000\n2008.000000\n0.685000\n0.776000\n8.000000\n-6.131000\n1.000000\n0.075300\n0.671000\n0.002220\n0.292000\n0.760000\n137.929000\n4.000000\n\n\nmax\n7.890023e+07\n100.000000\n99.000000\n4.995083e+06\n1.000000\n2021.000000\n0.988000\n1.000000\n11.000000\n3.744000\n1.000000\n0.969000\n0.996000\n1.000000\n1.000000\n1.000000\n243.507000\n5.000000\n\n\n\n\n\n\n\n\n\n7.5.2.4 max()/min()\nReturns the max/min values of numeric columns. If the function is applied on non-numeric columns, it will return the maximum/minimum value based on the order of the alphabet.\n\n#The max() method applied on a Series\nspotify_data['artist_popularity'].max()\n\nnp.int64(100)\n\n\n\n#The max() method applied on a DataFrame\nspotify_data.max()\n\nartist_followers                    78900234\ngenres                                  rock\nartist_name                          高爾宣 OSN\nartist_popularity                        100\ntrack_name           행복했던 날들이었다 days gone by\ntrack_popularity                          99\nduration_ms                          4995083\nexplicit                                   1\nrelease_year                            2021\ndanceability                           0.988\nenergy                                   1.0\nkey                                       11\nloudness                               3.744\nmode                                       1\nspeechiness                            0.969\nacousticness                           0.996\ninstrumentalness                         1.0\nliveness                                 1.0\nvalence                                  1.0\ntempo                                243.507\ntime_signature                             5\ndtype: object\n\n\n\n\n7.5.2.5 mean()/median()\nReturns the mean/median values of numeric columns.\n\n# apply median to the numeric columns\n\nspotify_data.select_dtypes(include='number').median()\n\nartist_followers     535252.000000\nartist_popularity        64.000000\ntrack_popularity         36.000000\nduration_ms          218867.000000\nexplicit                  0.000000\nrelease_year           1994.000000\ndanceability              0.579000\nenergy                    0.591000\nkey                       5.000000\nloudness                 -8.645000\nmode                      1.000000\nspeechiness               0.043100\nacousticness              0.325000\ninstrumentalness          0.000011\nliveness                  0.141000\nvalence                   0.560000\ntempo                   118.002000\ntime_signature            4.000000\ndtype: float64\n\n\n\n\n7.5.2.6 std()\nReturns the standard deviation of numeric columns.\n\nspotify_data.select_dtypes(include='number').std()\n\nartist_followers     5.028746e+06\nartist_popularity    1.028918e+01\ntrack_popularity     1.647684e+01\nduration_ms          9.973214e+04\nexplicit             2.180260e-01\nrelease_year         1.848146e+01\ndanceability         1.594436e-01\nenergy               2.366309e-01\nkey                  3.532546e+00\nloudness             4.449731e+00\nmode                 4.698771e-01\nspeechiness          1.980684e-01\nacousticness         3.211417e-01\ninstrumentalness     2.095551e-01\nliveness             1.980759e-01\nvalence              2.500172e-01\ntempo                2.986422e+01\ntime_signature       4.580822e-01\ndtype: float64\n\n\n\n\n7.5.2.7 sample(n)\nReturns n random observations from a Pandas DataFrame.\n\nspotify_data.sample(4)\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n95984\n1179716\npop & rock\nmor ve ötesi\n65\nYalnız Şarkı\n36\n286440\n0\n1997\n0.441\n...\n9\n-9.126\n1\n0.0305\n0.000003\n0.000000\n0.0698\n0.282\n89.998\n4\n\n\n201066\n1582426\npop & rock\nAndrés Calamaro\n73\nClonazepán y circo\n36\n179093\n0\n1999\n0.381\n...\n9\n-7.031\n1\n0.0326\n0.153000\n0.000000\n0.0892\n0.409\n77.131\n3\n\n\n15002\n666680\nrock\nIndio Solari y los Fundamentalistas del Aire A...\n61\nEl Ruiseñor, el Amor y la Muerte\n45\n310173\n0\n2018\n0.583\n...\n0\n-6.853\n1\n0.0251\n0.087700\n0.008010\n0.0830\n0.446\n80.001\n4\n\n\n186925\n295211\njazz\nStan Getz\n68\nIn Your Own Sweet Way - Live In Kildevælds Chu...\n6\n365493\n0\n1960\n0.448\n...\n10\n-19.986\n0\n0.0431\n0.901000\n0.000008\n0.1290\n0.482\n140.798\n4\n\n\n\n\n4 rows × 21 columns\n\n\n\n\n\n7.5.2.8 dropna()\nDrops all observations with at least one missing value.\n\n#This code is not executed to avoid prining a large table\nspotify_data.dropna()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n2\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n7\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n\n\n3\n16996777\nrap\nJuice WRLD\n96\nRobbery\n0\n240527\n1\n2021\n0.708\n...\n2\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\n\n\n4\n5988689\nrap\nRoddy Ricch\n88\nBig Stepper\n0\n175170\n0\n2021\n0.753\n...\n8\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n243185\n2256652\njazz\nLouis Armstrong\n74\nStardust\n5\n213667\n0\n1923\n0.614\n...\n3\n-11.004\n0\n0.0541\n0.9700\n0.646000\n0.0514\n0.772\n122.319\n4\n\n\n243186\n2256652\njazz\nLouis Armstrong\n74\nKnockin' A Jug - 78 rpm Version\n6\n193760\n0\n1923\n0.788\n...\n10\n-14.032\n1\n0.3010\n0.8820\n0.844000\n0.1240\n0.676\n113.336\n4\n\n\n243187\n2256652\njazz\nLouis Armstrong\n74\nWhen It's Sleepy Time Down South\n4\n200200\n0\n1923\n0.527\n...\n3\n-14.814\n1\n0.0793\n0.9890\n0.000010\n0.1040\n0.370\n105.093\n4\n\n\n243188\n2256652\njazz\nLouis Armstrong\n74\nOn The Sunny Side Of The Street - Part 2\n4\n185973\n0\n1923\n0.559\n...\n0\n-9.804\n1\n0.0512\n0.9890\n0.847000\n0.4480\n0.576\n101.279\n4\n\n\n243189\n2256652\njazz\nLouis Armstrong\n74\nMy Sweet\n4\n195960\n0\n1923\n0.741\n...\n3\n-10.406\n1\n0.0505\n0.9270\n0.078800\n0.0633\n0.816\n105.840\n4\n\n\n\n\n243190 rows × 21 columns\n\n\n\n\n\n7.5.2.9 unique()\nThis functions provides the unique values of a Series. For example, let us find the number of unique genres of songs in the spotify dataset:\n\nspotify_data.genres.unique()\n\narray(['rap', 'pop', 'miscellaneous', 'metal', 'hip hop', 'rock',\n       'pop & rock', 'hoerspiel', 'folk', 'electronic', 'jazz', 'country',\n       'latin'], dtype=object)\n\n\n\n\n7.5.2.10 value_counts()\nThis function provides the number of observations of each value of a Series. For example, let us find the number of songs of each genre in the spotify dataset:\n\nspotify_data.genres.value_counts()\n\ngenres\npop              70441\nrock             49785\npop & rock       43437\nmiscellaneous    35848\njazz             13363\nhoerspiel        12514\nhip hop           7373\nfolk              2821\nlatin             2125\nrap               1798\nmetal             1659\ncountry           1236\nelectronic         790\nName: count, dtype: int64\n\n\nMore than half the songs in the dataset are pop, rock or pop & rock.\n\n\n7.5.2.11 isin()\nThis function provides a boolean Series indicating the position of certain values in a Series. The function is helpful in sub-setting data. For example, let us subset the songs that are either latin, rap, or metal:\n\nlatin_rap_metal_songs = spotify_data.loc[spotify_data.genres.isin(['latin','rap','metal']),:]\nlatin_rap_metal_songs.head()\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n16996777\nrap\nJuice WRLD\n96\nAll Girls Are The Same\n0\n165820\n1\n2021\n0.673\n...\n0\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n16996777\nrap\nJuice WRLD\n96\nLucid Dreams\n0\n239836\n1\n2021\n0.511\n...\n6\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n2\n16996777\nrap\nJuice WRLD\n96\nHear Me Calling\n0\n189977\n1\n2021\n0.699\n...\n7\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n\n\n3\n16996777\nrap\nJuice WRLD\n96\nRobbery\n0\n240527\n1\n2021\n0.708\n...\n2\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\n\n\n4\n5988689\nrap\nRoddy Ricch\n88\nBig Stepper\n0\n175170\n0\n2021\n0.753\n...\n8\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\n\n\n\n\n5 rows × 21 columns",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#data-manipulations-with-pandas",
    "href": "Pandas.html#data-manipulations-with-pandas",
    "title": "7  Pandas",
    "section": "7.6 Data manipulations with Pandas",
    "text": "7.6 Data manipulations with Pandas\n\n7.6.1 Sub-setting data\nIn the chapter on reading data, we learned about different ways for subsetting data, specifically:\n\nSelecting columns: df['column_name'], df[['col1', 'col2']]\nSelecting rows by label: df.loc[]\nSelecting rows by integer position: df.iloc[]\nSelecting by condition (Boolean indexing): df[df['column_name'] &gt; value]\nSelecting by multiple conditions: df[(df['column_name'] &gt; value) & (df['other_column'] == another_value)]\n\n\n\n7.6.2 Setting and Resetting indices\nIn pandas, the index of a dataframe can be accessed by using index attribute, it represents the row labels of a DataFrame. When you create a DataFrame without specifying an index, pandas automatically assigns a RangeIndex, which is a sequence of integers starting from 0.\n\n# read the txt data\nbestseller_data = pd.read_csv('../Data/bestseller_books.txt', sep=';')\nbestseller_data.head()\n\n\n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n0\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n1\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n2\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n3\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n4\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\n# accessing the index of the DataFrame\nbestseller_data.index\n\nRangeIndex(start=0, stop=550, step=1)\n\n\nAlternatively, we can use the index_col parameter in the pd.read_csv() function to set a specific column as the index when reading a CSV file in pandas. This allows us to directly specify which column should be used as the index of the resulting DataFrame.\n\nbestseller_index_data = pd.read_csv('../Data/bestseller_books.txt', sep=';', index_col='Unnamed: 0.1')\nbestseller_index_data.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\nbestseller_index_data.index\n\nIndex([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       540, 541, 542, 543, 544, 545, 546, 547, 548, 549],\n      dtype='int64', length=550)\n\n\nThe index is crucial when subsetting your dataset using methods like loc[], as it identifies the rows you want to select based on their labels.\n\n# get the book that User rating is greater than 4.5\nbestseller_index_data.loc[bestseller_index_data['User Rating']== 4.9].head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n40\n40\nBrown Bear, Brown Bear, What Do You See?\nBill Martin Jr.\n4.9\n14344\n5\n2017\nFiction\n\n\n41\n41\nBrown Bear, Brown Bear, What Do You See?\nBill Martin Jr.\n4.9\n14344\n5\n2019\nFiction\n\n\n81\n81\nDog Man and Cat Kid: From the Creator of Capta...\nDav Pilkey\n4.9\n5062\n6\n2018\nFiction\n\n\n82\n82\nDog Man: A Tale of Two Kitties: From the Creat...\nDav Pilkey\n4.9\n4786\n8\n2017\nFiction\n\n\n83\n83\nDog Man: Brawl of the Wild: From the Creator o...\nDav Pilkey\n4.9\n7235\n4\n2018\nFiction\n\n\n\n\n\n\n\nAdditionally, you can modify the index at any point after the DataFrame is created by using the set_index() function to re-index the rows based on existing column(s) of the DataFrame.\n\nbestseller_data_reindexed = bestseller_data.set_index('Unnamed: 0.1')\nbestseller_data_reindexed.index\n\nIndex([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       540, 541, 542, 543, 544, 545, 546, 547, 548, 549],\n      dtype='int64', name='Unnamed: 0.1', length=550)\n\n\nNote that, the index column does not have to uniquely identify each row. For example, if we need to subset data by Author frequently in our analysis, we can set it as our index\n\nbestseller_author_index = bestseller_data.set_index('Author')\nbestseller_author_index.loc['Jen Sincero']\n\n\n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nName\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\nAuthor\n\n\n\n\n\n\n\n\n\n\n\n\nJen Sincero\n546\n546\nYou Are a Badass: How to Stop Doubting Your Gr...\n4.7\n14331\n8\n2016\nNon Fiction\n\n\nJen Sincero\n547\n547\nYou Are a Badass: How to Stop Doubting Your Gr...\n4.7\n14331\n8\n2017\nNon Fiction\n\n\nJen Sincero\n548\n548\nYou Are a Badass: How to Stop Doubting Your Gr...\n4.7\n14331\n8\n2018\nNon Fiction\n\n\nJen Sincero\n549\n549\nYou Are a Badass: How to Stop Doubting Your Gr...\n4.7\n14331\n8\n2019\nNon Fiction\n\n\n\n\n\n\n\n\nbestseller_author_index.index\n\nIndex(['JJ Smith', 'Stephen King', 'Jordan B. Peterson', 'George Orwell',\n       'National Geographic Kids', 'George R. R. Martin',\n       'George R. R. Martin', 'Amor Towles', 'James Comey', 'Fredrik Backman',\n       ...\n       'R. J. Palacio', 'R. J. Palacio', 'R. J. Palacio', 'R. J. Palacio',\n       'R. J. Palacio', 'Jeff Kinney', 'Jen Sincero', 'Jen Sincero',\n       'Jen Sincero', 'Jen Sincero'],\n      dtype='object', name='Author', length=550)\n\n\nYou can revert it back to a default index using reset_index(). It converts the current index into a regular column and resets the index to a default integer sequence.\n\nbestseller_author_index.reset_index()\n\n\n\n\n\n\n\n\nAuthor\nUnnamed: 0.1\nUnnamed: 0\nName\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\nJJ Smith\n0\n0\n10-Day Green Smoothie Cleanse\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\nStephen King\n1\n1\n11/22/63: A Novel\n4.6\n2052\n22\n2011\nFiction\n\n\n2\nJordan B. Peterson\n2\n2\n12 Rules for Life: An Antidote to Chaos\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\nGeorge Orwell\n3\n3\n1984 (Signet Classics)\n4.7\n21424\n6\n2017\nFiction\n\n\n4\nNational Geographic Kids\n4\n4\n5,000 Awesome Facts (About Everything!) (Natio...\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n545\nJeff Kinney\n545\n545\nWrecking Ball (Diary of a Wimpy Kid Book 14)\n4.9\n9413\n8\n2019\nFiction\n\n\n546\nJen Sincero\n546\n546\nYou Are a Badass: How to Stop Doubting Your Gr...\n4.7\n14331\n8\n2016\nNon Fiction\n\n\n547\nJen Sincero\n547\n547\nYou Are a Badass: How to Stop Doubting Your Gr...\n4.7\n14331\n8\n2017\nNon Fiction\n\n\n548\nJen Sincero\n548\n548\nYou Are a Badass: How to Stop Doubting Your Gr...\n4.7\n14331\n8\n2018\nNon Fiction\n\n\n549\nJen Sincero\n549\n549\nYou Are a Badass: How to Stop Doubting Your Gr...\n4.7\n14331\n8\n2019\nNon Fiction\n\n\n\n\n550 rows × 9 columns\n\n\n\nIn summary, the index is a powerful feature of pandas for organizing and accessing data, and it can be set, changed, or reset as needed to suit your data analysis.\nLater, you will learn about hierarchical indexing, which allows you to create multi-level indices by passing multiple columns.\n\n\n7.6.3 Dropping a column\nIn pandas, you can drop a column from a DataFrame using the drop() method. This is useful when you want to remove columns that are redundant or when you want to simplify your DataFrame by excluding unnecessary data.\ndf.drop(columns='column_name')\ndf.drop(columns= ['col_1', 'col_2'])\n\n# drop the repeated rows in the bestseller dataframe\nbestseller_df = bestseller_index_data.drop(columns = 'Unnamed: 0')\nbestseller_df.head()\n\n\n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\n# another way to drop the column\nbestseller_index_data.drop('Unnamed: 0', axis=1).head()\n\n\n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\nNote: The df.drop() function can be used to remove rows by specifying row labels, index values, or based on conditions. It includes an axis parameter, which defaults to dropping rows (axis=0). Feel free to explore its additional options.\n\n\n7.6.4 Adding a column\nIn a DataFrame, it’s common to create new columns based on existing columns for analysis or prediction purposes. If the column name you specify does not already exist, a new column will be added to the DataFrame. If the column name already exists, the new values will overwrite the existing column.\nLet’s classify books based on their user rating\n\ndef classify_rating(rating):\n    if rating &gt;= 4.5:\n        return 'Highly Rated'\n    elif 3.0 &lt;= rating &lt; 4.5:\n        return 'Moderately Rated'\n    else:\n        return 'Low Rated'\n\n\nbestseller_df['Rating_Class'] = bestseller_df['User Rating'].apply(classify_rating)\n\nbestseller_df.head()\n\n\n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\nrating_class\nRating_class\nRating_Class\n\n\n\n\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\nHighly Rated\nHighly Rated\nHighly Rated\n\n\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\nHighly Rated\nHighly Rated\nHighly Rated\n\n\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\nHighly Rated\nHighly Rated\nHighly Rated\n\n\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\nHighly Rated\nHighly Rated\nHighly Rated\n\n\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\nHighly Rated\nHighly Rated\nHighly Rated\n\n\n\n\n\n\n\n\n\n7.6.5 Renaming a column\nTo rename a column in a pandas DataFrame, you can use the rename() method. .\n\nRenaming a single column\n\ndf.rename(columns={'old_name': 'new_name'}, inplace=False)\n\n# rename Year to Publication Year\nbestseller_df.rename(columns={'Year':'Publication Year'}, inplace=True)\nbestseller_df.head()\n\n\n\n\n\n\n\n\nName\nAuthor\nUser Rating\nReviews\nPrice\nPublication Year\nGenre\n\n\n\n\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\nRenameing multiple columns\n\nSpecifying a dictionary that maps old column names to new one\ndf_renamed_multiple = df.rename(columns={'col1': 'new_col1', 'col2': 'new_col2'})\n\n# rename Name to Book Name, and Author to Writer\nbestseller_df.rename(columns={'Name':'Book Name', 'Author':'Writer'}, inplace=True)\nbestseller_df.head()\n\n\n\n\n\n\n\n\nBook Name\nWriter\nUser Rating\nReviews\nPrice\nPublication Year\nGenre\n\n\n\n\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n\n\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n\n\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n\n\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n\n\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n\n\n\n\n\n\n\n\n\n7.6.6 Sorting data\nSorting dataset is a very common operation. The sort_values() function of Pandas can be used to sort a Pandas DataFrame or Series. Let us sort the spotify data in decreasing order of track_popularity:\n\nbestseller_sorted = bestseller_index_data.sort_values(by = 'User Rating', ascending = False)\nbestseller_sorted.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nName\nAuthor\nUser Rating\nReviews\nPrice\nYear\nGenre\n\n\n\n\n521\n521\nUnfreedom of the Press\nMark R. Levin\n4.9\n5956\n11\n2019\nNon Fiction\n\n\n420\n420\nThe Legend of Zelda: Hyrule Historia\nPatrick Thorpe\n4.9\n5396\n20\n2013\nFiction\n\n\n248\n248\nOh, the Places You'll Go!\nDr. Seuss\n4.9\n21834\n8\n2015\nFiction\n\n\n247\n247\nOh, the Places You'll Go!\nDr. Seuss\n4.9\n21834\n8\n2014\nFiction\n\n\n246\n246\nOh, the Places You'll Go!\nDr. Seuss\n4.9\n21834\n8\n2013\nFiction\n\n\n\n\n\n\n\n\n\n7.6.7 Ranking data\nWith the rank() function, we can rank the observations.\nFor example, let us add a new column to the bestseller data that provides the rank of the User Rating column:\n\nbestseller_ranked = bestseller_df.copy()\nbestseller_ranked['Rating_rank']=bestseller_ranked['User Rating'].rank()\nbestseller_ranked.head()\n\n\n\n\n\n\n\n\nBook Name\nWriter\nUser Rating\nReviews\nPrice\nPublication Year\nGenre\nRating_rank\n\n\n\n\n0\n10-Day Green Smoothie Cleanse\nJJ Smith\n4.7\n17350\n8\n2016\nNon Fiction\n317.5\n\n\n1\n11/22/63: A Novel\nStephen King\n4.6\n2052\n22\n2011\nFiction\n211.0\n\n\n2\n12 Rules for Life: An Antidote to Chaos\nJordan B. Peterson\n4.7\n18979\n15\n2018\nNon Fiction\n317.5\n\n\n3\n1984 (Signet Classics)\nGeorge Orwell\n4.7\n21424\n6\n2017\nFiction\n317.5\n\n\n4\n5,000 Awesome Facts (About Everything!) (Natio...\nNational Geographic Kids\n4.8\n7665\n12\n2019\nNon Fiction\n435.0\n\n\n\n\n\n\n\nNote the column Rating_rank. Why does it contain floating point numbers? Check the rank() documentation to find out!",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#arithematic-operations-within-and-between-dataframes",
    "href": "Pandas.html#arithematic-operations-within-and-between-dataframes",
    "title": "7  Pandas",
    "section": "7.7 Arithematic operations within and between DataFrames",
    "text": "7.7 Arithematic operations within and between DataFrames\nPandas is built on top of NumPy, and for arithmetic operations, it inherits many of NumPy’s powerful features. Pandas provides an intuitive and efficient way to perform arithmetic operations both within a DataFrame (on columns or rows) and between multiple DataFrames. These operations are element-wise, meaning they are applied to corresponding elements of the DataFrame or Series.\n\n7.7.1 Arithematic operations within a DataFrame\nYou can easily perform arithmetic operations on columns or rows within a DataFrame\n\nArithmetic Between Columns\n\n\n# Create a DataFrame\ndata = {'A': [1, 2, 3],\n        'B': [4, 5, 6],\n        'C': [7, 8, 9]}\ndf = pd.DataFrame(data)\n\n# Adding two columns\ndf['A_plus_B'] = df['A'] + df['B']\n\n# Multiplying two columns\ndf['A_times_C'] = df['A'] * df['C']\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nA_plus_B\nA_times_C\n\n\n\n\n0\n1\n4\n7\n5\n7\n\n\n1\n2\n5\n8\n7\n16\n\n\n2\n3\n6\n9\n9\n27\n\n\n\n\n\n\n\n\nArithmetic on Entire Rows\n\n\n# Summing values across columns for each row\ndf['row_sum'] = df.sum(axis=1)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nA_plus_B\nA_times_C\nrow_sum\n\n\n\n\n0\n1\n4\n7\n5\n7\n24\n\n\n1\n2\n5\n8\n7\n16\n38\n\n\n2\n3\n6\n9\n9\n27\n54\n\n\n\n\n\n\n\n\n\n7.7.2 Arithematic operations between DataFrames\nWhen performing operations between two DataFrames, pandas aligns the data based on both the index and column labels. If the indices or columns do not match, pandas automatically fills the missing values with NaN (not-a-number) unless a fill value is provided.\nLet us create two toy DataFrames:\n\n#Creating two toy DataFrames\ntoy_df1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])\ntoy_df2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])\n\n\n#DataFrame 1\ntoy_df1\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n3\n4\n\n\n2\n5\n6\n\n\n\n\n\n\n\n\n#DataFrame 2\ntoy_df2\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n100\n200\n\n\n1\n300\n400\n\n\n2\n500\n600\n\n\n\n\n\n\n\nElement by element operations between two DataFrames can be performed with the operators +, -, *,/,**, and %. Below is an example of element-by-element addition of two DataFrames:\n\n# Element-by-element arithmetic addition of the two DataFrames\ntoy_df1 + toy_df2\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n101\n202\n\n\n1\n303\n404\n\n\n2\n505\n606\n\n\n\n\n\n\n\nNote that these operations create problems when the row indices and/or column names of the two DataFrames do not match. See the example below:\n\n#Creating another toy example of a DataFrame\ntoy_df3 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'], index=[1,2,3])\ntoy_df3\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n1\n100\n200\n\n\n2\n300\n400\n\n\n3\n500\n600\n\n\n\n\n\n\n\n\n#Adding DataFrames with some unmatching row indices\ntoy_df1 + toy_df3\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\nNaN\nNaN\n\n\n1\n103.0\n204.0\n\n\n2\n305.0\n406.0\n\n\n3\nNaN\nNaN\n\n\n\n\n\n\n\nNote that the rows whose indices match between the two DataFrames are added up. The rest of the values are missing (or NaN) because only one of the DataFrames has that index.\nAs in the case of row indices, missing values will also appear in the case of unmatching column names, as shown in the example below.\n\ntoy_df4 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['b','c'])\ntoy_df4\n\n\n\n\n\n\n\n\nb\nc\n\n\n\n\n0\n100\n200\n\n\n1\n300\n400\n\n\n2\n500\n600\n\n\n\n\n\n\n\n\n#Adding DataFrames with some unmatching column names\ntoy_df1 + toy_df4\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\nNaN\n102\nNaN\n\n\n1\nNaN\n304\nNaN\n\n\n2\nNaN\n506\nNaN\n\n\n\n\n\n\n\n\n\n7.7.3 Arithmetic Between DataFrame and Series (Broadcasting)\nPandas supports broadcasting, where a Series can be broadcasted to match the dimensions of a DataFrame during an arithmetic operation.\n\n# Broadcasting: The row [1,2] (a Series) is added on every row in df2 \ntoy_df1.loc[0,:] + toy_df2\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n101\n202\n\n\n1\n301\n402\n\n\n2\n501\n602\n\n\n\n\n\n\n\nNote that the + operator is used to add values of a Series to a DataFrame based on column names. For adding a Series to a DataFrame based on row indices, we cannot use the + operator. Instead, we’ll need to use the add() function as explained below.\nBroadcasting based on row/column labels: We can use the add() function to broadcast a Series to a DataFrame. By default the Series adds based on column names, as in the case of the + operator.\n\n# Add the first row of df1 (a Series) to every row in df2 \ntoy_df2.add(toy_df1.loc[0,:])\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n101\n202\n\n\n1\n301\n402\n\n\n2\n501\n602\n\n\n\n\n\n\n\nFor broadcasting based on row indices, we use the axis argument of the add() function.\n\n# The second column of df1 (a Series) is added to every col in df2\ntoy_df2.add(toy_df1.loc[:,'b'],axis='index')\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n102\n202\n\n\n1\n304\n404\n\n\n2\n506\n606\n\n\n\n\n\n\n\n\n\n7.7.4 Converting a DataFrame to a NumPy Array\nIn pandas, you often need to convert a DataFrame or Series into a NumPy array to perform specific operations or to interface with libraries that require NumPy arrays.\nThere are two common methods for converting pandas objects to NumPy arrays: .values and .to_numpy().\n\n7.7.4.1 Using .values\n\nThe .values attribute returns the underlying NumPy array representation of a DataFrame or Series.\nIt provides a direct conversion, but it has been deprecated in favor of the newer .to_numpy() method because .values does not always handle all DataFrame types consistently.\n\n\ntoy_df1\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n3\n4\n\n\n2\n5\n6\n\n\n\n\n\n\n\n\ntoy_df1.values\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\n\n\n7.7.4.2 Using .to_numpy()\n\nThe .to_numpy() method is a recommended way to convert a DataFrame to a NumPy array.\nIt handles different types of data, such as nullable types or mixed data types, more consistently than .values.\nAdditional parameters: .to_numpy() can take parameters like dtype (to specify the data type) and copy (to create a copy or not).\n\n\ntoy_df1.to_numpy()\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\nIn the NumPy chapter, we explored the below case study that demonstrated the benefits of using matrix multiplication in NumPy over python loops. However, this same operation cannot be directly performed in a pandas DataFrame in the same way.\n\nfood_qty = pd.read_csv('../data/food_quantity.csv',index_col=0)\nprice = pd.read_csv('../data/price.csv',index_col=0)\n\n\n%%time\nstore_expense = pd.DataFrame(0.0, columns=price.columns, index = food_qty.index)\nfor person in store_expense.index:\n    for store in store_expense.columns:\n        for food in food_qty.columns:\n            store_expense.loc[person, store] += food_qty.loc[person, food]*price.loc[food, store]\n\nstore_expense\n\nCPU times: total: 0 ns\nWall time: 5.01 ms\n\n\n\n\n\n\n\n\n\nTarget\nKroger\n\n\nPerson\n\n\n\n\n\n\nBen\n50.0\n49.0\n\n\nBarbara\n58.5\n61.0\n\n\nBeth\n43.5\n43.5\n\n\n\n\n\n\n\n\n%%time\n# using matrix multiplication in numpy\nimport numpy as np\nnp.dot(food_qty.values, price.values)\n\nCPU times: total: 0 ns\nWall time: 996 μs\n\n\narray([[50. , 49. ],\n       [58.5, 61. ],\n       [43.5, 43.5]])\n\n\n\n# converting the result to a DataFrame\npd.DataFrame(np.dot(food_qty.values, price.values), columns=price.columns, index=food_qty.index)\n\n\n\n\n\n\n\n\nTarget\nKroger\n\n\nPerson\n\n\n\n\n\n\nBen\n50.0\n49.0\n\n\nBarbara\n58.5\n61.0\n\n\nBeth\n43.5\n43.5",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#advanced-dataframe-operations",
    "href": "Pandas.html#advanced-dataframe-operations",
    "title": "7  Pandas",
    "section": "7.8 Advanced DataFrame Operations",
    "text": "7.8 Advanced DataFrame Operations\nPandas provides powerful methods for applying functions to DataFrame rows, columns, or individual elements. These methods allow for flexible data transformations, making them essential for advanced data manipulation.\n\n7.8.1 apply(): Row-wise, or Column-wise\nThe apply() function in pandas is used to apply a function along an axis of a DataFrame.\n\nRow-wise: You can apply a function to each row using axis=1.\nColumn-wise: You can apply a function to each column using axis=0 (default behavior).\n\nLet’s create a DataFrame and apply a custom function to each row.\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n})\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n4\n7\n\n\n1\n2\n5\n8\n\n\n2\n3\n6\n9\n\n\n\n\n\n\n\n\n# Function to compute the row sum\ndef row_sum(row):\n    return row.sum()\n\n\n# Apply the function row-wise (axis=1)\ndf['Row_Sum'] = df.apply(row_sum, axis=1)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nRow_Sum\n\n\n\n\n0\n1\n4\n7\n12\n\n\n1\n2\n5\n8\n15\n\n\n2\n3\n6\n9\n18\n\n\n\n\n\n\n\nLet’s apply the same function to each column next\n\n# Apply the function column-wise (axis=0)\ncolumn_sum = df.apply(sum, axis=0)\ncolumn_sum\n\nA           6\nB          15\nC          24\nRow_Sum    45\ndtype: int64\n\n\n\n\n7.8.2 map(): Element-wise\nmap() is typically used for element-wise transformations on a Series.\n\n# write the function to compute square of a number\ndef square(x):\n    return x ** 2\n\n# use the map() function to apply a function element-wise\ndf['A_squared'] = df['A'].map(square)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nRow_Sum\nA_squared\n\n\n\n\n0\n1\n4\n7\n12\n1\n\n\n1\n2\n5\n8\n15\n4\n\n\n2\n3\n6\n9\n18\n9\n\n\n\n\n\n\n\nElement-wise transformation on an entire DataFrame.\n\ndf.map(square)\n\n\n\n\n\n\n\n\nA\nB\nC\nRow_Sum\nA_squared\n\n\n\n\n0\n1\n16\n49\n144\n1\n\n\n1\n4\n25\n64\n225\n16\n\n\n2\n9\n36\n81\n324\n81\n\n\n\n\n\n\n\nWhen applying a function to an entire DataFrame, it’s important to ensure that all columns have the same data type. Otherwise, you might encounter errors or unexpected behavior.\n\n# Create a DataFrame\ndf_str = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': ['I', 'major', 'in'],\n    'C': [7, 8, 9]\n})\n# uncomment the following line to run the code\n# df_str.map(square)\n\n\n\n7.8.3 Key Differences Between map() and apply()\n\n\n\n\n\n\n\n\nFeature\nmap()\napply()\n\n\n\n\nApplies to\nBoth Series and DataFrame\nBoth Series and DataFrame\n\n\nFunctionality\nElement-wise transformations\nCan apply more complex functions (row-wise or column-wise for DataFrame)\n\n\nInput\nFunction, dictionary, or another Series\nFunction\n\n\nUse Case\nUsed for simpler element-wise replacements or mappings\nUsed for more complex operations that can act on rows, columns, or individual elements\n\n\nAxis Option\nN/A\nCan specify axis in DataFrame (row-wise or column-wise)\n\n\n\n\n7.8.3.1 Summary:\n\nmap(): Primarily used for element-wise operations. It can take functions, dictionaries, or Series as input for substitution or transformation.\napply(): Apply complex functions across rows or columns.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#introduction-to-lambda-functions-in-pandas",
    "href": "Pandas.html#introduction-to-lambda-functions-in-pandas",
    "title": "7  Pandas",
    "section": "7.9 Introduction to Lambda Functions in Pandas",
    "text": "7.9 Introduction to Lambda Functions in Pandas\nA lambda function is a small, anonymous function defined using the lambda keyword in Python. It can take any number of arguments but can only have one expression. In pandas, lambda functions are often used for quick and concise operations on data.\n\n7.9.1 Syntax of Lambda Functions:\nlambda arguments: expression\n\n\n7.9.2 Key Features\n\nAnonymous: Lambda functions are defined without a name.\nConcise: They are typically used for small, simple functions that are not reused elsewhere.\nSingle Expression: They can only contain one expression, which is evaluated and returned.\n\nLambda functions are commonly used with apply() and map() to efficiently transform data in pandas, allowing for quick, inline function definitions without the need to explicitly define separate functions.\nLet’s use a lambda function to rewrite the apply() and map() operations from the previous example\n\ndf['A_squared_lamdba'] = df['A'].map(lambda x: x ** 2)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nRow_Sum\nA_squared\nA_squared_lamdba\n\n\n\n\n0\n1\n4\n7\n12\n1\n1\n\n\n1\n2\n5\n8\n15\n4\n4\n\n\n2\n3\n6\n9\n18\n9\n9\n\n\n\n\n\n\n\n\ndf_squared = df.apply(lambda x: x ** 2)\ndf_squared\n\n\n\n\n\n\n\n\nA\nB\nC\nRow_Sum\nA_squared\nA_squared_lamdba\n\n\n\n\n0\n1\n16\n49\n144\n1\n1\n\n\n1\n4\n25\n64\n225\n16\n16\n\n\n2\n9\n36\n81\n324\n81\n81\n\n\n\n\n\n\n\nLet’s look at other examples using lambda\nExample 1: adding a new column based on existing columns\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\n\n# Use a lambda function to create a new column 'C' as the sum of 'A' and 'B'\ndf['C'] = df.apply(lambda row: row['A'] + row['B'], axis=1)\nprint(df)\n\n   A  B  C\n0  1  4  5\n1  2  5  7\n2  3  6  9\n\n\nExample 2: Filter rows based on a condition\n\n# Filter rows where values in column 'A' are greater than 1\nfiltered_df = df[df['A'].apply(lambda x: x &gt; 1)]\nprint(filtered_df)\n\n   A  B  C\n1  2  5  7\n2  3  6  9",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#understanding-inplacetrue-and-inplacefalse",
    "href": "Pandas.html#understanding-inplacetrue-and-inplacefalse",
    "title": "7  Pandas",
    "section": "7.10 Understanding inplace=True and inplace=False",
    "text": "7.10 Understanding inplace=True and inplace=False\nAbove, we covered multiple methods that allow you to modify a DataFrame or Series. The inplace parameter of these methods determines whether the operation modifies the original object or returns a new object.\n\ninplace=True: Modifies the original DataFrame or Series directly and returns None. The changes are made in place, meaning the original object is altered.\ninplace=False: Returns a new DataFrame or Series with the modifications, leaving the original object unchanged. This is the default behavior for most methods.\n\nNote: The default value for the inplace parameter is False for most methods.\nUsage Example:\n\n# Create a DataFrame\ninplace_df = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\n\n# Drop a column without modifying the original DataFrame\ndf_dropped = inplace_df.drop('A', axis=1)\nprint(df_dropped)  # New DataFrame without column 'A'\nprint(inplace_df)          # Original DataFrame remains unchanged\n\n# Drop a column in place\ninplace_df.drop('A', axis=1, inplace=True)\nprint(inplace_df)          # Original DataFrame is now modified\n\n   B\n0  4\n1  5\n2  6\n   A  B\n0  1  4\n1  2  5\n2  3  6\n   B\n0  4\n1  5\n2  6",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#case-study",
    "href": "Pandas.html#case-study",
    "title": "7  Pandas",
    "section": "7.11 Case study",
    "text": "7.11 Case study\nTo see the application of arithematic operations on DataFrames, let us see the case study below.\nSong recommendation: Spotify recommends songs based on songs listened by the user. Suppose you have listened to the song drivers license. Spotify intends to recommend you 5 songs that are similar to drivers license. Which songs should it recommend?\nLet us see the available song information that can help us identify songs similar to drivers license. The columns attribute of DataFrame will display all the columns names. The description of some of the column names relating to audio features is here.\n\nspotify_data.columns\n\nIndex(['artist_followers', 'genres', 'artist_name', 'artist_popularity',\n       'track_name', 'track_popularity', 'duration_ms', 'explicit',\n       'release_year', 'danceability', 'energy', 'key', 'loudness', 'mode',\n       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n       'valence', 'tempo', 'time_signature'],\n      dtype='object')\n\n\nSolution approach: We have several features of a song. Let us find songs similar to drivers license in terms of danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature and tempo. Note that we are considering only audio features for simplicity.\nTo find the songs most similar to drivers license, we need to define a measure that quantifies the similarity. Let us define similarity of a song with drivers license as the Euclidean distance of the song from drivers license, where the coordinates of a song are: (danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, time_signature, tempo). Thus, similarity can be formulated as:\n\\[\\scriptsize\nSimilarity_{DL-S} = \\sqrt{(danceability_{DL}-danceability_{S})^2+(energy_{DL}-energy_{S})^2 +\\dots + (tempo_{DL}-tempo_{S})^2}\n\\]\nwhere the subscript DL stands for drivers license and S stands for any song. The top 5 songs with the least value of \\(Similarity_{DL-S}\\) will be the most similar to drivers lincense and should be recommended.\nLet us subset the columns that we need to use to compute the Euclidean distance.\n\naudio_features = spotify_data[['danceability', 'energy', 'key', 'loudness','mode','speechiness',\n                               'acousticness', 'instrumentalness', 'liveness','valence', 'tempo', 'time_signature']]\n\n\naudio_features.head()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n0.673\n0.529\n0\n-7.226\n1\n0.3060\n0.0769\n0.000338\n0.0856\n0.203\n161.991\n4\n\n\n1\n0.511\n0.566\n6\n-7.230\n0\n0.2000\n0.3490\n0.000000\n0.3400\n0.218\n83.903\n4\n\n\n2\n0.699\n0.687\n7\n-3.997\n0\n0.1060\n0.3080\n0.000036\n0.1210\n0.499\n88.933\n4\n\n\n3\n0.708\n0.690\n2\n-5.181\n1\n0.0442\n0.3480\n0.000000\n0.2220\n0.543\n79.993\n4\n\n\n4\n0.753\n0.597\n8\n-8.469\n1\n0.2920\n0.0477\n0.000000\n0.1970\n0.616\n76.997\n4\n\n\n\n\n\n\n\n\n#Distribution of values of audio_features\naudio_features.describe()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\ncount\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n\n\nmean\n0.568357\n0.580633\n5.240326\n-9.432548\n0.670928\n0.111984\n0.383938\n0.071169\n0.223756\n0.552302\n119.335060\n3.884177\n\n\nstd\n0.159444\n0.236631\n3.532546\n4.449731\n0.469877\n0.198068\n0.321142\n0.209555\n0.198076\n0.250017\n29.864219\n0.458082\n\n\nmin\n0.000000\n0.000000\n0.000000\n-60.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.462000\n0.405000\n2.000000\n-11.990000\n0.000000\n0.033200\n0.070000\n0.000000\n0.098100\n0.353000\n96.099250\n4.000000\n\n\n50%\n0.579000\n0.591000\n5.000000\n-8.645000\n1.000000\n0.043100\n0.325000\n0.000011\n0.141000\n0.560000\n118.002000\n4.000000\n\n\n75%\n0.685000\n0.776000\n8.000000\n-6.131000\n1.000000\n0.075300\n0.671000\n0.002220\n0.292000\n0.760000\n137.929000\n4.000000\n\n\nmax\n0.988000\n1.000000\n11.000000\n3.744000\n1.000000\n0.969000\n0.996000\n1.000000\n1.000000\n1.000000\n243.507000\n5.000000\n\n\n\n\n\n\n\nNote that the audio features differ in terms of scale. Some features like key have a wide range of [0,11], while others like danceability have a very narrow range of [0,0.988]. If we use them directly, features like danceability will have a much higher influence on \\(Similarity_{DL-S}\\) as compared to features like key. Assuming we wish all the features to have equal weight in quantifying a song’s similarity to drivers license, we should scale the features, so that their values are comparable.\nLet us scale the value of each column to a standard uniform distribution: \\(U[0,1]\\).\nFor scaling the values of a column to \\(U[0,1]\\), we need to subtract the minimum value of the column from each value, and divide by the range of values of the column. For example, danceability can be standardized as follows:\n\n#Scaling danceability to U[0,1]\ndanceability_value_range = audio_features.danceability.max()-audio_features.danceability.min()\ndanceability_std = (audio_features.danceability-audio_features.danceability.min())/danceability_value_range\ndanceability_std\n\n0         0.681174\n1         0.517206\n2         0.707490\n3         0.716599\n4         0.762146\n            ...   \n243185    0.621457\n243186    0.797571\n243187    0.533401\n243188    0.565789\n243189    0.750000\nName: danceability, Length: 243190, dtype: float64\n\n\nHowever, it will be cumbersome to repeat the above code for each audio feature. We can instead write a function that scales values of a column to \\(U[0,1]\\), and apply the function on all the audio features.\n\n#Function to scale a column to U[0,1]\ndef scale_uniform(x):\n    return (x-x.min())/(x.max()-x.min())\n\nWe will use the Pandas function apply() to apply the above function to the DataFrame audio_features.\n\n#Scaling all audio features to U[0,1]\naudio_features_scaled = audio_features.apply(scale_uniform)\n\nThe above two blocks of code can be concisely written with the lambda function as:\n\naudio_features_scaled = audio_features.apply(lambda x: (x-x.min())/(x.max()-x.min()))\n\n\n#All the audio features are scaled to U[0,1]\naudio_features_scaled.describe()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\ncount\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n243190.000000\n\n\nmean\n0.575260\n0.580633\n0.476393\n0.793290\n0.670928\n0.115566\n0.385480\n0.071169\n0.223756\n0.552302\n0.490068\n0.776835\n\n\nstd\n0.161380\n0.236631\n0.321141\n0.069806\n0.469877\n0.204405\n0.322431\n0.209555\n0.198076\n0.250017\n0.122642\n0.091616\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.467611\n0.405000\n0.181818\n0.753169\n0.000000\n0.034262\n0.070281\n0.000000\n0.098100\n0.353000\n0.394647\n0.800000\n\n\n50%\n0.586032\n0.591000\n0.454545\n0.805644\n1.000000\n0.044479\n0.326305\n0.000011\n0.141000\n0.560000\n0.484594\n0.800000\n\n\n75%\n0.693320\n0.776000\n0.727273\n0.845083\n1.000000\n0.077709\n0.673695\n0.002220\n0.292000\n0.760000\n0.566427\n0.800000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\nSince we need to find the Euclidean distance from the song drivers license, let us find the index of the row containing features of drivers license.\n\n#Index of the row consisting of drivers license can be found with the index attribute\ndrivers_license_index = spotify_data[spotify_data.track_name=='drivers license'].index[0]\n\nNote that the object returned by the index attribute is of type pandas.core.indexes.numeric.Int64Index. The elements of this object can be retrieved like the elements of a python list. That is why the object is sliced with [0] to return the first element of the object. As there is only one observation with the track_name as drivers license, we sliced the first element. If there were multiple observations with track_name as drivers license, we will obtain the indices of all those observations with the index attribute.\n\naudio_features_scaled.loc[drivers_license_index,:]\n\ndanceability        0.592105\nenergy              0.436000\nkey                 0.909091\nloudness            0.803825\nmode                1.000000\nspeechiness         0.062023\nacousticness        0.723896\ninstrumentalness    0.000013\nliveness            0.105000\nvalence             0.132000\ntempo               0.590841\ntime_signature      0.800000\nName: 2398, dtype: float64\n\n\nNow, we’ll subtract the audio features of drivers license from all other songs (broadcasting):\n\n#Audio features of drivers license are being subtracted from audio features of all songs by broadcasting\nsongs_minus_DL = audio_features_scaled-audio_features_scaled.loc[drivers_license_index,:]\n\nNow, let us square the difference computed above. We’ll use the in-built python function pow() to square the difference:\n\nsongs_minus_DL_sq = songs_minus_DL.pow(2)\nsongs_minus_DL_sq.head()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n0\n0.007933\n0.008649\n0.826446\n0.000580\n0.0\n0.064398\n0.418204\n1.055600e-07\n0.000376\n0.005041\n0.005535\n0.0\n\n\n1\n0.005610\n0.016900\n0.132231\n0.000577\n1.0\n0.020844\n0.139498\n1.716100e-10\n0.055225\n0.007396\n0.060654\n0.0\n\n\n2\n0.013314\n0.063001\n0.074380\n0.005586\n1.0\n0.002244\n0.171942\n5.382400e-10\n0.000256\n0.134689\n0.050906\n0.0\n\n\n3\n0.015499\n0.064516\n0.528926\n0.003154\n0.0\n0.000269\n0.140249\n1.716100e-10\n0.013689\n0.168921\n0.068821\n0.0\n\n\n4\n0.028914\n0.025921\n0.033058\n0.000021\n0.0\n0.057274\n0.456981\n1.716100e-10\n0.008464\n0.234256\n0.075428\n0.0\n\n\n\n\n\n\n\nNow, we’ll sum the squares of differences from all audio features to compute the similarity of all songs to drivers license.\n\ndistance_squared = songs_minus_DL_sq.sum(axis = 1)\ndistance_squared.head()\n\n0    1.337163\n1    1.438935\n2    1.516317\n3    1.004043\n4    0.920316\ndtype: float64\n\n\nNow, we’ll sort these distances to find the top 5 songs closest to drivers’s license.\n\ndistances_sorted = distance_squared.sort_values()\ndistances_sorted.head()\n\n2398      0.000000\n81844     0.008633\n4397      0.011160\n130789    0.015018\n143744    0.015058\ndtype: float64\n\n\nUsing the indices of the top 5 distances, we will identify the top 5 songs most similar to drivers license:\n\nspotify_data.loc[distances_sorted.index[0:6],:]\n\n\n\n\n\n\n\n\nartist_followers\ngenres\nartist_name\nartist_popularity\ntrack_name\ntrack_popularity\nduration_ms\nexplicit\nrelease_year\ndanceability\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\n\n\n\n\n2398\n1444702\npop\nOlivia Rodrigo\n88\ndrivers license\n99\n242014\n1\n2021\n0.585\n...\n10\n-8.761\n1\n0.0601\n0.721\n0.000013\n0.105\n0.132\n143.874\n4\n\n\n81844\n2264501\npop\nJay Chou\n74\n安靜\n49\n334240\n0\n2001\n0.513\n...\n10\n-7.853\n1\n0.0281\n0.688\n0.000008\n0.116\n0.123\n143.924\n4\n\n\n4397\n25457\npop\nTerence Lam\n60\n拼命無恙 in Bb major\n52\n241062\n0\n2020\n0.532\n...\n10\n-9.690\n1\n0.0269\n0.674\n0.000000\n0.117\n0.190\n151.996\n4\n\n\n130789\n176266\npop\nAlan Tam\n54\n從後趕上\n8\n258427\n0\n1988\n0.584\n...\n10\n-11.889\n1\n0.0282\n0.707\n0.000002\n0.107\n0.124\n140.147\n4\n\n\n143744\n396326\npop & rock\nLaura Branigan\n64\nHow Am I Supposed to Live Without You\n40\n263320\n0\n1983\n0.559\n...\n10\n-8.260\n1\n0.0355\n0.813\n0.000083\n0.134\n0.185\n139.079\n4\n\n\n35627\n1600562\npop\nTiziano Ferro\n68\nNon Me Lo So Spiegare\n44\n240040\n0\n2014\n0.609\n...\n11\n-7.087\n1\n0.0352\n0.706\n0.000000\n0.130\n0.207\n146.078\n4\n\n\n\n\n6 rows × 21 columns\n\n\n\nWe can see the top 5 songs most similar to drivers license in the track_name column above. Interestingly, three of the five songs are Asian! These songs indeed sound similar to drivers license!",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "Pandas.html#independent-practice",
    "href": "Pandas.html#independent-practice",
    "title": "7  Pandas",
    "section": "7.12 Independent Practice:",
    "text": "7.12 Independent Practice:\n\n7.12.1 Practice exercise 1\n\n7.12.1.1 \nRead the file Top 10 Albums By Year.csv. This file contains the top 10 albums for each year from 1990 to 2021. Each row corresponds to a unique album.\n\n\n7.12.1.2 \nPrint the summary statistics of the data, and answer the following questions:\n\nWhat proportion of albums have 15 or lesser tracks? Mention a range for the proportion.\nWhat is the mean length of a track (in minutes)?\n\n\n\n7.12.1.3 \nWhy is Worldwide Sales not included in the summary statistics table printed in the above step?\n\n\n7.12.1.4 \nUpdate the DataFrame so that Worldwide Sales is included in the summary statistics table. Print the summary statistics table.\nHint: Sometimes it may not be possible to convert an object to numeric(). For example, the object ‘hi’ cannot be converted to a numeric() by the python compiler. To avoid getting an error, use the errors argument of to_numeric() to force such conversions to NaN (missing value).\n\n\n7.12.1.5 \nCreate a new column called mean_sales_per_year that computes the average worldwide sales per year for each album, assuming that the worldwide sales are as of 2022. Print the first 5 rows of the updated DataFrame.\n\n\n7.12.1.6 \nFind the album having the highest worldwide sales per year, and its artist.\n\n\n7.12.1.7 \nSubset the data to include only Hip-Hop albums. How many Hip_Hop albums are there?\n\n\n7.12.1.8 \nWhich album amongst hip-hop has the higest mean sales per year per track, and who is its artist?\n\n\n\n7.12.2 Practice exercise 2\n\n7.12.2.1 \nRead the file STAT303-1 survey for data analysis.csv.\n\n\n7.12.2.2 \nHow many observations and variables are there in the data?\n\n\n7.12.2.3 \nRename all the columns of the data, except the first two columns, with the shorter names in the list new_col_names given below. The order of column names in the list is the same as the order in which the columns are to be renamed starting with the third column from the left.\n\n\n7.12.2.4 \nRename the following columns again:\n\nRename do_you_smoke to smoke.\nRename are_you_an_introvert_or_extrovert to introvert_extrovert.\n\nHint: Use the function rename()\n\n\n7.12.2.5 \nFind the proportion of people going to more than 4 parties per month. Use the variable parties_per_month.\n\n\n7.12.2.6 \nAmong the people who go to more than 4 parties a month, what proportion of them are introverts?\n\n\n7.12.2.7 \nFind the proportion of people in each category of the variable how_happy.\n\n\n7.12.2.8 \nAmong the people who go to more than 4 parties a month, what proportion of them are either Pretty happy or Very happy?\n\n\n7.12.2.9 \nExamine the column num_insta_followers. Some numbers in the column contain a comma(,) or a tilde(~). Remove both these characters from the numbers in the column.\nHint: You may use the function str.replace() of the Pandas Series class.\n\n\n7.12.2.10 \nConvert the column num_insta_followers to numeric. Coerce the errors.\n\n\n7.12.2.11 \nWhat is the mean internet_hours_per_day for the top 46 people in terms of number of instagram followers?\n\n\n7.12.2.12 \nWhat is the mean internet_hours_per_day for the remaining people?\n\n\n\n7.12.3 Practice exercise 3\n\n7.12.3.1 \nUse the updated dataset from Practice exercise 2.\nThe last four variables in the dataset are:\n\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\nEach of the above variables has values - Agree / Disagree. Replace Agree with 1 and Disagree with 0.\nHint : You can do it with any one of the following methods:\n\nUse the map() function\nUse the apply() function with the lambda function\nUse the replace() function\n\nTwo of the above methods avoid a for-loop. Which ones?",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pandas</span>"
    ]
  },
  {
    "objectID": "data_types_in_pandas.html",
    "href": "data_types_in_pandas.html",
    "title": "8  More on Pandas",
    "section": "",
    "text": "8.1 Types of Data\nEach column(feature) in a pandas dataframe has a datatype associated with it. Those datatypes can be grouped into Numerical, Categorical, and Dates.\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nCode\nmovie_ratings = pd.read_csv('./datasets/movie_ratings.csv')\nmovie_ratings.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\nNov 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n\n\n1\nMajor Dundee\n14873\n14873\n3800000\nApr 07 1965\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n\n\n2\nThe Informers\n315000\n315000\n18000000\nApr 24 2009\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n\n\n3\nBuffalo Soldiers\n353743\n353743\n15000000\nJul 25 2003\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n\n\n4\nThe Last Sin Eater\n388390\n388390\n2200000\nFeb 09 2007\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\nThe dtypes property is used to find the data types associated with each column in the DataFrame.\nCode\nmovie_ratings.dtypes\n\n\nTitle                 object\nUS Gross               int64\nWorldwide Gross        int64\nProduction Budget      int64\nRelease Date          object\nMPAA Rating           object\nSource                object\nMajor Genre           object\nCreative Type         object\nIMDB Rating          float64\nIMDB Votes             int64\ndtype: object",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More on Pandas</span>"
    ]
  },
  {
    "objectID": "data_types_in_pandas.html#types-of-data",
    "href": "data_types_in_pandas.html#types-of-data",
    "title": "8  More on Pandas",
    "section": "",
    "text": "8.1.1 Available Data Types and Associated Built-in Functions\nIn a DataFrame, columns can have different data types. Here are the common data types you’ll encounter and some built-in functions associated with each type:\n\nNumerical Data (int, float)\n\nBuilt-in functions: mean(), sum(), min(), max(), std(), median(), quantile(), etc.\n\nObject Data (str or mixed types)\n\nBuilt-in functions: str.contains(), str.startswith(), str.endswith(), str.lower(), str.upper(), str.replace(), etc.\n\nDatetime Data (dates)\n\nBuilt-in functions: dt.year, dt.month, dt.day, dt.strftime(), dt.weekday(), dt.hour, etc.\n\n\nThese functions help in exploring and transforming the data effectively depending on the type of data in each column.\n\n\n8.1.2 Data Type Filtering\nWe can filter the columns based on its data types\n\n\nCode\n\n# select just categorical(object) columns\nmovie_ratings.select_dtypes(include='object').head()\n\n\n\n\n\n\n\n\n\nTitle\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\n\n\n\n\n0\nOpal Dreams\nNov 22 2006\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n\n\n1\nMajor Dundee\nApr 07 1965\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n\n\n2\nThe Informers\nApr 24 2009\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n\n\n3\nBuffalo Soldiers\nJul 25 2003\nR\nAdapted screenplay\nComedy\nFiction\n\n\n4\nThe Last Sin Eater\nFeb 09 2007\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n\n\n\n\n\n\n\n\n\nCode\n# select the numeric columns\nmovie_ratings.select_dtypes(include='number').head()\n\n\n\n\n\n\n\n\n\nUS Gross\nWorldwide Gross\nProduction Budget\nIMDB Rating\nIMDB Votes\n\n\n\n\n0\n14443\n14443\n9000000\n6.5\n468\n\n\n1\n14873\n14873\n3800000\n6.7\n2588\n\n\n2\n315000\n315000\n18000000\n5.2\n7595\n\n\n3\n353743\n353743\n15000000\n6.9\n13510\n\n\n4\n388390\n388390\n2200000\n5.7\n1012\n\n\n\n\n\n\n\n\n\n8.1.3 Data Type Conversion\nOften, after the inital reading, we need to convert the datatypes of some of the columns to make them suitable for analysis,as the available functions and operations depend on the column’s data type. For example, the datatype of Release Date in the DataFrame movie_ratings is object. To perform datetime related computations on this variable, we’ll need to convert it to a datatime format. We’ll use the Pandas function to_datatime() to covert it to a datatime format. Similar functions such as to_numeric(), to_string() etc., can be used for other conversions.\nIn Pandas, the errors='coerce' parameter is often used in the context of data conversion, specifically when using the pd.to_numeric function. This argument tells Pandas to convert values that it can and set the ones it cannot convert to NaN. It’s a way of gracefully handling errors without raising an exception. Read the textbook for an example\n\n\nCode\n# check the datatype of release data column \nprint(movie_ratings['Release Date'].dtypes)\nmovie_ratings['Release Date'].head()\n\n\nobject\n\n\n0    Nov 22 2006\n1    Apr 07 1965\n2    Apr 24 2009\n3    Jul 25 2003\n4    Feb 09 2007\nName: Release Date, dtype: object\n\n\nNext, we’ll convert the Release Date column in the DataFrame to the datetime format to facilitate further analysis.\n\n\nCode\nmovie_ratings['Release Date'] = pd.to_datetime(movie_ratings['Release Date'])\n\n# Let's check the datatype of release data column again\nmovie_ratings['Release Date'].dtypes\n\n\ndtype('&lt;M8[ns]')\n\n\ndtype('&lt;M8[ns]') means a 64-bit datetime object with nanosecond precision stored in little-endian format. This data type is commonly used to represent timestamps in high-resolution time series data.\n\n\n8.1.4 Working with datatime Data\n\n8.1.4.1 the dt accessor\nThe .dt accessor is a powerful tool in pandas that allows you to extract and manipulate components of datetime columns in a DataFrame. This is useful for analysis and feature engineering when dealing with time-related data.\nYou can extract various parts of a datetime column using .dt\n\n\n\n\n\n\n\n\nAttribute\nDescription\nExample\n\n\n\n\n.dt.year\nExtracts the year\ndf['Release Date'].dt.year\n\n\n.dt.month\nExtracts the month (1-12)\ndf['Release Date'].dt.month\n\n\n.dt.day\nExtracts the day of the month (1-31)\ndf['Release Date'].dt.day\n\n\n.dt.hour\nExtracts the hour (0-23)\ndf['Release Date'].dt.hour\n\n\n.dt.minute\nExtracts the minute\ndf['Release Date'].dt.minute\n\n\n.dt.second\nExtracts the second\ndf['Release Date'].dt.second\n\n\n.dt.weekday\nExtracts the day of the week (0=Monday, 6=Sunday)\ndf['Release Date'].dt.weekday\n\n\n.dt.dayofyear\nExtracts the day of the year (1-366)\ndf['Release Date'].dt.dayofyear\n\n\n.dt.is_leap_year\nChecks if the year is a leap year\ndf['Release Date'].dt.is_leap_year\n\n\n\nLet’s add the year for the movie_ratings dataframe next\n\n\nCode\n# Extracting the year from the release date\nmovie_ratings['Release Year'] = movie_ratings['Release Date'].dt.year\nmovie_ratings.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\nRelease Year\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\n2006-11-22\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n2006\n\n\n1\nMajor Dundee\n14873\n14873\n3800000\n1965-04-07\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n1965\n\n\n2\nThe Informers\n315000\n315000\n18000000\n2009-04-24\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n2009\n\n\n3\nBuffalo Soldiers\n353743\n353743\n15000000\n2003-07-25\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n2003\n\n\n4\nThe Last Sin Eater\n388390\n388390\n2200000\n2007-02-09\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\n2007\n\n\n\n\n\n\n\nIn your pandas dataframe, if having start date and end date, your can calculate the time duration between them like below\n\n\nCode\n# let's calculate the days since release till Jan 1st 2024\nmovie_ratings['Days Since Release'] = (pd.Timestamp('2024-01-01') - movie_ratings['Release Date']).dt.days\nmovie_ratings.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\nDays Since Release\n\n\n\n\n0\nOpal Dreams\n14443\n14443\n9000000\n2006-11-22\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n6.5\n468\n6249\n\n\n1\nMajor Dundee\n14873\n14873\n3800000\n1965-04-07\nPG/PG-13\nAdapted screenplay\nWestern/Musical\nFiction\n6.7\n2588\n21453\n\n\n2\nThe Informers\n315000\n315000\n18000000\n2009-04-24\nR\nAdapted screenplay\nHorror/Thriller\nFiction\n5.2\n7595\n5365\n\n\n3\nBuffalo Soldiers\n353743\n353743\n15000000\n2003-07-25\nR\nAdapted screenplay\nComedy\nFiction\n6.9\n13510\n7465\n\n\n4\nThe Last Sin Eater\n388390\n388390\n2200000\n2007-02-09\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.7\n1012\n6170\n\n\n\n\n\n\n\nFiltering Data: Use extracted datetime components to filter rows. Aggregation and grouping using datetime components will be covered in future chapters.\n\n\nCode\n# Filter rows where the release month is January\njanuary_releases = movie_ratings[movie_ratings['Release Date'].dt.month == 1]\njanuary_releases.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS Gross\nWorldwide Gross\nProduction Budget\nRelease Date\nMPAA Rating\nSource\nMajor Genre\nCreative Type\nIMDB Rating\nIMDB Votes\nRelease Year\nDays Since Release\n\n\n\n\n15\nThr3e\n1008849\n1060418\n2400000\n2007-01-05\nPG/PG-13\nAdapted screenplay\nHorror/Thriller\nFiction\n5.0\n2825\n2007\n6205\n\n\n57\nImpostor\n6114237\n6114237\n40000000\n2002-01-04\nPG/PG-13\nAdapted screenplay\nHorror/Thriller\nFiction\n6.0\n9020\n2002\n8032\n\n\n62\nThe Last Station\n6616974\n6616974\n18000000\n2010-01-15\nR\nAdapted screenplay\nDrama\nNon-Fiction\n7.0\n3465\n2010\n5099\n\n\n63\nThe Big Bounce\n6471394\n6626115\n50000000\n2004-01-30\nPG/PG-13\nAdapted screenplay\nComedy\nFiction\n4.8\n9195\n2004\n7276\n\n\n84\nNot Easily Broken\n10572742\n10572742\n5000000\n2009-01-09\nPG/PG-13\nAdapted screenplay\nDrama\nFiction\n5.2\n1010\n2009\n5470\n\n\n\n\n\n\n\n\n\n\n8.1.5 Working with object Data\nIn pandas, the object data type is a flexible data type that can store a mix of text (strings), mixed types, or arbitrary Python objects. It is commonly used for string data and is a key part of working with categorical or unstructured data in pandas.\nSimilar to datetime objects having a dt accessor, a number of specialized string methods are available when using the str accessor. These methods have in general matching names with the equivalent built-in string methods for single elements, but are applied element-wise on each of the values of the columns.\n\n8.1.5.1 the str accessor in pandas\nThe str accessor in pandas provides a wide range of string methods that allow for efficient and convenient text processing on an entire Series of strings. Here are some commonly used str methods:\n\nString splitting: str.split()\nString joining: str.join()\nSubstrings: str.slice(start, stop), str[0]\nString Case Conversion: str.lower(), str.upper(), str.capitalize()\nWhitespace Removal: str.strip(), str.lstrip(), str.rstrip()\nReplacing and Removing: str.replace('old', 'new')\nPattern matching and extraction: str.contains('pattern'), startswith('prefix'), endswith('suffix')\nString length and counting: str.len(), str.count()\n\nLet’s use the well-known titanic dataset to illustrate how to manipulate string columns in pandas dataframe next\n\n\nCode\ntitanic = pd.read_csv('./Datasets/titanic.csv')\ntitanic.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n\nCode\ntitanic.Name\n\n\n0                                Braund, Mr. Owen Harris\n1      Cumings, Mrs. John Bradley (Florence Briggs Th...\n2                                 Heikkinen, Miss. Laina\n3           Futrelle, Mrs. Jacques Heath (Lily May Peel)\n4                               Allen, Mr. William Henry\n                             ...                        \n886                                Montvila, Rev. Juozas\n887                         Graham, Miss. Margaret Edith\n888             Johnston, Miss. Catherine Helen \"Carrie\"\n889                                Behr, Mr. Karl Howell\n890                                  Dooley, Mr. Patrick\nName: Name, Length: 891, dtype: object\n\n\nThe Name column varies in length and contains passengers’ last names, titles, and first names. By extracting the title from the Name column, we could infer the sex of the passenger and add it as a new feature. This could potentially be a significant predictor of survival, as the “ladies first” principle was often applied during the Titanic evacuation. Adding this feature may enhance the model’s ability to predict whether a passenger survived.\n\n\nCode\n# Let's check the length of the name of each passenger\ntitanic[\"Name\"].str.len()\n\n\n0      23\n1      51\n2      22\n3      44\n4      24\n       ..\n886    21\n887    28\n888    40\n889    21\n890    19\nName: Name, Length: 891, dtype: int64\n\n\n\n\nCode\n# check what is the maximum length of the name\ntitanic.loc[titanic[\"Name\"].str.len().idxmax(), \"Name\"]\n\n\n'Penasco y Castellana, Mrs. Victor de Satode (Maria Josefa Perez de Soto y Vallejo)'\n\n\n\n\nCode\n# get the observations that contains the word 'Mrs'\ntitanic[titanic.Name.str.contains('Mrs.')]\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n8\n9\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n10\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n\n\n15\n16\n1\n2\nHewlett, Mrs. (Mary D Kingcome)\n55.0\n0\n0\n248706\n16.0000\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n871\n872\n1\n1\nBeckwith, Mrs. Richard Leonard (Sallie Monypeny)\n47.0\n1\n1\n11751\n52.5542\nD35\nS\n\n\n874\n875\n1\n2\nAbelson, Mrs. Samuel (Hannah Wizosky)\n28.0\n1\n0\nP/PP 3381\n24.0000\nNaN\nC\n\n\n879\n880\n1\n1\nPotter, Mrs. Thomas Jr (Lily Alexenia Wilson)\n56.0\n0\n1\n11767\n83.1583\nC50\nC\n\n\n880\n881\n1\n2\nShelley, Mrs. William (Imanita Parrish Hall)\n25.0\n0\n1\n230433\n26.0000\nNaN\nS\n\n\n885\n886\n0\n3\nRice, Mrs. William (Margaret Norton)\n39.0\n0\n5\n382652\n29.1250\nNaN\nQ\n\n\n\n\n129 rows × 11 columns\n\n\n\n\n\nCode\n# get the observations that contains the word 'Mrs'\ntitanic.Name.str.count('Mrs.').sum()\n\n\n129\n\n\n\n\nCode\n# split the name column into two columns\ntitanic.Name.str.split(',', expand=True)\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nBraund\nMr. Owen Harris\n\n\n1\nCumings\nMrs. John Bradley (Florence Briggs Thayer)\n\n\n2\nHeikkinen\nMiss. Laina\n\n\n3\nFutrelle\nMrs. Jacques Heath (Lily May Peel)\n\n\n4\nAllen\nMr. William Henry\n\n\n...\n...\n...\n\n\n886\nMontvila\nRev. Juozas\n\n\n887\nGraham\nMiss. Margaret Edith\n\n\n888\nJohnston\nMiss. Catherine Helen \"Carrie\"\n\n\n889\nBehr\nMr. Karl Howell\n\n\n890\nDooley\nMr. Patrick\n\n\n\n\n891 rows × 2 columns\n\n\n\n\n\nCode\n# get the last part of the split\ntitanic.Name.str.split(',', expand=True).get(1)\n\n\n0                                  Mr. Owen Harris\n1       Mrs. John Bradley (Florence Briggs Thayer)\n2                                      Miss. Laina\n3               Mrs. Jacques Heath (Lily May Peel)\n4                                Mr. William Henry\n                          ...                     \n886                                    Rev. Juozas\n887                           Miss. Margaret Edith\n888                 Miss. Catherine Helen \"Carrie\"\n889                                Mr. Karl Howell\n890                                    Mr. Patrick\nName: 1, Length: 891, dtype: object\n\n\nCreate a new column Title that contains the title of the passengers\n\n\nCode\n# from the name, extract the title\ntitanic['Title'] = titanic.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntitanic['Title']\n\n\n0        Mr\n1       Mrs\n2      Miss\n3       Mrs\n4        Mr\n       ... \n886     Rev\n887    Miss\n888    Miss\n889      Mr\n890      Mr\nName: Title, Length: 891, dtype: object\n\n\n\n\nCode\n# get the unique titles\ntitanic.Title.unique()\n\n\narray(['Mr', 'Mrs', 'Miss', 'Master', 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess',\n       'Jonkheer'], dtype=object)\n\n\nLet’s create a mapping dictionary next\n\n\nCode\ntitle_sex_mapping = {\n    'Mr': 'Male',\n    'Mrs': 'Female',\n    'Miss': 'Female',\n    'Master': 'Male',\n    'Don': 'Male',\n    'Rev': 'Male',\n    'Dr': 'Male',  # Assumed to be Male unless you have additional context\n    'Mme': 'Female',\n    'Ms': 'Female',\n    'Major': 'Male',\n    'Lady': 'Female',\n    'Sir': 'Male',\n    'Mlle': 'Female',\n    'Col': 'Male',\n    'Capt': 'Male',\n    'the Countess': 'Female',\n    'Jonkheer': 'Male'\n}\n\n\n\n\nCode\ntitanic['Sex'] = titanic['Title'].map(title_sex_mapping)\ntitanic.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nTitle\nSex\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\nMr\nMale\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\nMrs\nFemale\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\nMiss\nFemale\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\nMrs\nFemale\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\nMr\nMale\n\n\n\n\n\n\n\n\n\nCode\ntitanic.Sex.value_counts()\n\n\nSex\nMale      578\nFemale    313\nName: count, dtype: int64\n\n\n\n\nCode\n# cross tabulation\npd.crosstab(titanic.Survived, titanic.Sex)\n\n\n\n\n\n\n\n\nSex\nFemale\nMale\n\n\nSurvived\n\n\n\n\n\n\n0\n81\n468\n\n\n1\n232\n110\n\n\n\n\n\n\n\nIn the Titanic disaster, more women survived than men due to the social norms and evacuation protocols followed during the sinking. The principle of “women and children first” was enforced when lifeboats were being loaded. Since there were not enough lifeboats for everyone on board, priority was given to women and children, which contributed to the higher survival rate among females compared to males.\n\n\n8.1.5.2 the re module in Python is used for regular expression\nThe re module in Python is used for regular expressions, which are powerful tools for text analysis and manipulation. Regular expressions allow you to search, match, and manipulate strings based on specific patterns.\nCommon Use Cases of re in String Text Analysis\n\nFinding patterns in text: re.search(r'\\d{4}-\\d{2}-\\d{2}', text) # searches for a date in the format YYYY-MM-DD\nExtracting Specific parts of a string: re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text) #extracts email addresses from the text.\nReplacing Parts of a String: re.sub(r'\\$\\d+', '[price]', text) #replacing any price formatted as $ with [price].\n\n\n8.1.5.2.1 Commonly Used Patterns in re\n\n\\d: Matches any digit (0-9).\n\\w: Matches any alphanumeric character (letters and numbers).\n\\s: Matches any whitespace character (spaces, tabs, newlines).\n[a-z]: Matches any lowercase letter from a to z.\n[A-Z]: Matches any uppercase letter from A to Z.\n*: Matches 0 or more occurrences of the preceding character.\n+: Matches 1 or more occurrences of the preceding character.\n?: Matches 0 or 1 occurrence of the preceding character.\n^: Matches the beginning of a string.\n$: Matches the end of a string.\n|: Acts as an OR operator.\n\n\n\nCode\ndata = pd.read_html('https://en.wikipedia.org/wiki/List_of_Chicago_Bulls_seasons')\nChicagoBulls = data[2]\nChicagoBulls.head()\n\n\n\n\n\n\n\n\n\nSeason\nTeam\nConference\nFinish\nDivision\nFinish.1\nWins\nLosses\nWin%\nGB\nPlayoffs\nAwards\nHead coach\n\n\n\n\n0\n1966–67\n1966–67\n—\n—\nWestern\n4th\n33\n48\n0.407\n11\nLost Division semifinals (Hawks) 3–0[19]\nJohnny Kerr (COY)[6]\nJohnny Kerr\n\n\n1\n1967–68\n1967–68\n—\n—\nWestern\n4th\n29\n53\n0.354\n27\nLost Division semifinals (Lakers) 4–1[20]\n—\nJohnny Kerr\n\n\n2\n1968–69\n1968–69\n—\n—\nWestern\n5th\n33\n49\n0.402\n22\nNaN\n—\nDick Motta\n\n\n3\n1969–70\n1969–70\n—\n—\nWestern\n3rd[c]\n39\n43\n0.476\n9\nLost Division semifinals (Hawks) 4–1[22]\n—\nDick Motta\n\n\n4\n1970–71\n1970–71\nWestern\n3rd\nMidwest[d]\n2nd\n51\n31\n0.622\n2\nLost conference semifinals (Lakers) 4–3[23]\nDick Motta (COY)[6]\nDick Motta\n\n\n\n\n\n\n\n\n\nCode\n# remove all charaters between box brackets inluding the brackets themselves in the columns Division, Finish, Playoffs, Awards\nimport re\ndef remove_brackets(x):\n    return re.sub(r'\\[.*?\\]', '', x)\n\n# Apply the function to each column separately using map\nChicagoBulls['Division'] = ChicagoBulls['Division'].map(remove_brackets)\nChicagoBulls['Finish'] = ChicagoBulls['Finish'].map(remove_brackets)\nChicagoBulls['Finish.1'] = ChicagoBulls['Finish.1'].map(remove_brackets)\n\nChicagoBulls.head()\n\n\n\n\n\n\n\n\n\nSeason\nTeam\nConference\nFinish\nDivision\nFinish.1\nWins\nLosses\nWin%\nGB\nPlayoffs\nAwards\nHead coach\n\n\n\n\n0\n1966–67\n1966–67\n—\n—\nWestern\n4th\n33\n48\n0.407\n11\nLost Division semifinals (Hawks) 3–0[19]\nJohnny Kerr (COY)[6]\nJohnny Kerr\n\n\n1\n1967–68\n1967–68\n—\n—\nWestern\n4th\n29\n53\n0.354\n27\nLost Division semifinals (Lakers) 4–1[20]\n—\nJohnny Kerr\n\n\n2\n1968–69\n1968–69\n—\n—\nWestern\n5th\n33\n49\n0.402\n22\nNaN\n—\nDick Motta\n\n\n3\n1969–70\n1969–70\n—\n—\nWestern\n3rd\n39\n43\n0.476\n9\nLost Division semifinals (Hawks) 4–1[22]\n—\nDick Motta\n\n\n4\n1970–71\n1970–71\nWestern\n3rd\nMidwest\n2nd\n51\n31\n0.622\n2\nLost conference semifinals (Lakers) 4–3[23]\nDick Motta (COY)[6]\nDick Motta\n\n\n\n\n\n\n\nAnother example\n\n\nCode\n\ngdp_data = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita\")[1]\n\ngdp_data.head()\n\n\n\n\n\n\n\n\n\nCountry/Territory\nIMF[4][5]\nWorld Bank[6]\nUnited Nations[7]\n\n\n\nCountry/Territory\nEstimate\nYear\nEstimate\nYear\nEstimate\nYear\n\n\n\n\n0\nMonaco\n—\n—\n240862\n2022\n240535\n2022\n\n\n1\nLiechtenstein\n—\n—\n187267\n2022\n197268\n2022\n\n\n2\nLuxembourg\n135321\n2024\n128259\n2023\n125897\n2022\n\n\n3\nBermuda\n—\n—\n123091\n2022\n117568\n2022\n\n\n4\nSwitzerland\n106098\n2024\n99995\n2023\n93636\n2022\n\n\n\n\n\n\n\n\n\nCode\n# drop the year column\ngdp_data.drop(columns=[\"Year\"], level=1, inplace=True, axis=1)\n\n# drop the level 1 column\ngdp_data = gdp_data.droplevel(1, axis=1)\n\n\n\n\nCode\n\ncolumn_name_cleaner = lambda x:re.split(r'\\[', x)[0]\n\ngdp_data.columns = gdp_data.columns.map(column_name_cleaner)\n\ngdp_data.head()\n\n\n\n\n\n\n\n\n\nCountry/Territory\nIMF\nWorld Bank\nUnited Nations\n\n\n\n\n0\nMonaco\n—\n240862\n240535\n\n\n1\nLiechtenstein\n—\n187267\n197268\n\n\n2\nLuxembourg\n135321\n128259\n125897\n\n\n3\nBermuda\n—\n123091\n117568\n\n\n4\nSwitzerland\n106098\n99995\n93636\n\n\n\n\n\n\n\n\n\n\n8.1.5.3 the NLTK Library for NLP (skipped)\nNLTK (Natural Language Toolkit) is a popular Python library for natural language processing (NLP) and text analysis. It provides a wide range of tools and resources for processing and analyzing human language data. NLTK is widely used in research, education, and industry for various text analysis tasks.\n\n8.1.5.3.1 Key Features and Capabilities of NLTK\n\nTokenization: Splits text into individual words (word tokenization) or sentences (sentence tokenization).\nStop Word Removal: Provides lists of common words (like “and”, “the”, “is”) in various languages that can be removed from text to reduce noise.\nStemming: Reduces words to their root form (e.g., “running” to “run”).\nLemmatization: Similar to stemming, but more sophisticated. It reduces words to their dictionary form using vocabulary and morphological analysis (e.g., “better” to “good”).\n\n\n\n\n\n8.1.6 Working with numerical Data\n\n8.1.6.1 Summary statistics across rows/columns in Pandas\nThe Pandas DataFrame class has functions such as sum() and mean() to compute sum over rows or columns of a DataFrame.\nBy default, functions like mean() and sum() compute the statistics for each column (i.e., all rows are aggregated) in the DataFrame. Let us compute the mean of all the numeric columns of the data:\n\n\nCode\nmovie_ratings.describe()\n\n\n\n\n\n\n\n\n\nUS Gross\nWorldwide Gross\nProduction Budget\nIMDB Rating\nIMDB Votes\n\n\n\n\ncount\n2.228000e+03\n2.228000e+03\n2.228000e+03\n2228.000000\n2228.000000\n\n\nmean\n5.076370e+07\n1.019370e+08\n3.816055e+07\n6.239004\n33585.154847\n\n\nstd\n6.643081e+07\n1.648589e+08\n3.782604e+07\n1.243285\n47325.651561\n\n\nmin\n0.000000e+00\n8.840000e+02\n2.180000e+02\n1.400000\n18.000000\n\n\n25%\n9.646188e+06\n1.320737e+07\n1.200000e+07\n5.500000\n6659.250000\n\n\n50%\n2.838649e+07\n4.266892e+07\n2.600000e+07\n6.400000\n18169.000000\n\n\n75%\n6.453140e+07\n1.200000e+08\n5.300000e+07\n7.100000\n40092.750000\n\n\nmax\n7.601676e+08\n2.767891e+09\n3.000000e+08\n9.200000\n519541.000000\n\n\n\n\n\n\n\n\n\nCode\n# select the numeric columns\nmovie_ratings.mean(numeric_only=True)\n\n\nUS Gross                  5.076370e+07\nWorldwide Gross           1.019370e+08\nProduction Budget         3.816055e+07\nIMDB Rating               6.239004e+00\nIMDB Votes                3.358515e+04\nRelease Year              2.002005e+03\nratio_wgross_by_budget    1.259483e+01\ndtype: float64\n\n\nUsing the axis parameter:\nThe axis parameter controls whether to compute the statistic across rows or columns: * The argument axis=0(deafult) denotes that the mean is taken over all the rows of the DataFrame. * For computing a statistic across column the argument axis=1 will be used.\nIf mean over a subset of columns is desired, then those column names can be subset from the data.\nFor example, let us compute the mean IMDB rating, and mean IMDB votes of all the movies:\n\n\nCode\nmovie_ratings[['IMDB Rating','IMDB Votes']].mean(axis = 0)\n\n\nIMDB Rating        6.239004\nIMDB Votes     33585.154847\ndtype: float64\n\n\nPandas sum function\n\n\nCode\ndata = [[10, 18, 11], [13, 15, 8], [9, 20, 3]]\ndf = pd.DataFrame(data )\ndf\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n10\n18\n11\n\n\n1\n13\n15\n8\n\n\n2\n9\n20\n3\n\n\n\n\n\n\n\n\n\nCode\n# By default, the sum method adds values accross rows and returns the sum for each column\ndf.sum()\n\n\n0    32\n1    53\n2    22\ndtype: int64\n\n\n\n\nCode\n# By specifying the column axis (axis='columns'), the sum() method add values accross columns and returns the sum of each row.\ndf.sum(axis = 'columns')\n\n\n0    39\n1    36\n2    32\ndtype: int64\n\n\n\n\nCode\n# in python, axis=1 stands for column, while axis=0 stands for rows\ndf.sum(axis = 1)\n\n\n0    39\n1    36\n2    32\ndtype: int64",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More on Pandas</span>"
    ]
  },
  {
    "objectID": "Data visualization.html",
    "href": "Data visualization.html",
    "title": "9  Introduction to Data Visualization",
    "section": "",
    "text": "9.1 The Art of Visualization: Choosing the Right Plot Type\n“One picture is worth a thousand words” - Fred R. Barnard\nVisual perception offers the highest bandwidth channel, as we acquire much more information through visual perception than with all of the other channels combined, as billions of our neurons are dedicated to this task. Moreover, the processing of visual information is, at its first stages, a highly parallel process. Thus, it is generally easier for humans to comprehend information with plots, diagrams and pictures, rather than with text and numbers. This makes data visualizations a vital part of data science. Some of the key purposes of data visualization are:\nThere are various types of plots available, and selecting the appropriate one is crucial for successful data visualization. The choice primarily depends on two factors: * The type of data you are working with, and * The role of visualization in your data analysis",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "Data visualization.html#the-art-of-visualization-choosing-the-right-plot-type",
    "href": "Data visualization.html#the-art-of-visualization-choosing-the-right-plot-type",
    "title": "9  Introduction to Data Visualization",
    "section": "",
    "text": "9.1.1 Data Classification for Visualization\nData visualization is commonly used to plot data in a pandas DataFrame. The data can be classified into two categories:\n\nNumeric Data: This type of data represents quantities and can take any value within a range. Common examples include age, height, temperature, etc.\nCategorical Data: This type of data represents distinct categories or groups. It can be nominal (no inherent order, like colors or names) or ordinal (with a defined order, like ratings).\n\n\n\n9.1.2 The Role of Visualization in Data Analysis\nData visualization is essential for effectively communicating insights derived from data analysis. By using various visualization techniques, we can uncover patterns, and understand relationships. Below, we discuss different types of data exploration and the relevant visualizations used for each.\n\n9.1.2.1 Univariate Exploration\nPurpose: Univariate exploration analyzes a single variable to understand its distribution, central tendency, and spread.\n\n9.1.2.1.1 Common Visualizations:\n\nHistograms: Display the frequency distribution of a numeric variable, helping to identify the shape of the data (e.g., normal, skewed).\nBox Plots: Summarize key statistics of a variable, including median, quartiles, and potential outliers.\nBar Plots: Show the count or proportion of categorical variables, revealing the frequency of each category.\nLine Plots: Used to display trends in numeric data over time, helping to visualize changes in a variable.\n\n\n\n9.1.2.1.2 Insights Gained:\n\nIdentify outliers and anomalies.\nUnderstand the range and distribution of values.\nDetermine central tendency (mean, median, mode).\n\n\n\n\n9.1.2.2 Bivariate Analysis\nPurpose: Bivariate analysis examines the relationship between two variables, helping to understand how changes in one variable might affect another.\n\n9.1.2.2.1 Common Visualizations:\n\nScatter Plots: Illustrate the relationship between two numeric variables, highlighting trends and correlations.\nGrouped Bar Plots: Compare categorical variables against a numeric variable, revealing trends across categories.\nHeatmaps: Represent correlation coefficients between pairs of variables, allowing easy identification of strong correlations.\n\n\n\n9.1.2.2.2 Insights Gained:\n\nAssess the strength and direction of relationships (positive, negative, or no correlation).\nIdentify potential predictive relationships for further analysis.\nDiscover patterns that may indicate causal relationships.\n\n\n\n\n9.1.2.3 Multivariate Analysis\nPurpose: Multivariate analysis investigates more than two variables simultaneously, providing a comprehensive view of complex relationships and interactions.\n\n9.1.2.3.1 Common Visualizations:\n\nPair Plots: Show pairwise relationships in a dataset, facilitating quick insights into correlations among multiple variables.\n3D Scatter Plots: Visualize the interaction between three numeric variables in a three-dimensional space.\nFacet Grids: Display multiple plots for different subsets of data, enabling comparisons across categories.\n\n\n\n9.1.2.3.2 Insights Gained:\n\nUnderstand interactions and dependencies among multiple variables.\nIdentify clusters or groups within the data.\nEnhance predictive modeling by considering multiple influences.\n\n\n\n\n\n9.1.3 Summary\nChoosing the appropriate plot depends on the data type and the specific analysis purpose. Numeric data typically requires plots that can handle continuous data (like line plots or histograms), while categorical data often benefits from comparisons (like bar plots or pie charts). Always consider what story you want to tell with your data and select your visualization method accordingly.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "Data visualization.html#visulization-tools",
    "href": "Data visualization.html#visulization-tools",
    "title": "9  Introduction to Data Visualization",
    "section": "9.2 Visulization Tools",
    "text": "9.2 Visulization Tools\nWe’ll use three libraries for making data visualizations - pandas, matplotlib, and seaborn.\nTo get started, let’s import these libraries.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# let's import numpy as well\nimport numpy as np\n\n\n9.2.1 Basic Plotting with Pandas\nIn previous chapters, we focused on using the pandas library for data reading and analysis. In addition to its powerful data manipulation capabilities, pandas also provides tools for creating basic plots, making it especially valuable for exploratory data analysis.\nIn this section, we will use the COVID dataset to demonstrate basic plotting techniques with pandas.\n\ncovid_df = pd.read_csv('./Datasets/covid.csv')\ncovid_df.head(5)\n\n\n\n\n\n\n\n\ndate\nnew_cases\ntotal_cases\nnew_deaths\ntotal_deaths\nnew_tests\ntotal_tests\ncases_per_million\ndeaths_per_million\ntests_per_million\n\n\n\n\n0\n2019-12-31\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\n0.0\n0.0\nNaN\n\n\n1\n2020-01-01\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\n0.0\n0.0\nNaN\n\n\n2\n2020-01-02\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\n0.0\n0.0\nNaN\n\n\n3\n2020-01-03\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\n0.0\n0.0\nNaN\n\n\n4\n2020-01-04\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\n0.0\n0.0\nNaN\n\n\n\n\n\n\n\nLet’s begin by visualizing the trend of new COVID cases over time using a line plot.\nA line plot is ideal for showing changes over continuous data, such as the progression of new cases over a series of dates.\n\ncovid_df.new_cases.plot()\n\n\n\n\n\n\n\n\nWhile this plot shows the overall trend, it’s hard to tell where the peak occurred, as there are no dates on the X-axis. We can use the date column as the index for the data frame to address this issue since it is a time series dataset\n\ncovid_df.set_index('date', inplace=True)\n\n\ncovid_df.new_cases.plot(rot=45)\n\n\n\n\n\n\n\n\nWith the date set as the index, we can observe that the peak occurred around March 2020. Next, let’s plot both new cases and new deaths together to compare their trends over the same time period.\n\ncovid_df[['new_deaths', 'new_cases']].plot()\n\n\n\n\n\n\n\n\nBy default, pandas generates line plots when using the .plot method. However, there are several parameters you can adjust to enhance the appearance of the line plot.\n\ncovid_df[['new_deaths', 'new_cases']].plot(figsize=(12, 6), linewidth=2, marker='o')\n\n\n\n\n\n\n\n\nYou can create other types of visualizations by setting the kind parameter in the plot function. The kind parameter accepts eleven different string values, which specify the type of plot:\n\n“area” is for area plots.\n“bar” is for vertical bar charts.\n“barh” is for horizontal bar charts.\n“box” is for box plots.\n“hexbin” is for hexbin plots.\n“hist” is for histograms.\n“kde” is for kernel density estimate charts.\n“density” is an alias for “kde”.\n“line” is for line graphs.\n“pie” is for pie charts.\n“scatter” is for scatter plots.\n\nLet’s next create a scatter plot to visualize the relationship between new cases and new deaths, and explore whether there’s a correlation between them.\n\ncovid_df.plot(kind='scatter', x='new_cases', y='new_deaths', color='r', alpha=0.5);\n\n\n\n\n\n\n\n\nNext, let’s examine the distribution of new deaths using a histogram.\n\ncovid_df.new_deaths.plot(kind='hist', color='r', alpha=0.5, bins=50);\n\n\n\n\n\n\n\n\n\ncovid_df.new_cases.plot(kind='box');\n\n\n\n\n\n\n\n\nFor more plot types and detailed information, refer to the official pandas documentation:\n\nSeries.plot\nDataFrame.plot\n\n\n9.2.1.1 Limitation of using pandas for plotting\nWhile pandas provides a straightforward way to create plots, there are some limitations to be aware of:\n\nCustomization: Pandas offers basic customization options, but it may not provide the level of detail or flexibility that you can achieve with matplotlib directly. For complex visualizations, you might need to switch to matplotlib for more control.\nPlot Types: The range of plot types available in pandas is limited compared to what you can create with matplotlib or seaborn. For instance, advanced plots like violin plots or 3D plots require switching to other libraries.\nAesthetic Choices: The default aesthetics in pandas may not be as visually appealing as those created using seaborn or other specialized visualization libraries. For polished presentations, additional customization might be necessary.\n\n\n\n\n9.2.2 Data Plotting with Matplotlib Pyplot Interface\nPandas data visualization is built on top of matplotlib. When you use the .plot() method in pandas, it internally calls matplotlib functions to create the plots.\nMatplotlib is:\n\na low-level graph plotting library in python that strives to emulate MATLAB,\ncan be used in Python scripts, Python and IPython shells, Jupyter notebooks and web application servers.\nis mostly written in python, a few segments are written in C, Objective-C and Javascript for Platform compatibility.\n\n\n9.2.2.1 Matplotlib pyplot\n\nMatplotlib is the whole package; pyplot is a module in matplotlib\nMost of the Matplotlib utilities lies under the pyplot module, and are usually imported under the plt alias:\n\n\nimport matplotlib.pyplot as plt\n\n\n\n9.2.2.2 Data Source\n\nPython lists, NumPy arrays as well as a pandas series\nHowever, all the sequences are internally converted to numpy arrays.\n\nLet’s create a python list to illustrate basic plotting with Matplotlib pyplot\n\nyield_apples = [0.895, 0.91, 0.919, 0.926, 0.929, 0.931]\n\n\n\n9.2.2.3 Basic Plotting\n\n9.2.2.3.1 Plotting the overall trend\n\nplt.plot(yield_apples)\n\n\n\n\n\n\n\n\nCalling the plt.plot function draws the line chart as expected. It also returns a list of plots drawn [&lt;matplotlib.lines.Line2D at 0x2194b571df0&gt;], shown within the output. We can include a semicolon (;) at the end of the last statement in the cell to avoiding showing the output and display just the graph.\n\nplt.plot(yield_apples);\n\n\n\n\n\n\n\n\n\n\n9.2.2.3.2 Customizing the X-axis\nThe X-axis of the plot currently shows list element indexes 0 to 5. The plot would be more informative if we could display the year for which we’re plotting the data. We can do this by two arguments plt.plot.\n\nyears = [2010, 2011, 2012, 2013, 2014, 2015]\nyield_apples = [0.895, 0.91, 0.919, 0.926, 0.929, 0.931]\n\n\nplt.plot(years, yield_apples);\n\n\n\n\n\n\n\n\n\n\n9.2.2.3.3 Ploting multiple lines\nYou can invoke the plt.plot function once for each line to plot multiple lines in the same graph. Let’s compare the yields of apples vs. oranges.\n\nyears = range(2000, 2012)\napples = [0.895, 0.91, 0.919, 0.926, 0.929, 0.931, 0.934, 0.936, 0.937, 0.9375, 0.9372, 0.939]\noranges = [0.962, 0.941, 0.930, 0.923, 0.918, 0.908, 0.907, 0.904, 0.901, 0.898, 0.9, 0.896, ]\n\n\nplt.plot(years, apples)\nplt.plot(years, oranges);\n\n\n\n\n\n\n\n\nWhen plt.plot command is called without any formatting parameters, pyplot uses the following defaults:\n\nFigure size: 6.4 X 4.8 inches\nPlot style: solid line\nLinewidth: 1.5\nColor: Blue (code ‘b’, hex code: ‘#1f77b4’)\n\nYou can also edit default styles directly by modifying the matplotlib.rcParams dictionary. Learn more: https://matplotlib.org/3.2.1/tutorials/introductory/customizing.html#matplotlib-rcparams .\nYou can customize default plot styles by directly modifying the matplotlib.rcParams dictionary. For more details, visit the official Matplotlib guide on customizing with rcParams.\n\nimport matplotlib\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (7, 4)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'\n\nConceptual model: Plotting in Matplotlib involves multiple levels of control, from setting the figure size to customizing individual text elements. To offer complete control over the plotting process, Matplotlib provides an object-oriented interface in a hierarchical structure. This approach allows users to create and manage Figure and Axes objects, which serve as the foundation for all plotting actions. In the next chapter, you will explore how to use this object-oriented interface to gain more precise control over your plots.\n\n\n\n9.2.2.4 Enhancing the plot\nMatplotlib provides a wide range of customizable components within a figure, allowing for fine-tuned control over every aspect of the plot. These components include elements like axes, labels, ticks, legends, and the overall layout. Each can be tailored to enhance the clarity, aesthetics, and effectiveness of the visual representation, making the plot more engaging and easier to interpret.\n\n\n\n\n\n\n\nFigure 9.1: Matplotlib anatomy of a figure\n\n\n\n\n\n9.2.2.4.1 Adding Axis Lables\nWe can add labels to the axes to show what each axis represents using the plt.xlabel and plt.ylabel methods.\n\nplt.plot(years, apples)\nplt.plot(years, oranges)\nplt.xlabel('Year')\nplt.ylabel('Yield (tons per hectare)');\n\n\n\n\n\n\n\n\n\n\n9.2.2.4.2 Adding Chart Title and Legend\nTo differentiate between multiple lines, we can include a legend within the graph using the plt.legend function. We can also set a title for the chart using the plt.title function.\n\nplt.plot(years, apples)\nplt.plot(years, oranges)\n\nplt.xlabel('Year')\nplt.ylabel('Yield (tons per hectare)')\n\nplt.title(\"Crop Yields in Kanto\")\nplt.legend(['Apples', 'Oranges']);\n\n\n\n\n\n\n\n\n\n\n9.2.2.4.3 Adding Line Markers\nWe can also show markers for the data points on each line using the marker argument of plt.plot. Matplotlib provides many different markers, like a circle, cross, square, diamond, etc. You can find the full list of marker types here.\n\nplt.plot(years, apples, marker='o')\nplt.plot(years, oranges, marker='x')\n\nplt.xlabel('Year')\nplt.ylabel('Yield (tons per hectare)')\n\nplt.title(\"Crop Yields in Kanto\")\nplt.legend(['Apples', 'Oranges']);\n\n\n\n\n\n\n\n\n\n\n9.2.2.4.4 Styling Lines and Markers\nThe plt.plot function supports many arguments for styling lines and markers:\n\ncolor or c: Set the color of the line (supported colors)\nlinestyle or ls: Choose between a solid or dashed line\nlinewidth or lw: Set the width of a line\nmarkersize or ms: Set the size of markers\nmarkeredgecolor or mec: Set the edge color for markers\nmarkeredgewidth or mew: Set the edge width for markers\nmarkerfacecolor or mfc: Set the fill color for markers\nalpha: Opacity of the plot\n\nCheck out the documentation for plt.plot to learn more: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot .\n\nplt.plot(years, apples, marker='s', c='b', ls='-', lw=2, ms=8, mew=2, mec='navy')\nplt.plot(years, oranges, marker='o', c='r', ls='--', lw=3, ms=10, alpha=.5)\n\nplt.xlabel('Year')\nplt.ylabel('Yield (tons per hectare)')\n\nplt.title(\"Crop Yields in Kanto\")\nplt.legend(['Apples', 'Oranges']);\n\n\n\n\n\n\n\n\nThe fmt argument provides a shorthand for specifying the marker shape, line style, and line color. It can be provided as the third argument to plt.plot.\nfmt = '[marker][line][color]'\n\nplt.plot(years, apples, 's-b')\nplt.plot(years, oranges, 'o--r')\n\nplt.xlabel('Year')\nplt.ylabel('Yield (tons per hectare)')\n\nplt.title(\"Crop Yields in Kanto\")\nplt.legend(['Apples', 'Oranges']);\n\n\n\n\n\n\n\n\nIf you don’t specify a line style in fmt, only markers are drawn.\n\nplt.plot(years, apples, 'sb')\nplt.plot(years, oranges, 'or')\nplt.title(\"Yield (tons per hectare)\");\n\n\n\n\n\n\n\n\n\n\n9.2.2.4.5 Changing the Figure Size\nYou can use the plt.figure function to change the size of the figure.\n\nplt.figure(figsize=(6, 4))\n\nplt.plot(years, oranges, 'or')\nplt.title(\"Yield of Oranges (tons per hectare)\");\n\n\n\n\n\n\n\n\n\n\n\n9.2.2.5 Plotting other types of plots with matplotlib pyplot\nLet’s read the fifa_data.csv as our data source\n\nfifa = pd.read_csv('./Datasets/fifa_data.csv')\nfifa.head(5)\n\n\n\n\n\n\n\n\nUnnamed: 0\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\n...\nComposure\nMarking\nStandingTackle\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nRelease Clause\n\n\n\n\n0\n0\n158023\nL. Messi\n31\nhttps://cdn.sofifa.org/players/4/19/158023.png\nArgentina\nhttps://cdn.sofifa.org/flags/52.png\n94\n94\nFC Barcelona\n...\n96.0\n33.0\n28.0\n26.0\n6.0\n11.0\n15.0\n14.0\n8.0\n€226.5M\n\n\n1\n1\n20801\nCristiano Ronaldo\n33\nhttps://cdn.sofifa.org/players/4/19/20801.png\nPortugal\nhttps://cdn.sofifa.org/flags/38.png\n94\n94\nJuventus\n...\n95.0\n28.0\n31.0\n23.0\n7.0\n11.0\n15.0\n14.0\n11.0\n€127.1M\n\n\n2\n2\n190871\nNeymar Jr\n26\nhttps://cdn.sofifa.org/players/4/19/190871.png\nBrazil\nhttps://cdn.sofifa.org/flags/54.png\n92\n93\nParis Saint-Germain\n...\n94.0\n27.0\n24.0\n33.0\n9.0\n9.0\n15.0\n15.0\n11.0\n€228.1M\n\n\n3\n3\n193080\nDe Gea\n27\nhttps://cdn.sofifa.org/players/4/19/193080.png\nSpain\nhttps://cdn.sofifa.org/flags/45.png\n91\n93\nManchester United\n...\n68.0\n15.0\n21.0\n13.0\n90.0\n85.0\n87.0\n88.0\n94.0\n€138.6M\n\n\n4\n4\n192985\nK. De Bruyne\n27\nhttps://cdn.sofifa.org/players/4/19/192985.png\nBelgium\nhttps://cdn.sofifa.org/flags/7.png\n91\n92\nManchester City\n...\n88.0\n68.0\n58.0\n51.0\n15.0\n13.0\n5.0\n10.0\n13.0\n€196.4M\n\n\n\n\n5 rows × 89 columns\n\n\n\n\n9.2.2.5.1 Histgram\n\nplt.figure(figsize=(8,5))\n\nplt.hist(fifa.Overall, color='#abcdef')\n\nplt.ylabel('Number of Players')\nplt.xlabel('Skill Level')\nplt.title('Distribution of Player Skills in FIFA 2018')\n\nText(0.5, 1.0, 'Distribution of Player Skills in FIFA 2018')\n\n\n\n\n\n\n\n\n\n\n\n9.2.2.5.2 Bar chart\n\n# plotting bar chart for the best players\nplt.figure(figsize=(8,5))\n\nfoot_preference = fifa['Preferred Foot'].value_counts()\n\nplt.bar(['Left', 'Right'], [foot_preference.iloc[1], foot_preference.iloc[0]], color='#abcdef')\n\nplt.ylabel('Number of Players')\nplt.title('Foot Preference of FIFA Players');\n\n\n\n\n\n\n\n\n\n\n9.2.2.5.3 Pie chart\n\nleft = fifa.loc[fifa['Preferred Foot'] == 'Left'].count().iloc[0]\nright = fifa.loc[fifa['Preferred Foot'] == 'Right'].count().iloc[0]\n\nplt.figure(figsize=(8,5))\n\nlabels = ['Left', 'Right']\ncolors = ['#abcdef', '#aabbcc']\n\nplt.pie([left, right], labels = labels, colors=colors, autopct='%.2f %%')\n\nplt.title('Foot Preference of FIFA Players');\n\n\n\n\n\n\n\n\nAnother Pie Chart on wight of players\n\nplt.figure(figsize=(8,5), dpi=100)\n\nplt.style.use('ggplot')\n\nfifa.Weight = [int(x.strip('lbs')) if type(x)==str else x for x in fifa.Weight]\n\nlight = fifa.loc[fifa.Weight &lt; 125].count().iloc[0]\nlight_medium = fifa[(fifa.Weight &gt;= 125) & (fifa.Weight &lt; 150)].count().iloc[0]\nmedium = fifa[(fifa.Weight &gt;= 150) & (fifa.Weight &lt; 175)].count().iloc[0]\nmedium_heavy = fifa[(fifa.Weight &gt;= 175) & (fifa.Weight &lt; 200)].count().iloc[0]\nheavy = fifa[fifa.Weight &gt;= 200].count().iloc[0]\n\nweights = [light,light_medium, medium, medium_heavy, heavy]\nlabel = ['under 125', '125-150', '150-175', '175-200', 'over 200']\nexplode = (.4,.2,0,0,.4)\n\nplt.title('Weight of Professional Soccer Players (lbs)')\n\nplt.pie(weights, labels=label, explode=explode, pctdistance=0.8,autopct='%.2f %%');\n\n\n\n\n\n\n\n\n\n\n9.2.2.5.4 Box and Whiskers Chart\n\nplt.figure(figsize=(5,8), dpi=100)\n\nplt.style.use('default')\n\nbarcelona = fifa.loc[fifa.Club == \"FC Barcelona\"]['Overall']\nmadrid = fifa.loc[fifa.Club == \"Real Madrid\"]['Overall']\nrevs = fifa.loc[fifa.Club == \"New England Revolution\"]['Overall']\n\n#bp = plt.boxplot([barcelona, madrid, revs], labels=['a','b','c'], boxprops=dict(facecolor='red'))\nbp = plt.boxplot([barcelona, madrid, revs], tick_labels=['FC Barcelona','Real Madrid','NE Revolution'], patch_artist=True, medianprops={'linewidth': 2})\n\nplt.title('Professional Soccer Team Comparison')\nplt.ylabel('FIFA Overall Rating')\n\nfor box in bp['boxes']:\n    # change outline color\n    box.set(color='#4286f4', linewidth=2)\n    # change fill color\n    box.set(facecolor = '#e0e0e0' )\n    # change hatch\n    #box.set(hatch = '/')\n\n\n\n\n\n\n\n\nYou’ve already learned how to create plots using Matplotlib’s Pyplot. Next, you’ll explore how to simplify and enhance your visualizations by using its powerful wrappers: Pandas and Seaborn.\n\n\n\n9.2.2.6 Limitations of Using Matplotlib for Plotting\nMatplotlib is a popular visualization library, but it has flaws.\n\nDefaults are not ideal (gridlines, background, etc need to be configured.)\nLibrary is low-level (doing anything complicated takes quite a bit of codes)\nLack of integration with pandas data structures\n\nTo address these challenges, Seaborn act as higher-level interfaces to Matplotlib, offering better defaults, simpler syntax, and seamless integration with DataFrames.\n\n\n\n9.2.3 Plotting with Seaborn\nSeaborn is a powerful data visualization library built on top of Matplotlib, designed to make statistical plots easier and more attractive. It integrates seamlessly with Pandas, making it an excellent tool for plotting data from DataFrames. Seaborn comes with better default aesthetics and provides more specialized plots that are easy to implement.\nSome of the key advantages of using Seaborn include:\n\nSimpler syntax: Seaborn simplifies the process of creating complex plots with just a few lines of code.\nBeautiful default styles: Seaborn’s default plots are more polished and aesthetically pleasing compared to Matplotlib’s defaults.\nSeamless Pandas integration: You can directly pass Pandas DataFrames to Seaborn, and it understands column names for axis labels and plot elements.\n\nseaborn comes with 17 built-in datasets. That means you don’t have to spend a whole lot of your time finding the right dataset and cleaning it up to make Seaborn-ready; rather you will focus on the core features of Seaborn visualization techniques to solve problems.\n\nimport seaborn as sns\n# get names of the builtin dataset\nsns.get_dataset_names()\n\n['anagrams',\n 'anscombe',\n 'attention',\n 'brain_networks',\n 'car_crashes',\n 'diamonds',\n 'dots',\n 'dowjones',\n 'exercise',\n 'flights',\n 'fmri',\n 'geyser',\n 'glue',\n 'healthexp',\n 'iris',\n 'mpg',\n 'penguins',\n 'planets',\n 'seaice',\n 'taxis',\n 'tips',\n 'titanic']\n\n\n\n9.2.3.1 Customizing Plot Aesthetics\nSeaborn provides a convenient function called sns.set_style() that allows users to customize the visual appearance of their plots. This function is particularly useful for enhancing the aesthetics of visualizations, making them more appealing and easier to interpret.\n\n9.2.3.1.1 Purpose of sns.set_style()\nThe primary purpose of sns.set_style() is to set the visual context and style for all plots created after the call. This allows for a consistent and visually pleasing representation of data across multiple visualizations.\n\n\n9.2.3.1.2 Available Style Options\nSeaborn offers several built-in styles that can be set using sns.set_style(). The options include:\n\ndarkgrid:\n\nA dark background with a grid overlay.\nIdeal for visualizing data with many points or intricate details.\n\nwhitegrid:\n\nA white background with a grid overlay.\nProvides a clean and professional look, suitable for most types of visualizations.\n\ndark:\n\nA dark background without gridlines.\nUseful for emphasizing data points without distractions from the grid.\n\nwhite:\n\nA simple white background without gridlines.\nOffers a minimalist aesthetic, focusing solely on the data.\n\nticks:\n\nA white background with ticks on the axes.\nCombines the clarity of a white background with a bit of added detail for reference.\n\n\n\n\n9.2.3.1.3 How to Use sns.set_style()\nTo apply a specific style, simply call sns.set_style() with the desired style name before creating your plots. Here’s an example:\n\nsns.set_style(\"whitegrid\")\n\nLet’s do a few common plots with Seaborn tips dataset\n\n# Load data into a Pandas dataframe\nflowers_df = sns.load_dataset(\"iris\")\nflowers_df.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\n\n9.2.3.2 Scatterplot\n\nsns.scatterplot(x=flowers_df.sepal_length, y=flowers_df.sepal_width);\n\n\n\n\n\n\n\n\n\n9.2.3.2.1 Adding Hues to the scatterplot\nNotice how the points in the above plot seem to form distinct clusters with some outliers. We can color the dots using the flower species as a hue. We can also make the points larger using the s argument.\n\nflowers_df.species.unique()\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nsns.scatterplot(x=flowers_df.sepal_length, y=flowers_df.sepal_width, hue=flowers_df.species, s=100);\n\n\n\n\n\n\n\n\nAdding hues makes the plot more informative. We can immediately tell that Setosa flowers have a smaller sepal length but higher sepal widths. In contrast, the opposite is true for Virginica flowers.\n\n\n9.2.3.2.2 Customizing Seaborn Figures\nSince Seaborn uses Matplotlib’s plotting functions internally, we can use functions like plt.figure and plt.title to modify the figure.\n\nplt.figure(figsize=(12, 6))\nplt.title('Sepal Dimensions')\n\nsns.scatterplot(x=flowers_df.sepal_length, \n                y=flowers_df.sepal_width, \n                hue=flowers_df.species,\n                s=100);\n\n\n\n\n\n\n\n\n\n\n9.2.3.2.3 Integration with Pandas Data Frames\nSeaborn has in-built support for Pandas data frames. Instead of passing each column as a series, you can provide column names and use the data argument to specify a data frame.\n\nplt.title('Sepal Dimensions')\nsns.scatterplot(x='sepal_length', \n                y='sepal_width', \n                hue='species',\n                s=100,\n                data=flowers_df);\n\n\n\n\n\n\n\n\n\n\n\n9.2.3.3 Histgram\n\nsns.histplot(data=flowers_df, x='sepal_width');\n\n\n\n\n\n\n\n\n\n# show kde(kernal density estimate)\nsns.histplot(data=flowers_df, x='sepal_width', kde=True);\n\n\n\n\n\n\n\n\n\n# adding hue\nsns.histplot(data=flowers_df, x=\"sepal_width\", hue=\"species\");\n\n\n\n\n\n\n\n\n\n\n9.2.3.4 Barplot\n\ntips_df = sns.load_dataset(\"tips\")\nsns.barplot(x='day', y='total_bill', data=tips_df);\n\n\n\n\n\n\n\n\n\nsns.barplot(x='day', y='tip', hue='sex', data=tips_df);\n\n\n\n\n\n\n\n\n\n# make the bars horizontal simply by switching the axes\nsns.barplot(x='tip', y='day', hue='sex', data=tips_df);\n\n\n\n\n\n\n\n\n\n\n9.2.3.5 Boxplot\nPurpose: Boxplots is a standardized way of visualizing the distribution of a continuous variable. They show five key metrics that describe the data distribution - median, 25th percentile value, 75th percentile value, minimum and maximum, as shown in the figure below. Note that the minimum and maximum exclude the outliers.\n\n\n\n\n\nExample: Create a box plot to compare the distributions of total tips based on the day of the week, differentiating between male and female patrons.\n\nsns.boxplot(data=tips_df, y='total_bill', x='day', hue='sex');\n\n\n\n\n\n\n\n\nFrom the above plot, what you can observe?\n\n\n9.2.3.6 Heatmap\nRepresent 2-dimensional data like a matrix or table using colors\n\nflights_df = sns.load_dataset(\"flights\").pivot(index=\"month\", columns=\"year\", values=\"passengers\")\nflights_df\n# you will learn pivot in the later chapters\n\n\n\n\n\n\n\nyear\n1949\n1950\n1951\n1952\n1953\n1954\n1955\n1956\n1957\n1958\n1959\n1960\n\n\nmonth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan\n112\n115\n145\n171\n196\n204\n242\n284\n315\n340\n360\n417\n\n\nFeb\n118\n126\n150\n180\n196\n188\n233\n277\n301\n318\n342\n391\n\n\nMar\n132\n141\n178\n193\n236\n235\n267\n317\n356\n362\n406\n419\n\n\nApr\n129\n135\n163\n181\n235\n227\n269\n313\n348\n348\n396\n461\n\n\nMay\n121\n125\n172\n183\n229\n234\n270\n318\n355\n363\n420\n472\n\n\nJun\n135\n149\n178\n218\n243\n264\n315\n374\n422\n435\n472\n535\n\n\nJul\n148\n170\n199\n230\n264\n302\n364\n413\n465\n491\n548\n622\n\n\nAug\n148\n170\n199\n242\n272\n293\n347\n405\n467\n505\n559\n606\n\n\nSep\n136\n158\n184\n209\n237\n259\n312\n355\n404\n404\n463\n508\n\n\nOct\n119\n133\n162\n191\n211\n229\n274\n306\n347\n359\n407\n461\n\n\nNov\n104\n114\n146\n172\n180\n203\n237\n271\n305\n310\n362\n390\n\n\nDec\n118\n140\n166\n194\n201\n229\n278\n306\n336\n337\n405\n432\n\n\n\n\n\n\n\nflights_df is a matrix with one row for each month and one column for each year. The values show the number of passengers (in thousands) that visited the airport in a specific month of a year. We can use the sns.heatmap function to visualize the footfall at the airport.\n\nplt.title(\"No. of Passengers (1000s)\")\nsns.heatmap(flights_df);\n\n\n\n\n\n\n\n\nThe brighter colors indicate a higher footfall at the airport. By looking at the graph, we can infer two things:\n\nThe footfall at the airport in any given year tends to be the highest around July & August.\nThe footfall at the airport in any given month tends to grow year by year.\n\nWe can also display the actual values in each block by specifying annot=True and using the cmap argument to change the color palette.\n\n# fmt = \"d\" decimal integer. output are the number in base 10\nplt.title(\"No. of Passengers (1000s)\")\nsns.heatmap(flights_df, fmt=\"d\", annot=True, cmap='Blues');\n\n\n\n\n\n\n\n\n\n\n9.2.3.7 Correlation Map\nA correlation map is a specific type of heatmap where the values represent the correlation coefficients between variables (ranging from -1 to 1). It visually shows the strength and direction of relationships between numerical variables.\nCorrelation may refer to any kind of association between two random variables. However, in this book, we will always consider correlation as the linear association between two random variables, or the Pearson’s correlation coefficient. Note that correlation does not imply causality and vice-versa.\nThe Pandas function corr() provides the pairwise correlation between all columns of a DataFrame, or between two Series. The function corrwith() provides the pairwise correlation of a DataFrame with another DataFrame or Series.\n\n#Pairwise correlation amongst all columns\nsurvey_data = pd.read_csv('./Datasets/survey_data_clean.csv')\n\nsurvey_data.head()\n\n\n\n\n\n\n\n\nTimestamp\nfav_alcohol\nparties_per_month\nsmoke\nweed\nintrovert_extrovert\nlove_first_sight\nlearning_style\nleft_right_brained\npersonality_type\n...\nused_python_before\ndominant_hand\nchildhood_in_US\ngender\nregion_of_residence\npolitical_affliation\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\n\n\n\n0\n2022/09/13 1:43:34 pm GMT-5\nI don't drink\n1.0\nNo\nOccasionally\nIntrovert\n0\nVisual (learn best through images or graphic o...\nLeft-brained (logic, science, critical thinkin...\nINFJ\n...\n1\nRight\n1\nFemale\nNortheast\nDemocrat\n0\n1\n0\n0\n\n\n1\n2022/09/13 5:28:17 pm GMT-5\nHard liquor/Mixed drink\n3.0\nNo\nOccasionally\nExtrovert\n0\nVisual (learn best through images or graphic o...\nLeft-brained (logic, science, critical thinkin...\nESFJ\n...\n1\nRight\n1\nMale\nWest\nDemocrat\n0\n1\n0\n0\n\n\n2\n2022/09/13 7:56:38 pm GMT-5\nHard liquor/Mixed drink\n3.0\nNo\nYes\nIntrovert\n0\nKinesthetic (learn best through figuring out h...\nLeft-brained (logic, science, critical thinkin...\nISTJ\n...\n0\nRight\n0\nFemale\nInternational\nNo affiliation\n0\n1\n0\n0\n\n\n3\n2022/09/13 10:34:37 pm GMT-5\nHard liquor/Mixed drink\n12.0\nNo\nNo\nExtrovert\n0\nVisual (learn best through images or graphic o...\nLeft-brained (logic, science, critical thinkin...\nENFJ\n...\n0\nRight\n1\nFemale\nSoutheast\nDemocrat\n0\n1\n0\n0\n\n\n4\n2022/09/14 4:46:19 pm GMT-5\nI don't drink\n1.0\nNo\nNo\nExtrovert\n1\nReading/Writing (learn best through words ofte...\nRight-brained (creative, art, imaginative, int...\nENTJ\n...\n0\nRight\n1\nFemale\nNortheast\nDemocrat\n1\n0\n0\n0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n#Pairwise correlation amongst all columns\nsurvey_data.select_dtypes(include='number').corr()\n\n\n\n\n\n\n\n\nparties_per_month\nlove_first_sight\nnum_insta_followers\nexpected_marriage_age\nexpected_starting_salary\nminutes_ex_per_week\nsleep_hours_per_day\nfarthest_distance_travelled\nfav_number\ninternet_hours_per_day\n...\nprocrastinator\nnum_clubs\nstudent_athlete\nAP_stats\nused_python_before\nchildhood_in_US\ncant_change_math_ability\ncan_change_math_ability\nmath_is_genetic\nmuch_effort_is_lack_of_talent\n\n\n\n\nparties_per_month\n1.000000\n0.096129\n0.239705\n-0.064079\n0.114881\n0.195561\n-0.052542\n-0.017081\n-0.050139\n0.087390\n...\n-0.056871\n-0.010514\n0.290830\n-0.013222\n-0.040033\n0.081905\n-0.052912\n0.055575\n-0.013374\n-0.029838\n\n\nlove_first_sight\n0.096129\n1.000000\n-0.024010\n-0.084406\n0.080138\n0.099244\n-0.025378\n-0.075539\n0.105095\n-0.007652\n...\n0.033951\n0.083342\n0.014595\n-0.062992\n-0.034692\n-0.118260\n0.005254\n0.020758\n-0.003710\n0.013376\n\n\nnum_insta_followers\n0.239705\n-0.024010\n1.000000\n-0.130157\n0.127226\n0.099341\n-0.042421\n0.011308\n-0.124763\n-0.028427\n...\n-0.089871\n0.265958\n0.044807\n0.005947\n-0.016201\n0.072622\n-0.150658\n0.130774\n-0.018411\n-0.165899\n\n\nexpected_marriage_age\n-0.064079\n-0.084406\n-0.130157\n1.000000\n-0.014881\n-0.088073\n0.182009\n-0.024038\n-0.008924\n-0.029772\n...\n-0.020012\n-0.137069\n-0.036122\n0.010447\n0.052727\n0.053759\n-0.072163\n0.087633\n-0.086898\n0.052813\n\n\nexpected_starting_salary\n0.114881\n0.080138\n0.127226\n-0.014881\n1.000000\n0.134065\n-0.005078\n-0.028329\n-0.028125\n0.017479\n...\n0.054273\n-0.100922\n-0.026219\n-0.084894\n-0.094541\n0.081142\n-0.011609\n0.019171\n0.078694\n0.097265\n\n\nminutes_ex_per_week\n0.195561\n0.099244\n0.099341\n-0.088073\n0.134065\n1.000000\n0.049593\n-0.153188\n0.038758\n-0.028457\n...\n-0.045149\n-0.024572\n0.576301\n-0.062544\n0.057760\n0.235492\n-0.101282\n0.134430\n-0.047772\n-0.045141\n\n\nsleep_hours_per_day\n-0.052542\n-0.025378\n-0.042421\n0.182009\n-0.005078\n0.049593\n1.000000\n0.104175\n-0.021909\n0.017435\n...\n-0.176579\n-0.163860\n0.058361\n-0.013909\n0.096528\n-0.059468\n-0.058086\n0.012174\n0.027052\n-0.022025\n\n\nfarthest_distance_travelled\n-0.017081\n-0.075539\n0.011308\n-0.024038\n-0.028329\n-0.153188\n0.104175\n1.000000\n-0.108661\n0.049450\n...\n0.032492\n-0.045214\n-0.158027\n0.010580\n0.012353\n-0.282821\n-0.046074\n0.017935\n0.110037\n0.046895\n\n\nfav_number\n-0.050139\n0.105095\n-0.124763\n-0.008924\n-0.028125\n0.038758\n-0.021909\n-0.108661\n1.000000\n-0.013070\n...\n0.085508\n-0.013696\n-0.014435\n0.091011\n0.030736\n0.072894\n-0.032534\n0.034319\n-0.063692\n-0.073777\n\n\ninternet_hours_per_day\n0.087390\n-0.007652\n-0.028427\n-0.029772\n0.017479\n-0.028457\n0.017435\n0.049450\n-0.013070\n1.000000\n...\n0.048239\n0.064527\n-0.017944\n0.001818\n0.051970\n0.033120\n-0.033902\n0.050258\n0.190205\n-0.053708\n\n\nonly_child\n-0.142519\n0.124345\n-0.152184\n-0.043141\n-0.088648\n-0.123371\n0.038126\n0.214377\n-0.024419\n-0.035022\n...\n0.073415\n-0.065484\n0.064136\n0.048031\n-0.139898\n-0.387711\n0.023089\n-0.019982\n0.058226\n0.092372\n\n\nnum_majors_minors\n-0.073127\n0.108730\n0.050431\n-0.055280\n0.021278\n0.044450\n-0.024339\n-0.012779\n0.023903\n-0.073775\n...\n-0.073806\n0.311266\n-0.035500\n-0.068640\n-0.073388\n-0.153529\n-0.077501\n0.024734\n-0.125809\n-0.064939\n\n\nhigh_school_GPA\n0.295646\n0.069288\n0.147402\n0.017052\n0.053354\n-0.076471\n-0.036904\n-0.064116\n-0.023081\n-0.034485\n...\n0.031561\n-0.020854\n0.006332\n0.066837\n0.072777\n0.005606\n-0.095025\n0.093416\n-0.082620\n0.001373\n\n\nNU_GPA\n-0.080548\n-0.114041\n0.004702\n0.011925\n-0.048069\n-0.108177\n0.143997\n0.038238\n-0.307656\n-0.014531\n...\n-0.269552\n0.016724\n-0.027378\n-0.026544\n-0.008536\n-0.028968\n0.002094\n-0.137330\n0.036731\n0.047840\n\n\nage\n-0.032771\n0.142384\n-0.230698\n0.060416\n-0.102632\n-0.040906\n-0.035890\n0.018811\n0.096818\n0.017515\n...\n-0.005892\n-0.127760\n-0.038315\n-0.026959\n0.009924\n-0.152784\n-0.005954\n0.014759\n-0.009315\n-0.126370\n\n\nheight\n-0.005405\n0.216072\n0.009318\n0.044577\n0.151517\n0.182090\n-0.010650\n-0.235067\n0.041298\n-0.023174\n...\n0.063263\n0.212038\n0.080953\n0.022484\n0.016110\n0.160309\n-0.055641\n0.101811\n-0.064383\n0.028509\n\n\nheight_father\n0.126741\n0.029419\n0.179684\n0.026949\n0.011450\n0.156227\n0.097593\n-0.118669\n-0.032717\n-0.047314\n...\n-0.111183\n0.022701\n0.155003\n-0.010982\n-0.006480\n0.137934\n-0.019593\n0.008157\n0.010222\n0.060439\n\n\nheight_mother\n0.079121\n0.082684\n0.129716\n0.075316\n0.033947\n0.114181\n-0.044089\n-0.134582\n-0.029568\n-0.091417\n...\n-0.078265\n0.091390\n0.053258\n-0.100647\n-0.021396\n0.119292\n0.027120\n0.034961\n-0.035449\n0.074492\n\n\nprocrastinator\n-0.056871\n0.033951\n-0.089871\n-0.020012\n0.054273\n-0.045149\n-0.176579\n0.032492\n0.085508\n0.048239\n...\n1.000000\n0.078341\n0.094363\n0.003053\n-0.016254\n-0.090868\n0.002462\n0.084419\n-0.001738\n0.081471\n\n\nnum_clubs\n-0.010514\n0.083342\n0.265958\n-0.137069\n-0.100922\n-0.024572\n-0.163860\n-0.045214\n-0.013696\n0.064527\n...\n0.078341\n1.000000\n-0.084562\n0.087438\n0.115062\n-0.021044\n-0.136249\n0.070002\n-0.090570\n-0.108851\n\n\nstudent_athlete\n0.290830\n0.014595\n0.044807\n-0.036122\n-0.026219\n0.576301\n0.058361\n-0.158027\n-0.014435\n-0.017944\n...\n0.094363\n-0.084562\n1.000000\n-0.040686\n-0.049288\n0.082888\n-0.066667\n-0.022576\n-0.060523\n0.121232\n\n\nAP_stats\n-0.013222\n-0.062992\n0.005947\n0.010447\n-0.084894\n-0.062544\n-0.013909\n0.010580\n0.091011\n0.001818\n...\n0.003053\n0.087438\n-0.040686\n1.000000\n0.089517\n0.106584\n0.081109\n0.029743\n-0.048375\n-0.018043\n\n\nused_python_before\n-0.040033\n-0.034692\n-0.016201\n0.052727\n-0.094541\n0.057760\n0.096528\n0.012353\n0.030736\n0.051970\n...\n-0.016254\n0.115062\n-0.049288\n0.089517\n1.000000\n0.041928\n-0.011217\n0.156806\n0.088566\n0.023366\n\n\nchildhood_in_US\n0.081905\n-0.118260\n0.072622\n0.053759\n0.081142\n0.235492\n-0.059468\n-0.282821\n0.072894\n0.033120\n...\n-0.090868\n-0.021044\n0.082888\n0.106584\n0.041928\n1.000000\n-0.008575\n0.057185\n-0.178003\n-0.013098\n\n\ncant_change_math_ability\n-0.052912\n0.005254\n-0.150658\n-0.072163\n-0.011609\n-0.101282\n-0.058086\n-0.046074\n-0.032534\n-0.033902\n...\n0.002462\n-0.136249\n-0.066667\n0.081109\n-0.011217\n-0.008575\n1.000000\n-0.672777\n0.294544\n0.101835\n\n\ncan_change_math_ability\n0.055575\n0.020758\n0.130774\n0.087633\n0.019171\n0.134430\n0.012174\n0.017935\n0.034319\n0.050258\n...\n0.084419\n0.070002\n-0.022576\n0.029743\n0.156806\n0.057185\n-0.672777\n1.000000\n-0.361546\n-0.131047\n\n\nmath_is_genetic\n-0.013374\n-0.003710\n-0.018411\n-0.086898\n0.078694\n-0.047772\n0.027052\n0.110037\n-0.063692\n0.190205\n...\n-0.001738\n-0.090570\n-0.060523\n-0.048375\n0.088566\n-0.178003\n0.294544\n-0.361546\n1.000000\n0.154083\n\n\nmuch_effort_is_lack_of_talent\n-0.029838\n0.013376\n-0.165899\n0.052813\n0.097265\n-0.045141\n-0.022025\n0.046895\n-0.073777\n-0.053708\n...\n0.081471\n-0.108851\n0.121232\n-0.018043\n0.023366\n-0.013098\n0.101835\n-0.131047\n0.154083\n1.000000\n\n\n\n\n28 rows × 28 columns\n\n\n\nQ: Which feature is the most correlated with NU_GPA?\n\nsurvey_data.select_dtypes(include='number').corrwith(survey_data.NU_GPA).sort_values(ascending = False)\n\nNU_GPA                           1.000000\nsleep_hours_per_day              0.143997\nnum_majors_minors                0.141988\nonly_child                       0.106440\nmuch_effort_is_lack_of_talent    0.047840\nfarthest_distance_travelled      0.038238\nmath_is_genetic                  0.036731\nnum_clubs                        0.016724\nexpected_marriage_age            0.011925\nnum_insta_followers              0.004702\ncant_change_math_ability         0.002094\nused_python_before              -0.008536\ninternet_hours_per_day          -0.014531\nAP_stats                        -0.026544\nstudent_athlete                 -0.027378\nchildhood_in_US                 -0.028968\nhigh_school_GPA                 -0.030883\nheight_father                   -0.040120\nexpected_starting_salary        -0.048069\nage                             -0.052039\nheight_mother                   -0.079276\nparties_per_month               -0.080548\nheight                          -0.099082\nminutes_ex_per_week             -0.108177\nlove_first_sight                -0.114041\ncan_change_math_ability         -0.137330\nprocrastinator                  -0.269552\nfav_number                      -0.307656\ndtype: float64\n\n\n\nsns.set(rc={'figure.figsize':(12,10)})\nsns.heatmap(survey_data.select_dtypes(include='number').corr());\n\n\n\n\n\n\n\n\nFrom the above map, we can see that:\n\nstudent athlete is strongly postively correlated with minutes_ex_per_week\nprocrastinator is strongly negatively correlated with NU_GPA",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "Data visualization.html#independent-study",
    "href": "Data visualization.html#independent-study",
    "title": "9  Introduction to Data Visualization",
    "section": "9.3 Independent Study",
    "text": "9.3 Independent Study\n\n9.3.1 Practice exercise 1\nRead the gas_price.csv file and plot the trend for each country over the time using matplotlib pyplot\n\n\n9.3.2 Practice exercise 2\n\n9.3.2.1 \nIs NU_GPA associated with parties_per_month? Analyze the association separately for Sophomores, Juniors, and Seniors (categories of the variable school_year).\nMake scatterplots of NU_GPA vs parties_per_month in a 1 x 3 grid, where each grid is for a distinct school_year. Plot the trendline as well for each scatterplot. Use the file survey_data_clean.csv.\n\n\n9.3.2.2 \nCapping the the values of parties_per_month to 30, and make the visualizations again.\n\n\n\n9.3.3 Practice exercise 3\nHow does the expected marriage age of the people of STAT303-1 depend on their characteristics? We’ll use visualizations to answer this question. Use data from the file survey_data_clean.csv. Proceed as follows:\n\n9.3.3.1 \nMake a visualization that compares the mean expected_marriage_age of introverts and extroverts (use the variable introvert_extrovert). What insights do you obtain?\n\n\n9.3.3.2 \nDoes the mean expected_marriage_age of introverts and extroverts depend on whether they believe in love in first sight (variable name: love_first_sight)? Update the previous visualization to answer the question.\n\n\n9.3.3.3 \nIn addition to love_first_sight, does the mean expected_marriage_age of introverts and extroverts depend on whether they are a procrastinator (variable name: procrastinator)? Update the previous visualization to answer the question.\n\n\n9.3.3.4 \nIs there any critical information missing in the above visualizations that, if revealed, may cast doubts on the patterns observed in them?\n\n\n\n9.3.4 Practice exercise 4\nRead Australia_weather.csv,\n\n9.3.4.1 \nCreate a histogram showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\n\n9.3.4.2 \nMake a density plot showing the distributions of maximum temperature in Sydney, Canberra and Melbourne.\n\n\n9.3.4.3 \nShow the distributions of the maximum and minimum temperatures in a single plot.\n\n\n9.3.4.4 \nCreate a scatter plot with a trendline for MinTemp and MaxTemp, including a confidence interval.\nHint: Using Seaborn, the regplot() function enables us to overlay a trendline on the scatter plot, complete with a 95% confidence interval for the trendline",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Visualization</span>"
    ]
  },
  {
    "objectID": "Advanced_Data_Visualization.html",
    "href": "Advanced_Data_Visualization.html",
    "title": "10  Advanced Data Visualization",
    "section": "",
    "text": "10.1 Matplotlib Plotting Interfaces\nIn the previous chapter, we explored basic plotting techniques using Pandas, Seaborn, and Matplotlib to create visualizations. Now, we’ll elevate our skills by diving into more advanced topics, such as crafting complex subplots and visualizing geospatial data, enabling us to build richer and more insightful plots.\nTo get started, let’s import necessary libraries.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "Advanced_Data_Visualization.html#matplotlib-plotting-interfaces",
    "href": "Advanced_Data_Visualization.html#matplotlib-plotting-interfaces",
    "title": "10  Advanced Data Visualization",
    "section": "",
    "text": "10.1.1 Pyplot interface and OOP Interface\nThere are two types of interfaces in Matplotlib for visualization and these are given below:\n\n\n\n10.1.2 Plot a simple figure using two interfaces\n\ngdp_data = pd.read_csv('datasets/gdp_lifeExpectancy.csv')\ngdp_data.head()\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n\n\n\n#Object-oriented interface\nfig, ax = plt.subplots() #Create a figure and an axes\nx = gdp_data.gdpPercap \ny = gdp_data.lifeExp\nax.plot(x,y,'o')   #Plot data on the axes\nax.set_xlabel('GDP per capita')    #Add an x-label to the axes\nax.set_ylabel('Life expectancy')   #Add a y-label to the axes\nax.set_title('Life expectancy vs GDP per capita from 1952 to 2007');\n\n\n\n\n\n\n\n\n\n#pyplot interface\nx = gdp_data.gdpPercap\ny = gdp_data.lifeExp\nplt.plot(x,y,'o') #By default, the plot() function makes a lineplot. The 'o' arguments specifies a scatterplot\nplt.xlabel('GDP per capita')  #Labelling the horizontal X-axis\nplt.ylabel('Life expectancy') #Labelling the verical Y-axis\nplt.title('Life expectancy vs GDP per capita from 1952 to 2007');\n\n\n\n\n\n\n\n\n\n\n10.1.3 Pyplot Interface\nIn the previous chapter, our plotting is completely based on pyplot interface of Matplotlib, which is just for basic plotting. You can easily generate plots using pyplot module in the matplotlib library just by importing matplotlib.pyplot module.\n\npyplot interface is a state-based interface. It implicitly tracks the plot that it wants to reference\nSimple functions are used to add plot elements and/or modify the plot as we need, whenever we need it.\nThe Pyplot interface shares a lot of similarities in syntax and methodology with MATLAB.\n\nHowever, The pyplot interface has limited functionality in these two cases:\n\nwhen there is a need to make multiple plots\nwhen we have to make plots that require a lot of customization.\n\nFor more advanced plotting with Matplotlib, you have to learn Object-Oriented Interface.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "Advanced_Data_Visualization.html#plotting-with-object-oriented-interface-of-matplotlib",
    "href": "Advanced_Data_Visualization.html#plotting-with-object-oriented-interface-of-matplotlib",
    "title": "10  Advanced Data Visualization",
    "section": "10.2 Plotting with Object-Oriented Interface of Matplotlib",
    "text": "10.2 Plotting with Object-Oriented Interface of Matplotlib\n\n10.2.1 Matplotlib Object Anatomy\nTo use Matplotlib’s object-oriented interface effectively, it’s important to understand the anatomy of a plot—how different visual elements are structured within a figure:\n\n\n\n10.2.2 Matplotlib Object Hierarchy\nMatplotlib plots follow a hierarchical structure, with two core components:\n\nFigure: The overall container for one or more plots.\nAxes: The individual plot area where data is visualized.\n\nEach Axes contains further elements such as:\n\nAxis (x-axis and y-axis)\nTitle, Labels, Ticks\nDrawable objects like Lines, Text, and Patches (collectively called Artists)\n\nHere’s a simplified view of this hierarchy:\nFigure\n └── Axes (1 or more)\n      ├── Axis (XAxis, YAxis)\n      ├── Title\n      ├── Label\n      ├── Tick\n      └── Artist objects (Lines, Text, Patches, etc.)\nUnderstanding this structure is key to using Matplotlib’s object-oriented interface, as it allows you to access and customize each component directly.\n\n10.2.2.1 Figure\nThe outermost object is the figure which is an instance of figure.Figure. It is the top level container for all the plot elements. The Figure is the final image that may contain one or more Axes and it keeps track of all the Axes. Figure is only a container and you can not plot data on figure.\n\n# To begin with, we create a fiture instance which provides an empty canvas\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\nFigure is just a container, note that creating figure using plt.figure() does not automatically create an axes and hence all you can see is the figure object.\n\n\n10.2.2.2 Axes\nAxes is the instance of matplotlib.axes.Axes. Axes is the area on which data are plotted. A given figure can contain many Axes, but a given Axes object can only be in one Figure.\n\n10.2.2.2.1 Creating an Axes Object Explicitly\nIn the OOP interface, Axes objects are usually created using plt.subplots() or plt.figure().add_subplot().\n\n# A figure only contains one axes by default\nfig = plt.figure()\n# add axes to the figure\nax = fig.add_subplot()\n\n\n\n\n\n\n\n\nWe create a blank axes (area) for plotting, if we need to add more axes to the figure\n\n# create a figure with 4 axes, arranged in 2 rows and 2 columns\nfig, axes = plt.subplots(2, 2)\n\n\n\n\n\n\n\n\n\n\n10.2.2.2.2 Components of an Axes Object\nThe Axes object contains several elements that make up the plot\n\nData plotting area: Contains the data (lines, bars, points, etc.).\nX-axis and Y-axis: Controls the axis limits, labels, and ticks.\nTitle and Labels: The overall title and labels for each axis.\nGridlines: Optional lines to help align the data visually.\nSpines: The borders around the plot.\nLegend: An optional component to explain the data series.\nAnnotations: Text or arrows highlighting points of interest.\n\nThis is what makes the Axes object central to any plot in the OOP interface of Matplotlib.\n\n# create data for plotting\nx = np.arange(10)\ny = x**2 \n\n# create a figure and axes\nfig,ax = plt.subplots(1,1) \n\n# plot the data\nax.plot(x,y)\n\n# set the title\nax.set_title(\"Exponential Plot\")\n \n# set the labels of x and y axes\nax.set_xlabel(\"age\")\nax.set_ylabel(\"Cell growth\")\n\n# set the limits of x and y axes\nax.set_xlim([0, 10])\nax.set_ylim([0, 100])\n\n# set the ticks of x and y axes\nax.set_xticks(range(0, 11, 2))\nax.set_yticks(range(0, 101, 20))\n\n# add grid\nax.grid(True)\n\n# add legend\nax.legend([\"y = x^2\"], loc = \"upper left\")\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n10.2.3 Creating Complex Plots with Multiple Subplots\nTo help illustrate subplots in matplotlib, we can cover two scenarios:\n\nsubplots that don’t overlap and\nsubplots inside other subplots\n\n\n10.2.3.1 Plotting non-overlapped subplots\nThis is the most common use case, where multiple plots are placed next to each other in a grid, without overlap. The plt.subplots() function allows for a clean layout where each plot is contained in its own space. You can specify the number of rows and columns to create a grid of subplots.\n\nflowers_df = sns.load_dataset('iris')\ntips_df = sns.load_dataset('tips')\nflights_df = sns.load_dataset(\"flights\").pivot(index=\"month\", columns=\"year\", values=\"passengers\")\n\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\n# Pass the axes into seaborn\naxes[0,0].set_title('Sepal Length vs. Sepal Width')\nsns.scatterplot(x=flowers_df.sepal_length, \n                y=flowers_df.sepal_width, \n                hue=flowers_df.species, \n                s=100, \n                ax=axes[0,0])\n\n# Use the axes for plotting\naxes[0,1].set_title('Distribution of Sepal Width')\nsns.histplot(flowers_df.sepal_width, ax=axes[0,1])\naxes[0,1].legend(['Setosa', 'Versicolor', 'Virginica'])\n\n# Pass the axes into seaborn\naxes[1,0].set_title('Restaurant bills')\nsns.barplot(x='day', y='total_bill', hue='sex', data=tips_df, ax=axes[1,0])\n\n# Pass the axes into seaborn\naxes[1,1].set_title('Flight traffic')\nsns.heatmap(flights_df, cmap='Blues', ax=axes[1,1])\n\nplt.tight_layout(pad=2)\n\n\n\n\n\n\n\n\n\nThe plt.tight_layout() ensures the subplots do not overlap by adjusting the spacing automatically.\nSeaborn and pandas are wrappers of Matplotlib. To create a Seaborn/pandas plot in a specific Matplotlib subplot, you pass the ax parameter to the its plotting function. This allows you to use their visualization capabilities while fully controlling the layout of the plot using Matplotlib’s plt.subplots()\n\n\n\n10.2.3.2 Plotting Nested Subplots (Subplots Inside Other Subplots)\nYou can create a subplot inside another plot using add_axes() or inset_axes() from matplotlib’s Axes object. This is useful for creating insets or focusing on a specific region within a larger plot.\nSyntax of add_axes()\nax = fig.add_axes([left, bottom, width, height])\nwhere\n\nleft: The x-position (horizontal starting point) of the axes, as a fraction of the figure width (0 to 1).\nbottom: The y-position (vertical starting point) of the axes, as a fraction of the figure height (0 to 1).\nwidth: The width of the axes, as a fraction of the figure width (0 to 1).\nheight: The height of the axes, as a fraction of the figure height (0 to 1).\n\nBelow is an example demonstrating the use of add_axes(). You can also explore the inset_axes() method for creating inset plots with more flexibility\n\n# create inset axes within the main plot axes  \n\nnp.random.seed(19680801)  # Fixing random state for reproducibility.\n\n# create some data to use for the plot\ndt = 0.001\nt = np.arange(0.0, 10.0, dt)\nr = np.exp(-t[:1000] / 0.05)  # impulse response\nx = np.random.randn(len(t))\ns = np.convolve(x, r)[:len(x)] * dt  # colored noise\n\nfig, main_ax = plt.subplots()\nmain_ax.plot(t, s)\nmain_ax.set_xlim(0, 1)\nmain_ax.set_ylim(1.1 * np.min(s), 2 * np.max(s))\nmain_ax.set_xlabel('time (s)')\nmain_ax.set_ylabel('current (nA)')\nmain_ax.set_title('Gaussian colored noise')\n\n# this is an inset axes over the main axes\nright_inset_ax = fig.add_axes([.65, .6, .2, .2], facecolor='k')\nright_inset_ax.hist(s, 400, density=True)\nright_inset_ax.set(title='Probability', xticks=[], yticks=[])\n\n# this is another inset axes over the main axes\nleft_inset_ax = fig.add_axes([.2, .6, .2, .2], facecolor='k')\nleft_inset_ax.plot(t[:len(r)], r)\nleft_inset_ax.set(title='Impulse response', xlim=(0, .2), xticks=[], yticks=[])\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n10.2.4 Advanced Customization with Matplotlib’s Object-Oriented Interface\nBelow, we are reading the dataset of noise complaints of type Loud music/Party received the police in New York City in 2016.\n\nnyc_party_complaints = pd.read_csv('datasets/party_nyc.csv')\nnyc_party_complaints.head()\n\n\n\n\n\n\n\n\nCreated Date\nClosed Date\nLocation Type\nIncident Zip\nCity\nBorough\nLatitude\nLongitude\nHour_of_the_day\nMonth_of_the_year\n\n\n\n\n0\n12/31/2015 0:01\n12/31/2015 3:48\nStore/Commercial\n10034.0\nNEW YORK\nMANHATTAN\n40.866183\n-73.918930\n0\n12\n\n\n1\n12/31/2015 0:02\n12/31/2015 4:36\nStore/Commercial\n10040.0\nNEW YORK\nMANHATTAN\n40.859324\n-73.931237\n0\n12\n\n\n2\n12/31/2015 0:03\n12/31/2015 0:40\nResidential Building/House\n10026.0\nNEW YORK\nMANHATTAN\n40.799415\n-73.953371\n0\n12\n\n\n3\n12/31/2015 0:03\n12/31/2015 1:53\nResidential Building/House\n11231.0\nBROOKLYN\nBROOKLYN\n40.678285\n-73.994668\n0\n12\n\n\n4\n12/31/2015 0:05\n12/31/2015 3:49\nResidential Building/House\n10033.0\nNEW YORK\nMANHATTAN\n40.850304\n-73.938516\n0\n12\n\n\n\n\n\n\n\nBelow, we will begin with basic plotting, utilizing Matplotlib’s object-oriented interface to handle more complex tasks, such as setting the major axis formatting. When it comes to advanced customization, Matplotlib’s object-oriented interface offers greater flexibility and control compared to the pyplot interface.\n\n10.2.4.1 Bar plots with Pandas\nPurpose of bar plots: Barplots are used to visualize any aggregate statistics of a continuous variable with respect to the categories or levels of a categorical variable.\nBar plots can be made using the pandas bar function with the DataFrame or Series, just like the line plots and scatterplots.\nLet us visualise the locations from where the the complaints are coming.\n\nax = nyc_party_complaints['Location Type'].value_counts().plot.bar(ylabel = 'Number of complaints')\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n\n\n\nIn the above code, we use ax.yaxis.set_major_formatter to format the y-axis labels in a currency style. From the above plot, we observe that most of the complaints come from residential buildings and houses, as one may expect.\nFor categorical variables, we can use the .value_counts() method to get the statistical frequency of each unique value.\n\nnyc_party_complaints['Location Type'].value_counts()\n\nLocation Type\nResidential Building/House    146040\nStreet/Sidewalk                42353\nStore/Commercial               17617\nClub/Bar/Restaurant            15766\nPark/Playground                 3036\nHouse of Worship                 602\nName: count, dtype: int64\n\n\nNext, Let is visualize the time of the year when most complaints occur.\n\nnyc_party_complaints['Month_of_the_year'].value_counts()\n\nMonth_of_the_year\n6     25933\n5     25192\n9     25000\n7     24502\n8     20833\n10    19332\n4     17718\n12    15730\n11    14146\n3     13880\n1     12171\n2     10977\nName: count, dtype: int64\n\n\n\n#Using the pandas function bar() to create bar plot\nax = nyc_party_complaints['Month_of_the_year'].value_counts().sort_index().plot.bar(ylabel = 'Number of complaints',\n                                                                              xlabel = \"Month\")\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n\n\n\nTry executing the code without sort_index() to figure out the purpose of using the function.\nFrom the above plot, we observe that most of the complaints occur during summer and early Fall.\nLet us create a stacked bar chart that combines both the above plots into a single plot. You may ignore the code used for re-shaping the data until Chapter 10. The purpose here is to show the utility of the pandas bar() function.\n\n#Reshaping the data to make it suitable for a stacked barplot - ignore this code until chapter 8\ncomplaints_location=pd.crosstab(nyc_party_complaints.Month_of_the_year, nyc_party_complaints['Location Type'])\ncomplaints_location.head()\n\n\n\n\n\n\n\nLocation Type\nClub/Bar/Restaurant\nHouse of Worship\nPark/Playground\nResidential Building/House\nStore/Commercial\nStreet/Sidewalk\n\n\nMonth_of_the_year\n\n\n\n\n\n\n\n\n\n\n1\n748\n24\n17\n9393\n1157\n832\n\n\n2\n570\n29\n16\n8383\n1197\n782\n\n\n3\n747\n39\n90\n9689\n1480\n1835\n\n\n4\n848\n53\n129\n11984\n1761\n2943\n\n\n5\n2091\n72\n322\n15676\n1941\n5090\n\n\n\n\n\n\n\n\n#Stacked bar plot showing number of complaints at different months of the year, and from different locations\nax = complaints_location.plot.bar(stacked=True,ylabel = 'Number of complaints',figsize=(15, 10), xlabel = 'Month')\nax.tick_params(axis = 'both',labelsize=15)\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n\n\n\nThe above plots gives the insights about location and day of the year simultaneously that were previously separately obtained by the individual plots.\nAn alternative to stacked barplots are side-by-side barplots, as shown below.\n\n#Side-by-side bar plot showing number of complaints at different months of the year, and from different locations\nax = complaints_location.plot.bar(ylabel = 'Number of complaints',figsize=(15, 10), xlabel = 'Month')\nax.tick_params(axis = 'both',labelsize=15)\nax.yaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n\n\n\nQuestion: In which scenarios should we use a stacked barplot instead of a side-by-side barplot and vice-versa?\n\n\n10.2.4.2 Bar plots with confidence intervals with Seaborn\nWe’ll group the data to obtain the total complaints for each Location Type, Borough, Month_of_the_year, and Hour_of_the_day. Note that you’ll learn grouping data in later chapters, so you may ignore the next code block. The grouping is done to shape the data in a suitable form for visualization.\n\n#Grouping the data to make it suitable for visualization using Seaborn. Ignore this code block until learn chapter 9.\nnyc_complaints_grouped = nyc_party_complaints[['Location Type','Borough','Month_of_the_year','Latitude','Hour_of_the_day']].groupby(['Location Type','Borough','Month_of_the_year','Hour_of_the_day'])['Latitude'].agg([('complaints','count')]).reset_index()\nnyc_complaints_grouped.head()\n\n\n\n\n\n\n\n\nLocation Type\nBorough\nMonth_of_the_year\nHour_of_the_day\ncomplaints\n\n\n\n\n0\nClub/Bar/Restaurant\nBRONX\n1\n0\n10\n\n\n1\nClub/Bar/Restaurant\nBRONX\n1\n1\n10\n\n\n2\nClub/Bar/Restaurant\nBRONX\n1\n2\n6\n\n\n3\nClub/Bar/Restaurant\nBRONX\n1\n3\n6\n\n\n4\nClub/Bar/Restaurant\nBRONX\n1\n4\n3\n\n\n\n\n\n\n\nLet us create a bar plot visualizing the average number of complaints with the time of the day.\n\nax = sns.barplot(x=\"Hour_of_the_day\", y = 'complaints',  data=nyc_complaints_grouped)\nax.figure.set_figwidth(15)\n\n\n\n\n\n\n\n\nFrom the above plot, we observe that most of the complaints are made around midnight. However, interestingly, there are some complaints at each hour of the day.\nNote that the above barplot shows the mean number of complaints in a month at each hour of the day. The black lines are the 95% confidence intervals of the mean number of complaints.\n\n\n\n10.2.5 pyplot: a convenience wrapper around the object-oriented interface\nWhile the pyplot interface is simpler for quick, basic plots, it ultimately wraps around the object-oriented structure of Matplotlib, meaning that it’s built on top of the object-oriented interface.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "Advanced_Data_Visualization.html#creating-subplots-with-seaborn",
    "href": "Advanced_Data_Visualization.html#creating-subplots-with-seaborn",
    "title": "10  Advanced Data Visualization",
    "section": "10.3 Creating Subplots with Seaborn",
    "text": "10.3 Creating Subplots with Seaborn\nWe previously demonstrated how Seaborn integrates seamlessly with Matplotlib’s object-oriented interface, allowing you to pass the ax argument to any Seaborn function, thereby directing the plot to a specific axis within a subplot grid.\nAdditionally, Seaborn offers a more convenient and simplified approach to creating subplots, thanks to its high-level functions and built-in integration with Matplotlib. Here’s how Seaborn makes working with subplots easier:\n\n10.3.1 Using Facetgrid\nSeaborn’s FacetGrid function make it very easy to create facet grids or subplots based on data dimensions (such as categories), which would require more manual effort with Matplotlib.\nYou can use the row and col parameters to control how the data is split into subplots along these dimensions.\n\n# Seaborn Example using FacetGrid:\ntips_df = sns.load_dataset(\"tips\")\ntips_df.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\ng = sns.FacetGrid(tips_df, col='time', row='smoker')\ng.map(sns.histplot, 'total_bill', color='r')\ng.set_titles(col_template=\"{col_name}\", row_template=\"Smoker: {row_name}\");\n\n\n\n\n\n\n\n\n\n# adding hue to the FacetGrid\ng = sns.FacetGrid(tips_df, col='time', row='smoker',hue='size')\n# Plot a scatterplot of the total bill and tip for each combination of time and smoker\ng.map(sns.scatterplot, 'total_bill', 'tip')\ng.set_titles(col_template=\"{col_name}\", row_template=\"Smoker: {row_name}\");\n\n\n\n\n\n\n\n\n\n#Visualizing the number of complaints with Month_of_the_year, Location Type, and Borough.\na = sns.FacetGrid(nyc_complaints_grouped, hue = 'Location Type', col = 'Borough',col_wrap=3,height=3.5,aspect = 1)\n# Plotting a lineplot to show the number of complaints with Month_of_the_year\na.map(sns.lineplot,'Month_of_the_year','complaints')\na.set_axis_labels(\"Month of the year\", \"Complaints\")\na.add_legend()\n\n\n\n\n\n\n\n\n\n\n10.3.2 Using Pairplot\nPairplots are used to visualize the association between all variable-pairs in the data. In other words, pairplots simultaneously visualize the scatterplots between all variable-pairs.\nLet us visualize the pair-wise association of tips variables in the tips dataset\n\nsns.pairplot(tips_df );\n\n\n\n\n\n\n\n\nLet us visualize the pair-wise association of nutrition variables in the starbucks drinks data.\n\nstarbucks_drinks = pd.read_csv('datasets/starbucks-menu-nutrition-drinks.csv')\nsns.pairplot(starbucks_drinks);\n\n\n\n\n\n\n\n\nIn the above pairplot, note that:\n\nThe histograms on the diagonal of the grid show the distribution of each of the variables.\nInstead of a histogram, we can visualize the density plot with the argument kde = True.\nThe scatterplots in the rest of the grid are the pair-wise plots of all the variables.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "Advanced_Data_Visualization.html#geosptial-plotting",
    "href": "Advanced_Data_Visualization.html#geosptial-plotting",
    "title": "10  Advanced Data Visualization",
    "section": "10.4 Geosptial Plotting",
    "text": "10.4 Geosptial Plotting\nThere are several widely used Python packages pecifically designed for working with geospatial datasets. In this lesson, we will cover:\n\nGeoPandas\nFolium\n\nLet’s import them\n\nimport geopandas as gpd\nimport geopandas \nimport folium\nimport geodatasets\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n  \"class\": algorithms.Blowfish,\n\n\n\n10.4.1 Static Plots with GeoPandas\nA shapefile is a widely-used format for storing geographic information system (GIS) data, specifically vector data. It contains geometries (like points, lines, and polygons) that represent features on the earth’s surface, along with associated attributes for each feature, such as names, populations, or other data relevant to the feature.\n\n10.4.1.1 Components of a Shapefile\nA shapefile isn’t a single file but a collection of files with the same name and different extensions, which work together to store geographic and attribute data:\n\n.shp: Stores the geometry (shapes of features, like points, lines, polygons).\n.shx: Contains an index to quickly access geometries in the .shp file.\n.dbf: A table storing attributes associated with each feature.\n\nThere may also be other optional files (e.g., .prj for projection information).\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(15, 10))\n\n# Plot your GeoDataFrame\nchicago = gpd.read_file(r'datasets/chicago_boundaries\\geo_export_26bce2f2-c163-42a9-9329-9ca6e082c5e9.shp')\nchicago.plot(column='community', ax=ax, legend=True, legend_kwds={'ncol': 2, 'bbox_to_anchor': (2, 1)})\n\n# Add title (optional)\nplt.title('Chicago Community Areas');\n\n\n\n\n\n\n\n\nLet’s print out the information in the shapefile\n\nchicago.head()\n\n\n\n\n\n\n\n\narea\narea_num_1\narea_numbe\ncomarea\ncomarea_id\ncommunity\nperimeter\nshape_area\nshape_len\ngeometry\n\n\n\n\n0\n0.0\n35\n35\n0.0\n0.0\nDOUGLAS\n0.0\n4.600462e+07\n31027.054510\nPOLYGON ((-87.60914 41.84469, -87.60915 41.844...\n\n\n1\n0.0\n36\n36\n0.0\n0.0\nOAKLAND\n0.0\n1.691396e+07\n19565.506153\nPOLYGON ((-87.59215 41.81693, -87.59231 41.816...\n\n\n2\n0.0\n37\n37\n0.0\n0.0\nFULLER PARK\n0.0\n1.991670e+07\n25339.089750\nPOLYGON ((-87.6288 41.80189, -87.62879 41.8017...\n\n\n3\n0.0\n38\n38\n0.0\n0.0\nGRAND BOULEVARD\n0.0\n4.849250e+07\n28196.837157\nPOLYGON ((-87.60671 41.81681, -87.6067 41.8165...\n\n\n4\n0.0\n39\n39\n0.0\n0.0\nKENWOOD\n0.0\n2.907174e+07\n23325.167906\nPOLYGON ((-87.59215 41.81693, -87.59215 41.816...\n\n\n\n\n\n\n\n\nchicago['geometry'].head()\n\n0    POLYGON ((-87.60914 41.84469, -87.60915 41.844...\n1    POLYGON ((-87.59215 41.81693, -87.59231 41.816...\n2    POLYGON ((-87.6288 41.80189, -87.62879 41.8017...\n3    POLYGON ((-87.60671 41.81681, -87.6067 41.8165...\n4    POLYGON ((-87.59215 41.81693, -87.59215 41.816...\nName: geometry, dtype: geometry\n\n\n\n# Check the column names to see available data fields\nprint(\"Columns in the shapefile:\", chicago.columns)\n\n# Check the data types of each column\nprint(\"Data types:\", chicago.dtypes)\n\n# View the spatial extent (bounding box) of the shapes\nprint(\"Bounding box:\", chicago.total_bounds)\n\n# Check the coordinate reference system (CRS)\nprint(\"CRS:\", chicago.crs)\n\nColumns in the shapefile: Index(['area', 'area_num_1', 'area_numbe', 'comarea', 'comarea_id',\n       'community', 'perimeter', 'shape_area', 'shape_len', 'geometry'],\n      dtype='object')\nData types: area           float64\narea_num_1      object\narea_numbe      object\ncomarea        float64\ncomarea_id     float64\ncommunity       object\nperimeter      float64\nshape_area     float64\nshape_len      float64\ngeometry      geometry\ndtype: object\nBounding box: [-87.94011408  41.64454312 -87.5241371   42.02303859]\nCRS: EPSG:4326\n\n\nTo enhance the geospatial plot, we’ll use the shapefile as a background to provide context for Chicago’s community areas. On top of that, we’ll layer points of interest, such as restaurants, and shops, to illustrate the city’s amenities. This approach will make the map more informative and visually engaging, with community boundaries as the foundation and key locations overlayed to highlight areas of interest.\nNext, we will add the Divvy bicycle stations on top of the chicago shapefile\n\n\n\n10.4.2 Dataset: Bicycle Sharing in Chicago\n\n\n\nDivvy is Chicagoland’s bike share system (in collaboration with Chicago Department of Transportation), with 6,000 bikes available at 570+ stations across Chicago and Evanston. Divvy provides residents and visitors with a convenient, fun and affordable transportation option for getting around and exploring Chicago.\nDivvy, like other bike share systems, consists of a fleet of specially designed, sturdy and durable bikes that are locked into a network of docking stations throughout the region. The bikes can be unlocked from one station and returned to any other station in the system. People use bike share to explore Chicago, commute to work or school, run errands, get to appointments or social engagements, and more.\nDivvy is available for use 24 hours/day, 7 days/week, 365 days/year, and riders have access to all bikes and stations across the system.\nWe will be using divvy trips in the year of 2013\n\n# read the csv file'divvy_2013.csv' into pandas pandas dataframe\ndata = pd.read_csv('datasets/divvy_2013.csv')\ndata.head()\n\n\n\n\n\n\n\n\ntrip_id\nusertype\ngender\nstarttime\nstoptime\ntripduration\nfrom_station_id\nfrom_station_name\nlatitude_start\nlongitude_start\n...\ndewpoint\nhumidity\npressure\nvisibility\nwind_speed\nprecipitation\nevents\nrain\nconditions\nmonth\n\n\n\n\n0\n3940\nSubscriber\nMale\n2013-06-27 01:06:00\n2013-06-27 09:46:00\n31177\n91\nClinton St & Washington Blvd\n41.88338\n-87.641170\n...\n64.9\n96.0\n29.75\n7.0\n0.0\n-9999.0\npartlycloudy\n0\nScattered Clouds\n6\n\n\n1\n4095\nSubscriber\nMale\n2013-06-27 12:06:00\n2013-06-27 12:11:00\n301\n85\nMichigan Ave & Oak St\n41.90096\n-87.623777\n...\n69.1\n55.0\n29.75\n10.0\n13.8\n-9999.0\nmostlycloudy\n0\nMostly Cloudy\n6\n\n\n2\n4113\nSubscriber\nMale\n2013-06-27 11:09:00\n2013-06-27 11:11:00\n140\n88\nMay St & Randolph St\n41.88397\n-87.655688\n...\n70.0\n61.0\n29.75\n10.0\n10.4\n-9999.0\nmostlycloudy\n0\nMostly Cloudy\n6\n\n\n3\n4118\nCustomer\nNaN\n2013-06-27 12:11:00\n2013-06-27 12:16:00\n316\n85\nMichigan Ave & Oak St\n41.90096\n-87.623777\n...\n69.1\n55.0\n29.75\n10.0\n13.8\n-9999.0\nmostlycloudy\n0\nMostly Cloudy\n6\n\n\n4\n4119\nSubscriber\nMale\n2013-06-27 11:12:00\n2013-06-27 11:13:00\n87\n88\nMay St & Randolph St\n41.88397\n-87.655688\n...\n70.0\n61.0\n29.75\n10.0\n10.4\n-9999.0\nmostlycloudy\n0\nMostly Cloudy\n6\n\n\n\n\n5 rows × 28 columns\n\n\n\nIn the Divvy dataset, each trip record includes the latitude and longitude coordinates of both the pickup and drop-off locations, which correspond to Divvy bike stations. These coordinates allow us to map the precise locations of each station, making it possible to visually display the network of Divvy stations across the city. By plotting these stations on a map, we can better understand the geographic distribution and accessibility of Divvy’s bike-sharing network.\nBelow are the basic data cleaning steps to extract the coordinates of the Divvy stations.\n\n# drop the duplicates in the column 'to_station_id', 'to_station_name', 'latitude_end', 'longitude_end'\n# data_station_same = data[['from_station_id', 'from_station_name', 'latitude_start', 'longitude_start', 'to_station_id', 'to_station_name', 'latitude_end', 'longitude_end']].drop_duplicates()\n# data_station_same.shape\n\n\n\n10.4.3 Adding the divvy station to the plot\nOnce the coordinates are prepared, we’ll add them as scatter plots on top of the Chicago shapefile\n\n# Adding the stations to the plot\nfig, ax = plt.subplots(figsize=(15, 10))\n\nchicago = gpd.read_file(r'datasets/chicago_boundaries\\geo_export_26bce2f2-c163-42a9-9329-9ca6e082c5e9.shp')\nchicago.plot(column='community', ax=ax, legend=True, legend_kwds={'ncol': 2, 'bbox_to_anchor': (2, 1)})\n\n# Plot the stations\nlonglat_df = data[[ 'latitude_start', 'longitude_start']].drop_duplicates()\n\nplt.scatter(longlat_df['longitude_start'], longlat_df['latitude_start'], s=10, alpha=0.5, color='black', marker='o')\n\n\n# Add title (optional)\nplt.title('Chicago Community Areas');\n\n\n\n\n\n\n\n\n\n\n10.4.4 Change the chicago shapefile\nUsing a different Chicago shapefile from GeoDa is a great way to observe how geographic boundaries or data details may vary\n\nchicago = gpd.read_file(geodatasets.get_path(\"geoda.chicago_commpop\"))\n\n# Plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\nchicago.boundary.plot(ax=ax)\nplt.scatter(data['longitude_start'], data['latitude_start'], s=10, alpha=0.5, color='black', marker='o')\nplt.title('Chicago Community Areas');\n\n\n\n\n\n\n\n\n\n\n10.4.5 Interactive Plotting\nAlongside static plots, geopandas can create interactive maps based on the folium library.\nCreating maps for interactive exploration mirrors the API of static plots in an explore() method of a GeoSeries or GeoDataFrame.\nHere’s an explanation of how explore() works and its key features:\nKey Features of explore():\n\nInteractive Map Display:\n\n\nWhen you call explore() on a Geodataframe (gdf), it launches an interactive map widget directly within your Jupyter notebook.\nThis map allows you to pan, zoom, and interact with the geometries (points, lines, polygons) in your Geodataframe.\n\n\nLayer Control:\n\n\nexplore() automatically adds the geometries from your Geodataframe as layers on the map.\nEach geometry type (points, lines, polygons) is displayed with appropriate styling and markers.\n\n\nTooltip Information:\n\n\nWhen you hover over a geometry in the map, explore() displays tooltip information that typically includes attribute data associated with that geometry.\nThis feature is useful for inspecting specific details or properties of individual features in your geospatial dataset.\n\n\nSearch and Filter:\n\n\nexplore() provides basic search and filter functionalities directly on the map.\nYou can search for specific attribute values or filter the displayed features based on attribute criteria defined in your Geodataframe.\n\n\nCustomization:\n\n\nAlthough explore() provides default styling and interaction behaviors, you can customize the map further using parameters or by manipulating the Geodataframe before calling explore().\n\n\n# use the geopandas explore default settings\nchicago = gpd.read_file(geodatasets.get_path(\"geoda.chicago_commpop\"))\n\nchicago.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAdding the population layer\n\n# Customerize the explore settings\nchicago = gpd.read_file(geodatasets.get_path(\"geoda.chicago_commpop\"))\n\nm = chicago.explore(\n    column=\"POP2010\",  # make choropleth based on \"POP2010\" column\n    scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend=True,  # show legend\n    k=10,  # use 10 bins\n    tooltip=False,  # hide tooltip\n    popup=[\"POP2010\", \"POP2000\"],  # show popup (on-click)\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"chicago\",  # name of the layer in the map\n)\n\nm\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe explore() method returns a folium.Map object, which can also be passed directly (as you do with ax in plot()). You can then use folium functionality directly on the resulting map. Next, let’s add the divvy station plot.\n\ntype(m)\n\nfolium.folium.Map\n\n\n\n\n10.4.6 Adding the divvy station on the interactive Folium.Map\nWe need to extract the station information from the trip dataset and add description to the station. You can skip this part\n\n# Helper function for adding the description to the station\ndef row_to_html(row):\n    row_df = pd.DataFrame(row).T\n    row_df.columns = [col.capitalize() for col in row_df.columns]\n    return row_df.to_html(index=False)\n\n\n# Extracting the latitude, longitude, and station name for plotting, and also counting the number of trips from each station\ngrouped_df = data.groupby(['from_station_name', 'latitude_start', 'longitude_start'])['trip_id'].count().reset_index()\ndisplay(grouped_df.sort_values('trip_id', ascending=False).head())\ngrouped_df.rename(columns={'from_station_name':'title', 'latitude_start':'latitude', 'longitude_start':'longitude', 'trip_id':'count'}, inplace=True)\ngrouped_df['description'] = grouped_df.apply(lambda row: row_to_html(row), axis=1)\ngeometry = gpd.points_from_xy(grouped_df['longitude'], grouped_df['latitude'])\ngeo_df = gpd.GeoDataFrame(grouped_df, geometry=geometry)\n# Optional: Assign Coordinate Reference System (CRS)\ngeo_df.crs = \"EPSG:4326\"  # WGS84 coordinate system\ngeo_df.head()\n\n\n\n\n\n\n\n\nfrom_station_name\nlatitude_start\nlongitude_start\ntrip_id\n\n\n\n\n75\nMillennium Park\n41.881032\n-87.624084\n207\n\n\n54\nLake Shore Dr & Monroe St\n41.881050\n-87.616970\n191\n\n\n72\nMichigan Ave & Oak St\n41.900960\n-87.623777\n186\n\n\n68\nMcClurg Ct & Illinois St\n41.891020\n-87.617300\n177\n\n\n73\nMichigan Ave & Pearson St\n41.897660\n-87.623510\n127\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\nlatitude\nlongitude\ncount\ndescription\ngeometry\n\n\n\n\n0\nAberdeen St & Jackson Blvd\n41.877726\n-87.654787\n28\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n &lt;thead...\nPOINT (-87.65479 41.87773)\n\n\n1\nAberdeen St & Madison St\n41.881487\n-87.654752\n28\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n &lt;thead...\nPOINT (-87.65475 41.88149)\n\n\n2\nAdler Planetarium\n41.866095\n-87.607267\n6\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n &lt;thead...\nPOINT (-87.60727 41.8661)\n\n\n3\nAshland Ave & Armitage Ave\n41.917859\n-87.668919\n20\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n &lt;thead...\nPOINT (-87.66892 41.91786)\n\n\n4\nAshland Ave & Augusta Blvd\n41.899643\n-87.667700\n27\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n &lt;thead...\nPOINT (-87.6677 41.89964)\n\n\n\n\n\n\n\nWe can add a hover tooltip (sometimes referred to as a tooltip or tooltip popup) for each point on your Folium map. This tooltip will appear when you hover over the markers on the map, providing additional information without needing to click on them. Here’s how you can modify your existing code to include hover tooltips:\n\nchicago = gpd.read_file(geodatasets.get_path(\"geoda.chicago_commpop\"))\n\nm = chicago.explore(\n    column=\"POP2010\",  # make choropleth based on \"POP2010\" column\n    scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend=True,  # show legend\n    k=10,  # use 10 bins\n    tooltip=False,  # hide tooltip\n    popup=[\"POP2010\", \"POP2000\"],  # show popup (on-click)\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"chicago\",  # name of the layer in the map\n)\n\ngeo_df.explore(\n    m=m,  # pass the map object\n    color=\"red\",  # use red color on all points\n    marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill\n    tooltip=\"description\",  # show \"name\" column in the tooltip\n    tooltip_kwds=dict(labels=False),  # do not show column label in the tooltip\n    name=\"divstation\",  # name of the layer in the map\n)\n \nm\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "Advanced_Data_Visualization.html#independent-study",
    "href": "Advanced_Data_Visualization.html#independent-study",
    "title": "10  Advanced Data Visualization",
    "section": "10.5 Independent Study",
    "text": "10.5 Independent Study\n\n10.5.1 Multiple plots in a single figure using Seaborn\nPurpose: Histogram and density plots visualize the distribution of a continuous variable.\nA histogram plots the number of observations occurring within discrete, evenly spaced bins of a random variable, to visualize the distribution of the variable. It may be considered a special case of a bar plot as bars are used to plot the observation counts.\nA density plot uses a kernel density estimate to approximate the distribution of random variable.\nUsing the tips_df dataset\n\n10.5.1.1 \nMake a histogram showing the distributions of total bill on each day of the week\n\n\n10.5.1.2 \nMake a density plot showing the distributions of total bills on each day.",
    "crumbs": [
      "Exploratory data analysis: Core tools",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "Data cleaning and preparation.html",
    "href": "Data cleaning and preparation.html",
    "title": "11  Data Cleaning and Preparation",
    "section": "",
    "text": "11.1 Handling missing data\nMissing values in a dataset can occur due to several reasons such as breakdown of measuring equipment, accidental removal of observations, lack of response by respondents, error on the part of the researcher, etc.\nLet us read the dataset GDP_missing_data.csv, in which we have randomly removed some values, or put missing values in some of the columns.\nWe’ll also read GDP_complete_data.csv, in which we have not removed any values. We’ll use this data later to assess the accuracy of our guess or estimate of missing values in GDP_missing_data.csv.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport seaborn as sns\n\nsns.set(font_scale=1.5)\n\n%matplotlib inline\ngdp_missing_values_data = pd.read_csv('./Datasets/GDP_missing_data.csv')\ngdp_complete_data = pd.read_csv('./Datasets/GDP_complete_data.csv')\ngdp_missing_values_data.head()\n\n\n\n\n\n\n\n\neconomicActivityFemale\ncountry\nlifeMale\ninfantMortality\ngdpPerCapita\neconomicActivityMale\nilliteracyMale\nilliteracyFemale\nlifeFemale\ngeographic_location\ncontraception\ncontinent\n\n\n\n\n0\n7.2\nAfghanistan\n45.0\n154.0\n2474.0\n87.5\nNaN\n85.0\n46.0\nSouthern Asia\nNaN\nAsia\n\n\n1\n7.8\nAlgeria\n67.5\n44.0\n11433.0\n76.4\n26.1\n51.0\n70.3\nNorthern Africa\nNaN\nAfrica\n\n\n2\n41.3\nArgentina\n69.6\n22.0\nNaN\n76.2\n3.8\n3.8\n76.8\nSouth America\nNaN\nSouth America\n\n\n3\n52.0\nArmenia\n67.2\n25.0\n13638.0\n65.0\nNaN\n0.5\n74.0\nWestern Asia\nNaN\nAsia\n\n\n4\n53.8\nAustralia\nNaN\n6.0\n54891.0\nNaN\n1.0\n1.0\n81.2\nOceania\nNaN\nOceania\nObserve that the gdp_missing_values_data dataset consists of some missing values shown as NaN (Not a Number).",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning and Preparation</span>"
    ]
  },
  {
    "objectID": "Data cleaning and preparation.html#handling-missing-data",
    "href": "Data cleaning and preparation.html#handling-missing-data",
    "title": "11  Data Cleaning and Preparation",
    "section": "",
    "text": "11.1.1 Identifying missing values in a dataframe\nThere are multiple ways to identify missing values in a dataframe\n\n11.1.1.1 describe() Method\nNote that the descriptive statistics methods associated with Pandas objects ignore missing values by default. Consider the summary statistics of gdp_missing_values_data:\n\ngdp_missing_values_data.describe()\n\n\n\n\n\n\n\n\neconomicActivityFemale\nlifeMale\ninfantMortality\ngdpPerCapita\neconomicActivityMale\nilliteracyMale\nilliteracyFemale\nlifeFemale\ncontraception\n\n\n\n\ncount\n145.000000\n145.000000\n145.000000\n145.000000\n145.000000\n145.000000\n145.000000\n145.000000\n84.000000\n\n\nmean\n45.935172\n65.491724\n37.158621\n24193.482759\n76.563448\n13.570028\n21.448897\n70.615862\n51.773810\n\n\nstd\n16.875922\n9.099256\n34.465699\n22748.764444\n7.854730\n16.497954\n25.497045\n9.923791\n31.930026\n\n\nmin\n1.900000\n36.000000\n3.000000\n772.000000\n51.200000\n0.000000\n0.000000\n39.100000\n0.000000\n\n\n25%\n35.500000\n62.900000\n10.000000\n6837.000000\n72.000000\n1.000000\n2.300000\n67.500000\n17.000000\n\n\n50%\n47.600000\n67.800000\n24.000000\n15184.000000\n77.300000\n6.600000\n9.720000\n73.900000\n65.000000\n\n\n75%\n55.900000\n72.400000\n54.000000\n35957.000000\n81.600000\n19.500000\n30.200000\n78.100000\n77.000000\n\n\nmax\n90.600000\n77.400000\n169.000000\n122740.000000\n93.000000\n70.500000\n90.800000\n82.900000\n79.000000\n\n\n\n\n\n\n\nObserve that the count statistics report the number of non-missing values of each column in the data, as the number of rows in the data (see code below) is more than the number of non-missing values of all the variables in the above table. Similarly, for the rest of the statistics, such as mean, std, etc., the missing values are ignored.\n\n#The dataset gdp_missing_values_data has 155 rows\ngdp_missing_values_data.shape[0]\n\n155\n\n\n\n\n11.1.1.2 info() Method\nShows the count of non-null entries in each column, helping you quickly identify columns with missing values.\n\ngdp_missing_values_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 155 entries, 0 to 154\nData columns (total 12 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   economicActivityFemale  145 non-null    float64\n 1   country                 155 non-null    object \n 2   lifeMale                145 non-null    float64\n 3   infantMortality         145 non-null    float64\n 4   gdpPerCapita            145 non-null    float64\n 5   economicActivityMale    145 non-null    float64\n 6   illiteracyMale          145 non-null    float64\n 7   illiteracyFemale        145 non-null    float64\n 8   lifeFemale              145 non-null    float64\n 9   geographic_location     155 non-null    object \n 10  contraception           84 non-null     float64\n 11  continent               155 non-null    object \ndtypes: float64(9), object(3)\nmemory usage: 14.7+ KB\n\n\n\n\n11.1.1.3 isnull() Method\nThis is one of the most direct methods. Using df.isnull() returns a DataFrame of Boolean values where True indicates a missing value. To get a summary, you can use df.isnull().sum() to see the count of missing values in each column.\nFor finding the number of missing values in each column of gdp_missing_values_data, we will sum up the missing values in each column of the dataset:\n\ngdp_missing_values_data.isnull().sum()\n\neconomicActivityFemale    10\ncountry                    0\nlifeMale                  10\ninfantMortality           10\ngdpPerCapita              10\neconomicActivityMale      10\nilliteracyMale            10\nilliteracyFemale          10\nlifeFemale                10\ngeographic_location        0\ncontraception             71\ncontinent                  0\ndtype: int64\n\n\n\n\n\n11.1.2 Types of Missing Values\nIn data science, missing values typically fall into three main types, each requiring different handling strategies:\n\n11.1.2.1 Missing Completely at Random (MCAR)\n\nDefinition: Missing values are entirely independent of any variables in the dataset.\nExample: A respondent accidentally skips a question on a survey.\nImpact: MCAR data can usually be ignored or imputed without biasing the analysis.\nHandling: Simple imputation methods, like filling with mean or median values, are often appropriate.\n\n\n\n11.1.2.2 Missing at Random (MAR)\n\nDefinition: The likelihood of a value being missing is related to other observed variables but not to the missing data itself.\nExample: People with higher incomes may be less likely to report their spending, but income data itself is not missing.\nImpact: Ignoring MAR values may bias results, so careful imputation based on related variables is recommended.\nHandling: More complex imputation methods, like conditional mean imputation or predictive modeling, are suitable.\n\n\n\n11.1.2.3 Missing Not at Random (MNAR)\n\nDefinition: The probability of missingness is related to the missing data itself, meaning the value is systematically missing.\nExample: Patients with severe health conditions might be less likely to report their health status, or students with low scores may be less likely to submit their grades.\nImpact: MNAR is the most challenging type, as missing values may introduce significant bias.\nHandling: Solutions often include sensitivity analysis, data augmentation, or modeling techniques that account for the missing mechanism, though sometimes domain-specific approaches are necessary.\n\nUnderstanding the type of missing data helps in selecting the right imputation method and mitigating potential biases in the analysis.\n\n\n11.1.2.4 Questions\n\n11.1.2.4.1 \nWhy can we ignore observations with missing values without risking skewing the analysis or trends in the case of Missing Completely at Random (MCAR)?\n\n\n11.1.2.4.2 \nWhy could ignoring missing values lead to biased results for Missing at Random (MAR) and Missing Not at Random (MNAR) data?\n\n\n11.1.2.4.3 \nFor the datset consisting of GDP per capita, think of hypothetical scenarios in which the missing values of GDP per capita can correspond to MCAR / MAR / MNAR.\n\n\n\n\n11.1.3 Methods for Handling missing values\n\n11.1.3.1 Removing Missing values\n\nRow/Column Removal: Use [df.dropna()] (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)\n\nWhen to Use: if the missing values are few or the rows/columns are not critical.\nRisks: Can reduce the dataset’s size, potentially losing valuable information.\n\n\nLet us drop the rows containing even a single value from gdp_missing_values_data.\n\ngdp_no_missing_data = gdp_missing_values_data.dropna()\n\nBy default, df.dropna() will drop any row that contains at least one missing value, leaving only the rows that are completely free of missing values.\n\n#Shape of gdp_no_missing_data\ngdp_no_missing_data.shape\n\n(42, 12)\n\n\nUsing df.dropna() to remove rows with missing values can sometimes lead to a significant reduction in data, which can be problematic if much of the data is valuable and non-missing. For example:\n\nImpact of Default Behavior: Dropping rows with even a single missing value reduced the number of rows from 155 to 42! This drastic reduction happens because, by default, dropna() removes any row with at least one missing value, keeping only rows that are completely complete.\nLoss of Non-Missing Data: Even though some columns may have very few missing values (e.g., at most 10), using dropna() without modification results in losing all non-missing data in affected rows. This is typically a poor choice, especially when valuable, non-missing data is removed unnecessarily.\n\nTo avoid losing too much data, you can adjust the behavior of dropna() using these parameters:\n\nhow Parameter\n\nControls the criteria for dropping rows or columns.\nhow='any' (default): Drops rows or columns if any values are missing.\n`how=‘all’: Drops rows or columns only if all values are missing\n\nthresh Parameter\n\nSets a minimum number of non-missing values required to retain the row or column.\nUseful when you want to keep rows or columns with substantial, but not complete, data.\n\n\nIf a few values of a column are missing, we can possibly estimate them using the rest of the data, so that we can (hopefully) maximize the information that can be extracted from the data. However, if most of the values of a column are missing, it may be harder to estimate its values.\nIn this dataset, we see that around 50% values of the contraception column is missing. Thus, we’ll drop the column as it may be hard to impute its values based on a relatively small number of non-missing values.\n\n#Deleting column with missing values in almost half of the observations\ngdp_missing_values_data.drop(['contraception'],axis=1,inplace=True)\ngdp_missing_values_data.shape\n\n(155, 11)\n\n\n\n\n11.1.3.2 Imputing Missing values\nThere are an unlimited number of ways to impute missing values. Some imputation methods are provided in the Pandas documentation.\nThe best way to impute them will depend on the problem:\n\nFor MCAR: Simple imputation is generally acceptable.\nFor MAR: Imputation should consider relationships with other variables, such as using conditional mean imputation.\nFor MNAR: Imputation requires careful analysis, and domain knowledge is essential to avoid bias.\n\nBelow are some of the most common methods. Recall that we randomly introduced missing values in gdp_missing_values_data, while the actual values are preserved in gdp_complete_data.\nWe will apply these methods to gdp_missing_values_data. To evaluate each method’s effectiveness in imputing missing values, we’ll compare the imputed gdpPerCapita values with the actual values and calculate the Root Mean Square Error (RMSE).\nTo visualize the imputed vs actual values, let’s define a helper function\n\n#Index of rows with missing values for GDP per capita\nnull_ind_gdpPC = gdp_missing_values_data.index[gdp_missing_values_data.gdpPerCapita.isnull()]\n\n#Defining a function to plot the imputed values vs actual values \ndef plot_actual_vs_predicted(y):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    plt.rc('xtick', labelsize=15) \n    plt.rc('ytick', labelsize=15) \n    x = gdp_complete_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    # y = y.loc[null_ind_gdpPC,'gdpPerCapita']\n    plt.scatter(x,y.loc[null_ind_gdpPC,'gdpPerCapita'])\n    z=np.polyfit(x,y.loc[null_ind_gdpPC,'gdpPerCapita'],1)\n    p=np.poly1d(z)\n    plt.plot(x,x,color='orange')\n    plt.xlabel('Actual GDP per capita',fontsize=15)\n    plt.ylabel('Imputed GDP per capita',fontsize=15)\n    ax.xaxis.set_major_formatter('${x:,.0f}')\n    ax.yaxis.set_major_formatter('${x:,.0f}')\n    plt.rc('axes', labelsize=10)  # Set all axis labels to fontsize 10\n    # plt.title('Actual vs Imputed values for GDP per capita',fontsize=20)\n    rmse = np.sqrt(np.mean((y.loc[null_ind_gdpPC,'gdpPerCapita']-x)**2))\n    plt.text(10000, 50000, 'RMSE = %.2f'%rmse, fontsize=15)\n\n\n11.1.3.2.1 Mean/Median/Mode Imputation\nThis approach replaces missing values with the mean or median of the column.\nLet’s impute missing values in the column by substituting them with the average of the non-missing values. Imputing with the mean tends to minimize the sum of squared differences between actual values and imputed values, especially in cases where data is Missing Completely at Random (MCAR). However, this might not hold true for other types of missing data, such as Missing at Random (MAR) or Missing Not at Random (MNAR).\nLet us impute missing values in the column as the average of the non-missing values of the column. The sum of squared differences between actual values and the imputed values is likely to be smaller if we impute using the mean. However, this may not be true in cases other than MCAR (Missing completely at random).\n\n# Extracting the columns with missing values\ncolumns_with_missing = [col for col in gdp_missing_values_data.columns if gdp_missing_values_data[col].isnull().any()]\ncolumns_with_missing\n\n['economicActivityFemale',\n 'lifeMale',\n 'infantMortality',\n 'gdpPerCapita',\n 'economicActivityMale',\n 'illiteracyMale',\n 'illiteracyFemale',\n 'lifeFemale']\n\n\n\n# Imputing missing values using the mean\ngdp_imputed_data_mean = gdp_missing_values_data[columns_with_missing].fillna(gdp_missing_values_data[columns_with_missing].mean())\n\n\nplot_actual_vs_predicted(gdp_imputed_data_mean)\n\n\n\n\n\n\n\n\nHere, since all columns with missing values are numerical, we could use the mean to impute these values. Using the mean is generally suitable only for numerical data, as it represents a central tendency specific to numbers. For categorical data, however, mean imputation would be inappropriate. Instead, the mode, which identifies the most frequently occurring value, is a more suitable choice for imputing missing values in categorical columns.\n\n\n11.1.3.2.2 Conditional Imputation: Use other related variables to predict missing values\nThere are three approaches within this category; please see the details below.\n\nImputing missing values based on correlated variables in the data\n\nIf a variable is highly correlated with another variable in the dataset, we can approximate its missing values using the trendline with the highly correlated variable.\nLet us visualize the distribution of GDP per capita for different continents.\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\nsns.boxplot(x = 'continent',y='gdpPerCapita',data = gdp_missing_values_data);\n\n\n\n\n\n\n\n\nWe observe that there is a distinct difference between the GDPs per capita of some of the contents. Let us impute the missing GDP per capita of a country as the mean GDP per capita of the corresponding continent. This imputation should be better than imputing the missing GDP per capita as the mean of all the non-missing values, as the GDP per capita of a country is likely to be closer to the mean GDP per capita of the continent, rather the mean GDP per capita of the whole world.\n\n#Finding the mean GDP per capita of the continent - please defer the understanding of this code to chapter 9.\navg_gdpPerCapita = gdp_missing_values_data['gdpPerCapita'].groupby(gdp_missing_values_data['continent']).mean()\navg_gdpPerCapita\n\ncontinent\nAfrica            7638.178571\nAsia             25922.750000\nEurope           45455.303030\nNorth America    19625.210526\nOceania          15385.857143\nSouth America    15360.909091\nName: gdpPerCapita, dtype: float64\n\n\n\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data_group_mean = gdp_missing_values_data.copy()\n\n\n#Replacing missing GDP per capita with the mean GDP per capita for the corresponding continent\n\ngdp_imputed_data_group_mean.gdpPerCapita = \\\ngdp_imputed_data_group_mean['gdpPerCapita'].fillna(gdp_imputed_data_group_mean['continent'].map(avg_gdpPerCapita))\nplot_actual_vs_predicted(gdp_imputed_data_group_mean)\n\n\n\n\n\n\n\n\nNote that the imputed values are closer to the actual values, and the RMSE has further reduced as expected.\nSuppose we wish to impute the missing values of each numeric column with the average of the non-missing values of the respective column corresponding to the continent of the observation. The above logic can be extended to each column as shown in the code below.\n\nall_columns_imputed_data = gdp_missing_values_data.iloc[:,[0,2,3,4,5,6,7,8]].apply(lambda x:\\\nx.fillna(gdp_imputed_data_group_mean['continent'].map(x.groupby(gdp_missing_values_data['continent']).mean())))\n\nUsing Regression\n\n#Let us identify the variable highly correlated with GDP per capita.\ngdp_missing_values_data.select_dtypes(include='number').corrwith(gdp_missing_values_data.gdpPerCapita)\n\neconomicActivityFemale    0.078332\nlifeMale                  0.579850\ninfantMortality          -0.572201\ngdpPerCapita              1.000000\neconomicActivityMale     -0.134108\nilliteracyMale           -0.479143\nilliteracyFemale         -0.448273\nlifeFemale                0.615954\ndtype: float64\n\n\n\n#The variable *lifeFemale* has the strongest correlation with GDP per capita. Let us use it to impute missing values of GDP per capita.\n\n#  Extract the variables lifeFemale and GDP per capita\nx = gdp_missing_values_data.lifeFemale\ny = gdp_missing_values_data.gdpPerCapita\n\n#  Identify non-missing indices\nidx_non_missing = np.isfinite(x) & np.isfinite(y)\n\n# Fit a linear regression model (degree=1) to predict GDP per capita given lifeFemale\nslope_intercept_trendline = np.polyfit(x[idx_non_missing],y[idx_non_missing],1)   #Finding the slope and intercept for the trendline\ncompute_y_given_x = np.poly1d(slope_intercept_trendline)\n\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data_lr = gdp_missing_values_data.copy()\n\n#Imputing missing values of GDP per capita using the linear regression model\ngdp_imputed_data_lr.loc[null_ind_gdpPC,'gdpPerCapita']=compute_y_given_x(gdp_missing_values_data.loc[null_ind_gdpPC,'lifeFemale'])\n\n\nplot_actual_vs_predicted(gdp_imputed_data_lr)\n\n\n\n\n\n\n\n\nKNN: K-nearest neighbor\nIn this method, we’ll impute the missing value of the variable as the mean value of the \\(K\\)-nearest neighbors having non-missing values for that variable. The neighbors to a data-point are identified based on their Euclidean distance to the point in terms of the standardized values of rest of the variables in the data.\nLet’s consider a toy example to understand missing value imputation by KNN. Suppose we have to impute missing values in a toy dataset, named as toy_data having 4 observations and 3 variables.\n\n#Toy example - A 4x3 array with missing values\nnan = np.nan\ntoy_data = np.array([[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]])\ntoy_data\n\narray([[ 1.,  2., nan],\n       [ 3.,  4.,  3.],\n       [nan,  6.,  5.],\n       [ 8.,  8.,  7.]])\n\n\nWe’ll use some functions from the sklearn library to perform the KNN imputation. It is much easier to directly use the algorithm from sklearn, instead of coding it from scratch.\n\n#Library to compute pair-wise Euclidean distance between all observations in the data\nfrom sklearn import metrics\n\n#Library to impute missing values with the KNN algorithm\nfrom sklearn import impute\n\nWe’ll use the sklearn function nan_euclidean_distances() to compute the Euclidean distance between all pairs of observations in the data.\n\n#This is the distance matrix containing the distance of the ith observation from the jth observation at the (i,j) position in the matrix\nmetrics.pairwise.nan_euclidean_distances(toy_data,toy_data)\n\narray([[ 0.        ,  3.46410162,  6.92820323, 11.29158979],\n       [ 3.46410162,  0.        ,  3.46410162,  7.54983444],\n       [ 6.92820323,  3.46410162,  0.        ,  3.46410162],\n       [11.29158979,  7.54983444,  3.46410162,  0.        ]])\n\n\nNote that the size of the above matrix is 4x4. This is because the \\((i,j)^{th}\\) element of the matrix is the distance of the \\(i^{th}\\) observation from the \\(j^{th}\\) observation. The matrix is symmetric because the distance of \\(i^{th}\\) observation to the \\(j^{th}\\) observation is the same as the distance of the \\(j^{th}\\) observation to the \\(i^{th}\\) observation.\nWe’ll use the sklearn function KNNImputer() to impute the missing value of a column in toy_data as the mean of the values of the \\(K\\) nearest neighbors to the observation that have non-missing values for that column.\nLet us impute the missing values in toy_data using the values of \\(K=2\\) nearest neighbors from the corresponding observation.\n\n#imputing missing values with 2 nearest neighbors, where the neighbors have equal weights\n\n#Define an object of type KNNImputer\nimputer = impute.KNNImputer(n_neighbors=2)\n\n#Use the object method 'fit_transform' to impute missing values\nimputer.fit_transform(toy_data)\n\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n\nThe third observation was the closest to the \\(2nd\\) and \\(4th\\) observations based on the Euclidean distance matrix. Thus, the missing value in the \\(3rd\\) row of the toy_data has been imputed as the mean of the values in the \\(2nd\\) and \\(4th\\) observations for the corresponding column. Similarly, the \\(1st\\) observation is the closest to the \\(2nd\\) and \\(3rd\\) observations. Thus the missing value in the \\(1st\\) row of toy_data has been imputed as the mean of the values in the \\(1st\\) and \\(2nd\\) observations for the corresponding column.\nLet us use KNN to impute the missing values of gdpPerCapita in gdp_missing_values_data. We’ll use only the numeric columns of the data in imputing the missing values. Also, we’ll ignore contraception as it has a lot of missing values, and thus may not be useful.\n\n#Considering numeric columns in the data to use KNN\nnum_cols = list(range(0,1))+list(range(2,9))\nnum_cols\n\n[0, 2, 3, 4, 5, 6, 7, 8]\n\n\nBefore computing the pair-wise Euclidean distance of observations, we must standardize the data so that all columns are at the same scale. This will avoid columns with a higher magnitude of values having a higher weight in determining the Euclidean distance. Unless there is a reason to give a higher weight to a column, we assume all columns to have the same weight in the Euclidean distance computation.\nWe can use the code below to scale the data. However, after imputing the missing values, the data is to be scaled back to the original scale, so that each variable is in the same units as in the original dataset. However, if the code below is used, we’ll lose the orginal scale of each of the columns.\n\n#Scaling data to compute equally weighted distances from the 'k' nearest neighbors\nscaled_data = gdp_missing_values_data.iloc[:,num_cols].apply(lambda x:(x-x.min())/(x.max()-x.min()))\n\nTo alleviate the problem of losing the orignial scale of the data, we’ll use the MinMaxScaler object of the sklearn library. The object will store the original scale of the data, which will help transform the data back to the original scale once the missing values have been imputed in the standardized data.\n\n# Scaling data - using sklearn\n\n#Create an object of type MinMaxScaler\nscaler = sk.preprocessing.MinMaxScaler()\n\n#Use the object method 'fit_transform' to scale the values to a standard uniform distribution\nscaled_data = pd.DataFrame(scaler.fit_transform(gdp_missing_values_data.iloc[:,num_cols]))\n\n\n#Imputing missing values with KNNImputer\n\n#Define an object of type KNNImputer\nimputer = impute.KNNImputer(n_neighbors=3, weights=\"uniform\")\n\n#Use the object method 'fit_transform' to impute missing values\nimputed_arr = imputer.fit_transform(scaled_data)\n\n\n#Scaling back the scaled array to obtain the data at the original scale\n\n#Use the object method 'inverse_transform' to scale back the values to the original scale of the data\nunscaled_data = scaler.inverse_transform(imputed_arr)\n\n\n#Note the method imputes the missing value of all the columns\n#However, we are interested in imputing the missing values of only the 'gdpPerCapita' column\ngdp_imputed_data_knn = gdp_missing_values_data.copy()\ngdp_imputed_data_knn.loc[:,'gdpPerCapita'] = unscaled_data[:,3]\n\n\n#Visualizing the accuracy of missing value imputation with KNN\nplot_actual_vs_predicted(gdp_imputed_data_knn)\n\n\n\n\n\n\n\n\nNote that the RMSE is the lowest in this method. It is because this method imputes missing values as the average of the values of “similar” observations, which is smarter and more robust than the previous methods.\nWe chose \\(K=3\\) in the missing value imputation for GDP per capita. However, the value of \\(K\\) is typically chosen using a method known as cross validation. We’ll learn about cross-validation in the next course of the sequence.\n\n\n11.1.3.2.3 Forward Fill/Backward Fill\nTo fill missing values in a column by copying the value of the previous or next non-missing observation, we can use forward fill (propagating the last valid observation forward) or backward fill (propagating the next valid observation backward).\nBelow, we demonstrate forward fill. To perform backward fill instead, simply replace ffill with bfill\n\n#Filling missing values: Method 1- Naive way\ngdp_imputed_data_ffill = gdp_missing_values_data.ffill()\n\nLet us next check how good is this method in imputing missing values.\n\nplot_actual_vs_predicted(gdp_imputed_data_ffill)\n\n\n\n\n\n\n\n\nWe observe that the accuracy of imputation is poor as GDP per capita can vary a lot across countries, and the data is not sorted by GDP per capita. There is no reason why the GDP per capita of a country should be close to the GDP per capita of the country in the observation above it.\n\n#Checking if any missing values are remaining\ngdp_imputed_data_ffill.isnull().sum()\n\neconomicActivityFemale    0\ncountry                   0\nlifeMale                  0\ninfantMortality           0\ngdpPerCapita              0\neconomicActivityMale      0\nilliteracyMale            1\nilliteracyFemale          0\nlifeFemale                0\ngeographic_location       0\ncontinent                 0\ndtype: int64\n\n\nAfter imputing missing values, note there is still one missing value for illiteracyMale. Can you guess why one missing value remained?",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning and Preparation</span>"
    ]
  },
  {
    "objectID": "Data cleaning and preparation.html#outlier-detection-and-handling",
    "href": "Data cleaning and preparation.html#outlier-detection-and-handling",
    "title": "11  Data Cleaning and Preparation",
    "section": "11.2 Outlier detection and handling",
    "text": "11.2 Outlier detection and handling\nAn outlier is an observation that is significantly different from the rest of the data. Outlier detection and handling are important in data science because outliers can significantly impact the quality, accuracy, and interpretability of data analysis and model performance. Here are the main reasons why it’s crucial to address outliers:\n\nPreventing Skewed Results: Outliers can distort statistical measures, such as the mean and standard deviation, leading to misleading interpretations of the data. For example, a few extreme values can inflate the mean, making the data seem larger than it is in reality.\nImproving Model Accuracy: Many machine learning algorithms are sensitive to outliers and can perform poorly if outliers are present. For instance, in linear regression, outliers can disproportionately affect the model by pulling the line of best fit toward them, reducing predictive accuracy.\nEnhancing Robustness of Models: Identifying and handling outliers can make models more robust and stable. By minimizing the influence of extreme values, models become less sensitive to noise, which improves generalization and reduces overfitting.\n\n\n11.2.1 Outlier detection\nOutlier detection is crucial for identifying unusual values in your data that may need to be investigated, removed, or transformed. Here are several popular methods for detecting outliers:\nWe will use College.csv that contains information about US universities as our dataset in this section. The description of variables of the dataset can be found on page 65 of this book.\n\n# Load the College dataset\ncollege = pd.read_csv('./Datasets/College.csv')\ncollege.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nPrivate\nApps\nAccept\nEnroll\nTop10perc\nTop25perc\nF.Undergrad\nP.Undergrad\nOutstate\nRoom.Board\nBooks\nPersonal\nPhD\nTerminal\nS.F.Ratio\nperc.alumni\nExpend\nGrad.Rate\n\n\n\n\n0\nAbilene Christian University\nYes\n1660\n1232\n721\n23\n52\n2885\n537\n7440\n3300\n450\n2200\n70\n78\n18.1\n12\n7041\n60\n\n\n1\nAdelphi University\nYes\n2186\n1924\n512\n16\n29\n2683\n1227\n12280\n6450\n750\n1500\n29\n30\n12.2\n16\n10527\n56\n\n\n2\nAdrian College\nYes\n1428\n1097\n336\n22\n50\n1036\n99\n11250\n3750\n400\n1165\n53\n66\n12.9\n30\n8735\n54\n\n\n3\nAgnes Scott College\nYes\n417\n349\n137\n60\n89\n510\n63\n12960\n5450\n450\n875\n92\n97\n7.7\n37\n19016\n59\n\n\n4\nAlaska Pacific University\nYes\n193\n146\n55\n16\n44\n249\n869\n7560\n4120\n800\n1500\n76\n72\n11.9\n2\n10922\n15\n\n\n\n\n\n\n\n\n11.2.1.1 Visualization Methods\n\nBox Plot: A box plot is a quick and visual way to detect outliers, where points outside the “whiskers” are considered outliers.\n\nLet us visualize outliers in average instructional expenditure per student given by the variable Expend.\n\nax=college.boxplot(column = 'Expend')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\n\n\n\n\nThere are several outliers (shown as circles in the above boxplot), which correspond to high values of average instructional expenditure per student.\n\nHistogram: Histograms can reveal unusual values as isolated bars, helping to identify outliers in a single feature.\n\n\n# create a histogram of expend\ncollege['Expend'].plot(kind='hist', edgecolor='black', bins=20)\nplt.xlabel('Expenditure')\nplt.ylabel('Frequency')\nplt.title('Histogram of Expenditure')\nplt.show()\n\n\n\n\n\n\n\n\n\nScatter Plot: A scatter plot can reveal outliers, especially in two-dimensional data. Outliers will often appear as points separated from the main cluster.\n\nLet’s make a scatterplot of ‘Grad.Rate’ vs ‘Expend’ with a trendline, to visualize potential outliers\n\nsns.set(font_scale=1.5)\nax=sns.regplot(data = college, x = \"Expend\", y = \"Grad.Rate\",scatter_kws={\"color\": \"orange\"}, line_kws={\"color\": \"blue\"})\nax.xaxis.set_major_formatter('${x:,.0f}')\nax.set_xlabel('Expenditure per student')\nax.set_ylabel('Graduation rate')\nplt.show()\n\n\n\n\n\n\n\n\nThe trendline indicates a positive correlation between Expend and Grad.Rate. However, there seems to be a lot of noise and presence of outliers in the data.\n\n\n11.2.1.2 Statistical Methods\nVisualization helps us identify potential outliers. To address them effectively, we need to extract these outlier instances using a defined threshold. Two commonly used statistical methods for this purpose are the Z-Score method and the Interquartile Range (IQR) (Tukey’s fences).\n\n11.2.1.2.1 Z-Score (Standard Score)\nCalculates how many standard deviations a data point is from the mean. Commonly, data points with a Z-score greater than 3 (or less than -3) are considered outliers.\n\nfrom scipy import stats\n\n# Calculate Z-scores and identify outliers\ncollege['z_score'] = stats.zscore(college['Expend'])\ncollege['is_z_score_outlier'] = college['z_score'].abs() &gt; 3\n\n# Filter to show only the outliers\nz_score_outliers = college[college['is_z_score_outlier']]\nprint(z_score_outliers.shape)\nz_score_outliers.head()\n\n(16, 21)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nPrivate\nApps\nAccept\nEnroll\nTop10perc\nTop25perc\nF.Undergrad\nP.Undergrad\nOutstate\n...\nBooks\nPersonal\nPhD\nTerminal\nS.F.Ratio\nperc.alumni\nExpend\nGrad.Rate\nz_score\nis_z_score_outlier\n\n\n\n\n20\nAntioch University\nYes\n713\n661\n252\n25\n44\n712\n23\n15476\n...\n400\n1100\n69\n82\n11.3\n35\n42926\n48\n6.374709\nTrue\n\n\n144\nColumbia University\nYes\n6756\n1930\n871\n78\n96\n3376\n55\n18624\n...\n550\n300\n97\n98\n5.9\n21\n30639\n99\n4.020159\nTrue\n\n\n158\nDartmouth College\nYes\n8587\n2273\n1087\n87\n99\n3918\n32\n19545\n...\n550\n1100\n95\n99\n4.7\n49\n29619\n98\n3.824698\nTrue\n\n\n174\nDuke University\nYes\n13789\n3893\n1583\n90\n98\n6188\n53\n18590\n...\n625\n1162\n95\n96\n5.0\n44\n27206\n97\n3.362296\nTrue\n\n\n191\nEmory University\nYes\n8506\n4168\n1236\n76\n97\n5544\n192\n17600\n...\n600\n870\n97\n98\n5.0\n28\n28457\n96\n3.602024\nTrue\n\n\n\n\n5 rows × 21 columns\n\n\n\nBoxplot identifies outliers based on the Tukey’s fences criterion:\n\n\n11.2.1.2.2 IQR method (Tukey’s fences)\nJohn Tukey proposed that observations outside the range \\([Q1 - 1.5(Q3-Q1), Q3+1.5(Q3-Q1)]\\) are outliers, where \\(Q1\\) and \\(Q3\\) are the lower \\((25\\%)\\) and upper \\((75\\%)\\) quartiles respectively. Let us detect outliers based on this threshold.\n\n# Calculate the first and third quartiles, and the IQR\nQ1 = np.percentile(college['Expend'], 25)\nQ3 = np.percentile(college['Expend'], 75)\nIQR = Q3 - Q1\n\n# Define the lower and upper bounds for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Add a new column to indicate IQR outliers\ncollege['is_IQR_outlier'] = (college['Expend'] &lt; lower_bound) | (college['Expend'] &gt; upper_bound)\n\n# Filter to show only the IQR outliers\niqr_outliers = college[college['is_IQR_outlier']]\nprint(iqr_outliers.shape)\niqr_outliers.head()\n\n(48, 22)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nPrivate\nApps\nAccept\nEnroll\nTop10perc\nTop25perc\nF.Undergrad\nP.Undergrad\nOutstate\n...\nPersonal\nPhD\nTerminal\nS.F.Ratio\nperc.alumni\nExpend\nGrad.Rate\nz_score\nis_z_score_outlier\nis_IQR_outlier\n\n\n\n\n3\nAgnes Scott College\nYes\n417\n349\n137\n60\n89\n510\n63\n12960\n...\n875\n92\n97\n7.7\n37\n19016\n59\n1.792851\nFalse\nTrue\n\n\n16\nAmherst College\nYes\n4302\n992\n418\n83\n96\n1593\n5\n19760\n...\n1598\n93\n98\n8.4\n63\n21424\n100\n2.254295\nFalse\nTrue\n\n\n20\nAntioch University\nYes\n713\n661\n252\n25\n44\n712\n23\n15476\n...\n1100\n69\n82\n11.3\n35\n42926\n48\n6.374709\nTrue\nTrue\n\n\n60\nBowdoin College\nYes\n3356\n1019\n418\n76\n100\n1490\n8\n19030\n...\n875\n93\n96\n11.2\n52\n20447\n96\n2.067073\nFalse\nTrue\n\n\n64\nBrandeis University\nYes\n4186\n2743\n740\n48\n77\n2819\n62\n19380\n...\n1000\n90\n97\n9.8\n24\n17150\n84\n1.435271\nFalse\nTrue\n\n\n\n\n5 rows × 22 columns\n\n\n\nUsing the IQR method, more instances in the college dataset are marked as outliers.\n\n\n11.2.1.2.3 When to use each:\n\nUse IQR when:\n\nData is skewed\nYou don’t want to assume any distribution\nYou want a more sensitive detector\n\nUse Z-score when:\n\nData is approximately normal\nYou want a more conservative approach\nYou need a method that’s easily interpretable\n\n\nThe actual number of outliers marked by each method will depend on your specific dataset’s distribution and characteristics.\n\n\n\n\n11.2.2 Common Methods for Handling outliers\nOnce we identify outlier instances using statistical methods, the next step is to handle them. Below are some commonly used approaches:\n\nRemoving Outliers: Discarding extreme values if they are likely due to errors or are irrelevant for the analysis.\nWinsorizing: Capping/flooring outliers to a specific percentile to reduce their influence without removing them completely.\nReplacing with the median: Replacing outlier values with the median of the remaining data. The median is less affected by extreme values than the mean, making it an ideal choice for imputation when dealing with outliers\nTransforming Data: Applying transformations (e.g., log, square root) to reduce the impact of outliers.\n\n\ndef method_removal():\n    \"\"\"Method 1: Remove outliers\"\"\"\n    data_cleaned = college[~college['is_outlier']]['Expend']\n    return data_cleaned\n\n\ndef method_capping():\n    \"\"\"Method 2: Capping (Winsorization)\"\"\"\n    data_capped = college['Expend'].copy()\n    data_capped[data_capped &lt; lower_bound] = lower_bound\n    data_capped[data_capped &gt; upper_bound] = upper_bound\n    return data_capped\n\n\ndef method_mean_replacement():\n    \"\"\"Method 3: Replace with median\"\"\"\n    data_median = college['Expend'].copy()\n    median_value = college[~college['is_outlier']]['Expend'].median()\n    data_median[college['is_outlier']] = median_value\n    return data_median\n\n\ndef method_log_transformation():\n    \"\"\"Method 4: Log transformation\"\"\"\n    data_log = np.log1p(college['Expend'])\n    return data_log\n\nYour Practice: Try each method and compare their results to see how they differ in handling outliers.\nIn real-world applications, handling outliers should be approached on a case-by-case basis. However, here are some general recommendations:\nRecommendations for Different Scenarios:\n\nData Removal:\n\nUse when: You have a large dataset and can afford to lose data\nPros: Simple, removes all outlier influence\nCons: Loss of data, potential loss of important information\n\nCapping (Winsorization):\n\nUse when: You want to preserve data count while limiting extreme values\nPros: Preserves data size, reduces impact of outliers\nCons: Artificial boundary creation, potential loss of genuine extreme events\n\nMedian Replacement:\n\nUse when: The data is normally distributed, and outliers are verified as errors or anomalies not representing true values\nPros: Maintains data size, simple to implement\nCons: Reduces variance, may not represent the true distribution\n\nLog Transformation:\n\nUse when: Data has a right-skewed distribution, and you want to reduce the impact of large outliers.\nPros: Reduces skewness, minimizes the effect of extreme values, can make data more normal-like.\nCons: Only applicable to positive values, may not be effective for extremely high outliers.",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning and Preparation</span>"
    ]
  },
  {
    "objectID": "Data cleaning and preparation.html#data-binning",
    "href": "Data cleaning and preparation.html#data-binning",
    "title": "11  Data Cleaning and Preparation",
    "section": "11.3 Data binning",
    "text": "11.3 Data binning\nData binning is a powerful technique in data analysis where continuous numerical data is grouped into discrete intervals or “bins.” Imagine it as sorting items into distinct categories or “buckets” based on their values, making it easier to observe trends, patterns, and groupings within large datasets. We encounter binning in everyday life without even realizing it.\nFor example, consider age groups: instead of listing every individual age, we often refer to broader categories like “under 20,” “20-30,” “30-40,” or “40 and over.” This binning approach helps simplify data interpretation and comparison across different age groups. Another common example is tax brackets. Tax rates are often applied to ranges of income, like “up to $11,000,” “$11,001 to $44,725,” and so forth. Here, income values are grouped into bins to determine applicable tax rates, making complex tax data easier to manage and understand.\n\n\n\n\n    \n    \n\n\n\n\n11.3.1 Key reasons for binning:\n\nBetter intepretation of data\n\nMaking better recommendations\n\nSmooth data, reduce noise\n\nExamples:\nBinning to better interpret data\n\nThe number of flu cases everyday may be binned to seasons such as fall, spring, winter and summer, to understand the effect of season on flu.\n\nBinning to make recommendations:\n\nA doctor may like to group patient age into bins. Grouping patient ages into categories such as Age &lt;=12, 12&lt;Age&lt;=18, 18&lt;Age&lt;=65, Age&gt;65 may help recommend the kind/doses of covid vaccine a patient needs.\nA credit card company may want to bin customers based on their spend, as “High spenders”, “Medium spenders” and “Low spenders”. Binning will help them design customized marketing campaigns for each bin, thereby increasing customer response (or revenue). On the other hand, they use the same campaign for customers withing the same bin, thus minimizng marketing costs.\n\nBinning to smooth data, and reduce noise\n\nA sales company may want to bin their total sales to a weekly / monthly / yearly level to reduce the noise in day-to-day sales.\n\nUsing our college dataset, let’s apply binning to better interpret the relationship between instructional expenditure per student (Expend) and graduation rate (Grad.Rate) for U.S. universities, and make relevant recommendations\nHere is the previous scatterplot of ‘Grad.Rate’ vs ‘Expend’ with a trendline.\n\n# scatter plot of Expend vs Grad.Rate\nax=sns.regplot(data = college, x = \"Expend\", y = \"Grad.Rate\",scatter_kws={\"color\": \"orange\"}, line_kws={\"color\": \"blue\"})\nax.xaxis.set_major_formatter('${x:,.0f}')\nax.set_xlabel('Expenditure per student')\nax.set_ylabel('Graduation rate')\nplt.show()\n\n\n\n\n\n\n\n\nThe trendline indicates a positive correlation between Expend and Grad.Rate. However, there seems to be a lot of noise and presence of outliers in the data, which makes it hard to interpret the overall trend.\n\n\n11.3.2 Common binning methods:\n\nEqual-width: Divides range into equal-sized intervals\nEqual-size: Places equal number of observations in each bin\nCustom: Based on domain knowledge (like age groups above)\n\nWe’ll bin Expend to see if we can better analyze its association with Grad.Rate. However, let us first visualize the distribution of Expend.\n\n#Visualizing the distribution of expend\nax=sns.histplot(data = college, x= 'Expend')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\n\n\n\n\nThe distribution of Extend is right skewed with potentially some extremely high outlying values.\n\n11.3.2.1 Binning with equal width bins\nWe’ll use the Pandas function cut() to bin Expend. This function creates bins such that all bins have the same width.\n\n#Using the cut() function in Pandas to bin \"Expend\"\nBinned_expend = pd.cut(college['Expend'],3,retbins = True)\nBinned_expend\n\n(0      (3132.953, 20868.333]\n 1      (3132.953, 20868.333]\n 2      (3132.953, 20868.333]\n 3      (3132.953, 20868.333]\n 4      (3132.953, 20868.333]\n                ...          \n 772    (3132.953, 20868.333]\n 773    (3132.953, 20868.333]\n 774    (3132.953, 20868.333]\n 775     (38550.667, 56233.0]\n 776    (3132.953, 20868.333]\n Name: Expend, Length: 777, dtype: category\n Categories (3, interval[float64, right]): [(3132.953, 20868.333] &lt; (20868.333, 38550.667] &lt; (38550.667, 56233.0]],\n array([ 3132.953     , 20868.33333333, 38550.66666667, 56233.        ]))\n\n\nThe cut() function returns a tuple of length 2. The first element of the tuple are the bins, while the second element is an array containing the cut-off values for the bins.\n\ntype(Binned_expend)\n\ntuple\n\n\n\nlen(Binned_expend)\n\n2\n\n\nOnce the bins are obtained, we’ll add a column in the dataset that indicates the bin for Expend.\n\n#Creating a categorical variable to store the level of expenditure on a student\ncollege['Expend_bin'] = Binned_expend[0]\ncollege.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nPrivate\nApps\nAccept\nEnroll\nTop10perc\nTop25perc\nF.Undergrad\nP.Undergrad\nOutstate\n...\nPhD\nTerminal\nS.F.Ratio\nperc.alumni\nExpend\nGrad.Rate\nz_score\nis_z_score_outlier\nis_IQR_outlier\nExpend_bin\n\n\n\n\n0\nAbilene Christian University\nYes\n1660\n1232\n721\n23\n52\n2885\n537\n7440\n...\n70\n78\n18.1\n12\n7041\n60\n-0.501910\nFalse\nFalse\n(3132.953, 20868.333]\n\n\n1\nAdelphi University\nYes\n2186\n1924\n512\n16\n29\n2683\n1227\n12280\n...\n29\n30\n12.2\n16\n10527\n56\n0.166110\nFalse\nFalse\n(3132.953, 20868.333]\n\n\n2\nAdrian College\nYes\n1428\n1097\n336\n22\n50\n1036\n99\n11250\n...\n53\n66\n12.9\n30\n8735\n54\n-0.177290\nFalse\nFalse\n(3132.953, 20868.333]\n\n\n3\nAgnes Scott College\nYes\n417\n349\n137\n60\n89\n510\n63\n12960\n...\n92\n97\n7.7\n37\n19016\n59\n1.792851\nFalse\nTrue\n(3132.953, 20868.333]\n\n\n4\nAlaska Pacific University\nYes\n193\n146\n55\n16\n44\n249\n869\n7560\n...\n76\n72\n11.9\n2\n10922\n15\n0.241803\nFalse\nFalse\n(3132.953, 20868.333]\n\n\n\n\n5 rows × 23 columns\n\n\n\nSee the variable Expend_bin in the above dataset.\nLet us visualize the Expend bins over the distribution of the Expend variable.\n\n#Visualizing the bins for instructional expediture on a student\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nax=sns.histplot(data = college, x= 'Expend')\nplt.vlines(Binned_expend[1], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\n\n\n\n\nBy default, the bins created have equal width. They are created by dividing the range between the maximum and minimum value of Expend into the desired number of equal-width intervals. We can label the bins as well as follows.\n\ncollege['Expend_bin'] = pd.cut(college['Expend'],3,labels = ['Low expend','Med expend','High expend'])\ncollege['Expend_bin']\n\n0       Low expend\n1       Low expend\n2       Low expend\n3       Low expend\n4       Low expend\n          ...     \n772     Low expend\n773     Low expend\n774     Low expend\n775    High expend\n776     Low expend\nName: Expend_bin, Length: 777, dtype: category\nCategories (3, object): ['Low expend' &lt; 'Med expend' &lt; 'High expend']\n\n\nNow that we have binned the variable Expend, let us see if we can better visualize the association of graduation rate with expenditure per student using Expened_bin.\n\n#Visualizing average graduation rate vs categories of instructional expenditure per student\nsns.barplot(x = 'Expend_bin', y = 'Grad.Rate', data = college)\n\n\n\n\n\n\n\n\nIt seems that the graduation rate is the highest for universities with medium level of expenditure per student. This is different from the trend we saw earlier in the scatter plot. Let us investigate.\nLet us find the number of universities in each bin.\n\ncollege['Expend_bin'].value_counts()\n\nExpend_bin\nLow expend     751\nMed expend      21\nHigh expend      5\nName: count, dtype: int64\n\n\nThe bin High expend consists of only 5 universities, or 0.6% of all the universities in the dataset. These universities may be outliers that are skewing the trend (as also evident in the histogram above).\nLet us see if we get the correct trend with the outliers removed from the data.\n\n#Data without outliers\ncollege_data_without_outliers = college[((college.Expend&gt;=lower_bound) & (college.Expend&lt;=upper_bound))]\n\n\nBinned_data = pd.cut(college_data_without_outliers['Expend'],3,labels = ['Low expend','Med expend','High expend'],retbins = True)\ncollege_data_without_outliers.loc[:,'Expend_bin'] = Binned_data[0]\n\n\nsns.barplot(x = 'Expend_bin', y = 'Grad.Rate', data = college_data_without_outliers)\n\n\n\n\n\n\n\n\nWith the outliers removed, we obtain the correct overall trend, even in the case of equal-width bins. Note that these bins have unequal number of observations as shown below.\n\nax=sns.histplot(data = college_data_without_outliers, x= 'Expend')\nfor i in range(4):\n    plt.axvline(Binned_data[1][i], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\n\n\n\n\nNote that the right tail of the histogram has disappered since we removed outliers.\n\ncollege_data_without_outliers['Expend_bin'].value_counts()\n\nExpend_bin\nMed expend     327\nLow expend     314\nHigh expend     88\nName: count, dtype: int64\n\n\nInstead of removing outliers, we can address the issue by binning observations into equal-sized bins, ensuring each bin has the same number of observations. Let’s explore this approach next.\n\n\n11.3.2.2 Binning with equal sized bins\nLet us bin the variable Expend such that each bin consists of the same number of observations.\nWe’ll use the Pandas function qcut() to make equal-sized bins (in contrast to equal-width bins in the previous section).\n\n#Using the Pandas function qcut() to create bins with the same number of observations\nBinned_expend = pd.qcut(college['Expend'],3,retbins = True)\ncollege['Expend_bin'] = Binned_expend[0]\n\nEach bin has the same number of observations with qcut():\n\ncollege['Expend_bin'].value_counts()\n\nExpend_bin\n(3185.999, 7334.333]    259\n(7334.333, 9682.333]    259\n(9682.333, 56233.0]     259\nName: count, dtype: int64\n\n\nLet us visualize the Expend bins over the distribution of the Expend variable.\n\n#Visualizing the bins for instructional expediture on a student\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nax=sns.histplot(data = college, x= 'Expend')\nplt.vlines(Binned_expend[1], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\n\n\n\n\nNote that the bin-widths have been adjusted to have the same number of observations in each bin. The bins are narrower in domains of high density, and wider in domains of sparse density.\nLet us again make the barplot visualizing the average graduate rate with level of instructional expenditure per student.\n\ncollege['Expend_bin'] = pd.qcut(college['Expend'],3,labels = ['Low expend','Med expend','High expend'])\na=sns.barplot(x = 'Expend_bin', y = 'Grad.Rate', data = college)\n\n\n\n\n\n\n\n\nNow we see the same trend that we saw in the scatterplot, but without the noise. We have smoothed the data. Note that making equal-sized bins helps reduce the effect of outliers in the overall trend.\nSuppose this analysis was done to provide recommendations to universities for increasing their graduation rate. With binning, we can can provide one recommendation to ‘Low expend’ universities, and another one to ‘Med expend’ universities. For example, the recommendations can be:\n\n‘Low expend’ universities can expect an increase of 9 percentage points in Grad.Rate, if they migrate to the ‘Med expend’ category.\n‘Med expend’ universities can expect an increase of 7 percentage points in Grad.Rate, if they migrate to the ‘High expend’ category.\n\nThe numbers in the above recommendations are based on the table below.\n\ncollege.groupby(college.Expend_bin, observed=False)['Grad.Rate'].mean()\n\nExpend_bin\nLow expend     57.343629\nMed expend     66.057915\nHigh expend    72.988417\nName: Grad.Rate, dtype: float64\n\n\nWe can also provide recommendations based on the confidence intervals of the mean graduation rate (Grad.Rate). The confidence intervals, calculated below, are derived using a method known as bootstrapping. For a detailed explanation of bootstrapping, please refer to this article on Wikipedia.\n\n# Bootstrapping to find 95% confidence intervals of Graduation Rate of US universities based on average expenditure per student\nconfidence_intervals = {}\n\n# Loop through each expenditure bin\nfor expend_bin, data_sub in college.groupby('Expend_bin', observed=False):\n    # Generate bootstrap samples for the graduation rate\n    samples = np.random.choice(data_sub['Grad.Rate'], size=(10000, len(data_sub)), replace=True)\n    \n    # Calculate the mean of each sample\n    sample_means = samples.mean(axis=1)\n    \n    # Calculate the 95% confidence interval\n    lower_bound = np.percentile(sample_means, 2.5)\n    upper_bound = np.percentile(sample_means, 97.5)\n    \n    # Store the result in the dictionary\n    confidence_intervals[expend_bin] = (round(lower_bound, 2), round(upper_bound, 2))\n\n# Print the results\nfor expend_bin, ci in confidence_intervals.items():\n    print(f\"95% Confidence interval of Grad.Rate for {expend_bin} universities = [{ci[0]}, {ci[1]}]\")\n\n95% Confidence interval of Grad.Rate for Low expend universities = [55.36, 59.36]\n95% Confidence interval of Grad.Rate for Med expend universities = [64.15, 67.95]\n95% Confidence interval of Grad.Rate for High expend universities = [71.03, 74.92]\n\n\nApart from equal-width and equal-sized bins, custom bins can be created using the bins argument. Suppose, bins are to be created for Expend with cutoffs \\(\\$10,000, \\$20,000, \\$30,000... \\$60,000\\). Then, we can use the bins argument as in the code below:\n\n\n11.3.2.3 Binning with custom bins\n\nBinned_expend = pd.cut(college.Expend,bins = list(range(0,70000,10000)),retbins=True)\n\n\n#Visualizing the bins for instructional expediture on a student\nax=sns.histplot(data = college, x= 'Expend')\nplt.vlines(Binned_expend[1], 0,100,color='red')\nplt.xlabel('Expenditure per student');\nplt.ylabel('Graduation rate');\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\n\n\n\n\nNext, let’s create a custom bin with unequal sizes and widths.\n\n# Create bins and labels\nbins = [college['Expend'].min(), 10000, 40000, college['Expend'].max()]\nlabels = ['Low Expenditure', 'Medium Expenditure', 'High Expenditure']\n\n# Binning the 'Expend' column\ncollege['Expend_Binned'] = pd.cut(college['Expend'], bins=bins, labels=labels)\n\n# Visualizing the bins using a histogram\nax = sns.histplot(data=college, x='Expend', kde=False)\n\n# Add vertical lines for the bin edges (the bin edges are the values in 'bins')\nfor bin_edge in bins[1:-1]:  # skip the first and last bins since they are the edges\n    plt.vlines(bin_edge, 0, ax.get_ylim()[1], color='red')\n\n# Add labels and formatting\nplt.xlabel('Expenditure per student')\nplt.ylabel('Frequency')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nAs custom bin-cutoffs can be specified with the cut() function, custom bin quantiles can be specified with the qcut() function.",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning and Preparation</span>"
    ]
  },
  {
    "objectID": "Data cleaning and preparation.html#dummy-indicator-variables",
    "href": "Data cleaning and preparation.html#dummy-indicator-variables",
    "title": "11  Data Cleaning and Preparation",
    "section": "11.4 Dummy / Indicator variables",
    "text": "11.4 Dummy / Indicator variables\nDummy or indicator variables are binary variables created to represent categorical data in numerical form, typically used in regression and machine learning models. Each category is represented by a separate dummy variable with values of 0 or 1.\n\n11.4.1 Purpose:\n\nEnables categorical data to be used in quantitative analysis: Models like linear regression and many machine learning algorithms require numerical input, making dummy variables essential for handling categorical data.\nPreserves information: Converts categorical information into a format that models can interpret without losing meaning.\n\n\n\n11.4.2 When to use dummy variables\nDummy variables are especially useful in:\n\nRegression models: For categorical predictors, such as region, gender, or type of product.\nMachine learning algorithms: Many models, including tree-based methods, can benefit from dummy variables for categorical features.\n\n\n\n11.4.3 How to Create Dummy variables\nThe pandas library in Python has a built-in function, pd.get_dummies(), to generate dummy variables. If a column in a DataFrame has \\(k\\) distinct values, we will get a DataFrame with \\(k\\) columns containing 0s and 1s\nLet us make dummy variables with the equal-sized bins we created for the average instruction expenditure per student.\n\n#Using the Pandas function qcut() to create bins with the same number of observations\nBinned_expend = pd.qcut(college['Expend'],3,retbins = True,labels = ['Low_expend','Med_expend','High_expend'])\ncollege['Expend_bin'] = Binned_expend[0]\n\n\n#Making dummy variables based on the levels (categories) of the 'Expend_bin' variable\ndummy_Expend = pd.get_dummies(college['Expend_bin'], dtype=int)\n\nThe dummy data dummy_Expend has a value of \\(1\\) if the observation corresponds to the category referenced by the column name.\n\ndummy_Expend.head()\n\n\n\n\n\n\n\n\nLow_expend\nMed_expend\nHigh_expend\n\n\n\n\n0\n1\n0\n0\n\n\n1\n0\n0\n1\n\n\n2\n0\n1\n0\n\n\n3\n0\n0\n1\n\n\n4\n0\n0\n1\n\n\n\n\n\n\n\nAdding the dummy columns back to the original dataframe\n\n#Concatenating the dummy variables to the original data\ncollege = pd.concat([college,dummy_Expend],axis=1)\nprint(college.shape)\ncollege.head()\n\n(777, 26)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nPrivate\nApps\nAccept\nEnroll\nTop10perc\nTop25perc\nF.Undergrad\nP.Undergrad\nOutstate\n...\nperc.alumni\nExpend\nGrad.Rate\nz_score\nis_z_score_outlier\nis_IQR_outlier\nExpend_bin\nLow_expend\nMed_expend\nHigh_expend\n\n\n\n\n0\nAbilene Christian University\nYes\n1660\n1232\n721\n23\n52\n2885\n537\n7440\n...\n12\n7041\n60\n-0.501910\nFalse\nFalse\nLow_expend\n1\n0\n0\n\n\n1\nAdelphi University\nYes\n2186\n1924\n512\n16\n29\n2683\n1227\n12280\n...\n16\n10527\n56\n0.166110\nFalse\nFalse\nHigh_expend\n0\n0\n1\n\n\n2\nAdrian College\nYes\n1428\n1097\n336\n22\n50\n1036\n99\n11250\n...\n30\n8735\n54\n-0.177290\nFalse\nFalse\nMed_expend\n0\n1\n0\n\n\n3\nAgnes Scott College\nYes\n417\n349\n137\n60\n89\n510\n63\n12960\n...\n37\n19016\n59\n1.792851\nFalse\nTrue\nHigh_expend\n0\n0\n1\n\n\n4\nAlaska Pacific University\nYes\n193\n146\n55\n16\n44\n249\n869\n7560\n...\n2\n10922\n15\n0.241803\nFalse\nFalse\nHigh_expend\n0\n0\n1\n\n\n\n\n5 rows × 26 columns\n\n\n\nWe can find the correlation between the dummy variables and graduation rate to identify if any of the dummy variables will be useful to estimate graduation rate (Grad.Rate).\n\n#Finding if dummy variables will be useful to estimate 'Grad.Rate'\ndummy_Expend.corrwith(college['Grad.Rate'])\n\nLow_expend    -0.334456\nMed_expend     0.024492\nHigh_expend    0.309964\ndtype: float64\n\n\nThe dummy variables Low expend and High expend may contribute in explaining Grad.Rate in a regression model.\n\n\n11.4.4 Using drop_first to Avoid Multicollinearity\nIn cases like linear regression, you may want to drop one dummy variable to avoid multicollinearity (perfect correlation between variables).\n\n# create dummy variables, dropping the first to avoid multicollinearity\ndummy_Expend = pd.get_dummies(college['Expend_bin'], dtype=int, drop_first=True)\n\n# add the dummy variables to the original dataset\ncollege = pd.concat([college, dummy_Expend], axis=1)\n\ncollege.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nPrivate\nApps\nAccept\nEnroll\nTop10perc\nTop25perc\nF.Undergrad\nP.Undergrad\nOutstate\n...\nGrad.Rate\nz_score\nis_z_score_outlier\nis_IQR_outlier\nExpend_bin\nLow_expend\nMed_expend\nHigh_expend\nMed_expend\nHigh_expend\n\n\n\n\n0\nAbilene Christian University\nYes\n1660\n1232\n721\n23\n52\n2885\n537\n7440\n...\n60\n-0.501910\nFalse\nFalse\nLow_expend\n1\n0\n0\n0\n0\n\n\n1\nAdelphi University\nYes\n2186\n1924\n512\n16\n29\n2683\n1227\n12280\n...\n56\n0.166110\nFalse\nFalse\nHigh_expend\n0\n0\n1\n0\n1\n\n\n2\nAdrian College\nYes\n1428\n1097\n336\n22\n50\n1036\n99\n11250\n...\n54\n-0.177290\nFalse\nFalse\nMed_expend\n0\n1\n0\n1\n0\n\n\n3\nAgnes Scott College\nYes\n417\n349\n137\n60\n89\n510\n63\n12960\n...\n59\n1.792851\nFalse\nTrue\nHigh_expend\n0\n0\n1\n0\n1\n\n\n4\nAlaska Pacific University\nYes\n193\n146\n55\n16\n44\n249\n869\n7560\n...\n15\n0.241803\nFalse\nFalse\nHigh_expend\n0\n0\n1\n0\n1\n\n\n\n\n5 rows × 28 columns",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning and Preparation</span>"
    ]
  },
  {
    "objectID": "Data cleaning and preparation.html#independent-study",
    "href": "Data cleaning and preparation.html#independent-study",
    "title": "11  Data Cleaning and Preparation",
    "section": "11.5 Independent Study",
    "text": "11.5 Independent Study\n\n11.5.1 Practice exercise 1\nRead survey_data_clean.csv. Split the columns of the dataset, such that all columns with categorical values transform into dummy variables with each category corresponding to a column of 0s and 1s. Leave the Timestamp column.\nAs all categorical columns are transformed to dummy variables, all columns have numeric values.\nWhat is the total number of columns in the transformed data? What is the total number of columns of the original data?\nFind the:\n\nTop 5 variables having the highest positive correlation with NU_GPA.\nTop 5 variables having the highest negative correlation with NU_GPA.\n\n\n\n11.5.2 Practice exercise 2\nConsider the dataset survey_data_clean.csv . Find the number of outliers in each column of the dataset based on both the Z-score and IQR criteria. Do not use a for loop.\nWhich column(s) have the maximum number of outliers using Z-score?\nWhich column(s) have the maximum number of outliers using IQR?\nDo you think the outlying observations identified for those columns(s) should be considered as outliers? If not, then which type of columns should be considered when finding outliers?",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Cleaning and Preparation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html",
    "href": "Data aggregation.html",
    "title": "12  Data Grouping and Aggregation",
    "section": "",
    "text": "12.1 Grouping by a single column\nGrouping and aggregation are essential in data science because they enable meaningful analysis and insights from complex and large-scale datasets. Here are key reasons why they are important:\nIn this chapter, we are going to see grouping and aggregating using pandas. Grouping and aggregating will help to achieve data analysis easily using various functions. These methods will help us to the group and summarize our data and make complex analysis comparatively easy.\nThrougout this chapter, we will use gdp_lifeExpectancy.csv, let’s read the csv file to pandas dataframe first\ngroupby() allows you to split a DataFrame based on the values in one or more columns. First, we’ll explore grouping by a single column.",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html#grouping-by-a-single-column",
    "href": "Data aggregation.html#grouping-by-a-single-column",
    "title": "12  Data Grouping and Aggregation",
    "section": "",
    "text": "12.1.1 Syntax of groupby()\n\nDataFrame.groupby(by=\"column_name\") – Groups data by the specified column.\n\nConsider the life expectancy dataset. suppose we want to group by the observations by continent by passing it as an argument to the groupby() method.\n\n#Creating a GroupBy object\ngrouped = gdp_lifeExp_data.groupby('continent')\n#This will split the data into groups that correspond to values of the column 'continent'\n\nThe groupby() method returns a GroupBy object.\n\n#A 'GroupBy' objects is created with the `groupby()` function\ntype(grouped)\n\npandas.core.groupby.generic.DataFrameGroupBy\n\n\nThe GroupBy object grouped contains the information of the groups in which the data is distributed. Each observation has been assigned to a specific group of the column(s) used to group the data. However, note that the dataset is not physically split into different DataFrames. For example, in the above case, each observation is assigned to a particular group depending on the value of the continent for that observation. However, all the observations are still in the same DataFrame data.\n\n\n12.1.2 Attributes and methods of the GroupBy object\n\n12.1.2.1 keys\nThe object(s) grouping the data are called key(s). Here continent is the group key. The keys of the GroupBy object can be seen using Its keys attribute.\n\n#Key(s) of the GroupBy object\ngrouped.keys\n\n'continent'\n\n\n\n\n12.1.2.2 ngroups\nThe number of groups in which the data is distributed based on the keys can be seen with the ngroups attribute.\n\n#The number of groups based on the key(s)\ngrouped.ngroups\n\n5\n\n\nThe group names are the keys of the dictionary, while the row labels are the corresponding values\n\n#Group names\ngrouped.groups.keys()\n\ndict_keys(['Africa', 'Americas', 'Asia', 'Europe', 'Oceania'])\n\n\n\n\n12.1.2.3 groups\nThe groups attribute of the GroupBy object contains the group labels (or names) and the row labels of the observations in each group, as a dictionary.\n\n#The groups (in the dictionary format)\ngrouped.groups\n\n{'Africa': [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, ...], 'Americas': [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 432, 433, 434, 435, ...], 'Asia': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, ...], 'Europe': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 516, 517, 518, 519, ...], 'Oceania': [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103]}\n\n\n\n\n12.1.2.4 groups.values\nThe groups.values attribute of the GroupBy object contains the row labels of the observations in each group, as a dictionary.\n\n#Group values are the row labels corresponding to a particular group\ngrouped.groups.values()\n\ndict_values([Index([  24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n       ...\n       1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703],\n      dtype='int64', length=624), Index([  48,   49,   50,   51,   52,   53,   54,   55,   56,   57,\n       ...\n       1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643],\n      dtype='int64', length=300), Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679],\n      dtype='int64', length=396), Index([  12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n       ...\n       1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607],\n      dtype='int64', length=360), Index([  60,   61,   62,   63,   64,   65,   66,   67,   68,   69,   70,   71,\n       1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103],\n      dtype='int64')])\n\n\n\n\n12.1.2.5 size()\nThe size() method of the GroupBy object returns the number of observations in each group.\n\n#Number of observations in each group\ngrouped.size()\n\ncontinent\nAfrica      624\nAmericas    300\nAsia        396\nEurope      360\nOceania      24\ndtype: int64\n\n\n\n\n12.1.2.6 first()\nThe first non missing element of each group is returned with the first() method of the GroupBy object.\n\n#The first element of the group can be printed using the first() method\ngrouped.first()\n\n\n\n\n\n\n\n\ncountry\nyear\nlifeExp\npop\ngdpPercap\n\n\ncontinent\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n1952\n43.077\n9279525\n2449.008185\n\n\nAmericas\nArgentina\n1952\n62.485\n17876956\n5911.315053\n\n\nAsia\nAfghanistan\n1952\n28.801\n8425333\n779.445314\n\n\nEurope\nAlbania\n1952\n55.230\n1282697\n1601.056136\n\n\nOceania\nAustralia\n1952\n69.120\n8691212\n10039.595640\n\n\n\n\n\n\n\n\n\n12.1.2.7 get_group()\nThis method returns the observations for a particular group of the GroupBy object.\n\n#Observations for individual groups can be obtained using the get_group() function\ngrouped.get_group('Asia')\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1675\nYemen, Rep.\nAsia\n1987\n52.922\n11219340\n1971.741538\n\n\n1676\nYemen, Rep.\nAsia\n1992\n55.599\n13367997\n1879.496673\n\n\n1677\nYemen, Rep.\nAsia\n1997\n58.020\n15826497\n2117.484526\n\n\n1678\nYemen, Rep.\nAsia\n2002\n60.308\n18701257\n2234.820827\n\n\n1679\nYemen, Rep.\nAsia\n2007\n62.698\n22211743\n2280.769906\n\n\n\n\n396 rows × 6 columns",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html#data-aggregation-within-groups",
    "href": "Data aggregation.html#data-aggregation-within-groups",
    "title": "12  Data Grouping and Aggregation",
    "section": "12.2 Data aggregation within groups",
    "text": "12.2 Data aggregation within groups\n\n12.2.1 Common Aggregation Functions\nAggregation functions are essential for summarizing and analyzing data in pandas. These functions allow you to compute summary statistics for your data, making it easier to identify trends and patterns.\nBelow are some of the most commonly used aggregation functions when working with grouped data in pandas:\n\nmean() – Calculates the average value of the group.\nsum() – Computes the total value by summing all elements in the group.\nmin() – Finds the minimum value in the group.\nmax() – Finds the maximum value in the group.\ncount() – Returns the number of occurrences or entries in the group.\nmedian() – Finds the middle value in the sorted group.\nstd() – Calculates the standard deviation, measuring the spread or variation in the values of the group.\n\nEach of these functions can help summarize and provide insights into different aspects of the grouped data.\nConsider the life expectancy dataset, let’s find the mean life expectancy, population and GDP per capita for each country during the period of 1952 -2007.\nNext, we’ll find the mean statistics for each group with the mean() method. The method will be applied on all columns of the DataFrame and all groups.\n\n#Grouping the observations by 'country'\ngrouped_country = gdp_lifeExp_data.drop (['continent', 'year'], axis = 1).groupby('country')\n\n#Finding the mean stastistic of all columns of the DataFrame and all groups\ngrouped_country.mean()\n\n\n\n\n\n\n\n\nlifeExp\npop\ngdpPercap\n\n\ncountry\n\n\n\n\n\n\n\nAfghanistan\n37.478833\n1.582372e+07\n802.674598\n\n\nAlbania\n68.432917\n2.580249e+06\n3255.366633\n\n\nAlgeria\n59.030167\n1.987541e+07\n4426.025973\n\n\nAngola\n37.883500\n7.309390e+06\n3607.100529\n\n\nArgentina\n69.060417\n2.860224e+07\n8955.553783\n\n\n...\n...\n...\n...\n\n\nVietnam\n57.479500\n5.456857e+07\n1017.712615\n\n\nWest Bank and Gaza\n60.328667\n1.848606e+06\n3759.996781\n\n\nYemen, Rep.\n46.780417\n1.084319e+07\n1569.274672\n\n\nZambia\n45.996333\n6.353805e+06\n1358.199409\n\n\nZimbabwe\n52.663167\n7.641966e+06\n635.858042\n\n\n\n\n142 rows × 3 columns\n\n\n\nNext, we’ll find the standard deviation statistics for each group with the std() method.\n\ngrouped_country.std()\n\n\n\n\n\n\n\n\nlifeExp\npop\ngdpPercap\n\n\ncountry\n\n\n\n\n\n\n\nAfghanistan\n5.098646\n7.114583e+06\n108.202929\n\n\nAlbania\n6.322911\n8.285855e+05\n1192.351513\n\n\nAlgeria\n10.340069\n8.613355e+06\n1310.337656\n\n\nAngola\n4.005276\n2.672281e+06\n1165.900251\n\n\nArgentina\n4.186470\n7.546609e+06\n1862.583151\n\n\n...\n...\n...\n...\n\n\nVietnam\n12.172331\n2.052585e+07\n567.482251\n\n\nWest Bank and Gaza\n11.000069\n1.023057e+06\n1716.840614\n\n\nYemen, Rep.\n11.019302\n5.590408e+06\n609.939160\n\n\nZambia\n4.453246\n3.096949e+06\n247.494984\n\n\nZimbabwe\n7.071816\n3.376895e+06\n133.689213\n\n\n\n\n142 rows × 3 columns\n\n\n\nAlternatively, you can compute other statistical metrics.\n\n\n12.2.2 Multiple aggregations and Custom aggregation using agg()\n\n12.2.2.1 Multiple aggregations\nDirectly applying the aggregate methods of the GroupBy object such as mean, count, etc., lets us apply only one function at a time. Also, we may wish to apply an aggregate function of our own, which is not there in the set of methods of the GroupBy object, such as the range of values of a column.\nThe agg() function of a GroupBy object lets us aggregate data using:\n\nMultiple aggregation functions\nCustom aggregate functions (in addition to in-built functions like mean, std, count etc.)\n\nConsider the life expectancy dataset, Let us use the agg() method of the GroupBy object to simultaneously find the mean and standard deviation of the gdpPercap for each country.\nFor aggregating by multiple functions, we pass a list of strings to agg(), where the strings are the function names.\n\ngrouped_country['gdpPercap'].agg(['mean','std']).sort_values(by = 'mean',ascending = False).head()\n\n\n\n\n\n\n\n\nmean\nstd\n\n\ncountry\n\n\n\n\n\n\nKuwait\n65332.910472\n33882.139536\n\n\nSwitzerland\n27074.334405\n6886.463308\n\n\nNorway\n26747.306554\n13421.947245\n\n\nUnited States\n26261.151347\n9695.058103\n\n\nCanada\n22410.746340\n8210.112789\n\n\n\n\n\n\n\n\n\n12.2.2.2 Custom aggregation\nIn addition to the mean and standard deviation of the gdpPercap of each country, let us also include the range of gdpPercap in the table above using lambda function\n\ngrouped_country['gdpPercap'].agg(lambda x: x.max() - x.min())\n\ncountry\nAfghanistan            342.670088\nAlbania               4335.973390\nAlgeria               3774.359280\nAngola                3245.635491\nArgentina             6868.064587\n                         ...     \nVietnam               1836.509912\nWest Bank and Gaza    5595.075290\nYemen, Rep.           1499.052330\nZambia                 705.723500\nZimbabwe               392.478061\nName: gdpPercap, Length: 142, dtype: float64\n\n\n\n# define a function that calculates the range\ndef range_func(x):\n    return x.max() - x.min()\n\n\n# apply the range function to the 'gdpPercap' column besides the mean and standard deviation\ngrouped_country['gdpPercap'].agg(['mean', 'std', range_func]).sort_values(by = 'range_func', ascending = False)\n\n\n\n\n\n\n\n\nmean\nstd\nrange_func\n\n\ncountry\n\n\n\n\n\n\n\nKuwait\n65332.910472\n33882.139536\n85404.702920\n\n\nSingapore\n17425.382267\n14926.147774\n44828.041413\n\n\nNorway\n26747.306554\n13421.947245\n39261.768450\n\n\nHong Kong, China\n16228.700865\n12207.329731\n36670.557461\n\n\nIreland\n15758.606238\n11573.311022\n35465.716022\n\n\n...\n...\n...\n...\n\n\nRwanda\n675.669043\n142.229906\n388.246772\n\n\nSenegal\n1533.121694\n105.399353\n344.572767\n\n\nAfghanistan\n802.674598\n108.202929\n342.670088\n\n\nEthiopia\n509.115155\n96.427627\n328.659296\n\n\nBurundi\n471.662990\n99.329720\n292.403419\n\n\n\n\n142 rows × 3 columns\n\n\n\nFor aggregating by multiple functions & changing the column names resulting from those functions, we pass a list of tuples to agg(), where each tuple is of length two, and contains the new column name & the function to be applied.\n\n#Simultaneous renaming of columns while grouping\ngrouped_country['gdpPercap'].agg([('Average','mean'),('Standard Deviation','std'),('90th Percentile',lambda x:x.quantile(0.9))]).sort_values(by = '90th Percentile',ascending = False)\n\n\n\n\n\n\n\n\nAverage\nStandard Deviation\n90th Percentile\n\n\ncountry\n\n\n\n\n\n\n\nKuwait\n65332.910472\n33882.139536\n109251.315590\n\n\nNorway\n26747.306554\n13421.947245\n44343.894158\n\n\nUnited States\n26261.151347\n9695.058103\n38764.132898\n\n\nSingapore\n17425.382267\n14926.147774\n35772.742520\n\n\nSwitzerland\n27074.334405\n6886.463308\n34246.394240\n\n\n...\n...\n...\n...\n\n\nLiberia\n604.814141\n98.988329\n706.275527\n\n\nMalawi\n575.447212\n122.999953\n689.590541\n\n\nBurundi\n471.662990\n99.329720\n615.597260\n\n\nMyanmar\n439.333333\n175.401531\n592.300000\n\n\nEthiopia\n509.115155\n96.427627\n577.448804\n\n\n\n\n142 rows × 3 columns\n\n\n\n\n\n\n12.2.3 Multiple aggregate functions on multiple columns\nLet us find the mean and standard deviation of lifeExp and pop for each country\n\n# find the meand and standard deviation of the 'lifeExp' and 'pop' column for each country\ngrouped_country[['lifeExp', 'pop']].agg(['mean', 'std']).sort_values(by = ('lifeExp', 'mean'), ascending = False)\n\n\n\n\n\n\n\n\nlifeExp\npop\n\n\n\nmean\nstd\nmean\nstd\n\n\ncountry\n\n\n\n\n\n\n\n\nIceland\n76.511417\n3.026593\n2.269781e+05\n4.854168e+04\n\n\nSweden\n76.177000\n3.003990\n8.220029e+06\n6.365660e+05\n\n\nNorway\n75.843000\n2.423994\n4.031441e+06\n4.107955e+05\n\n\nNetherlands\n75.648500\n2.486363\n1.378680e+07\n2.005631e+06\n\n\nSwitzerland\n75.565083\n4.011572\n6.384293e+06\n8.582009e+05\n\n\n...\n...\n...\n...\n...\n\n\nMozambique\n40.379500\n4.599184\n1.204670e+07\n4.457509e+06\n\n\nGuinea-Bissau\n39.210250\n4.937369\n8.820084e+05\n3.132917e+05\n\n\nAngola\n37.883500\n4.005276\n7.309390e+06\n2.672281e+06\n\n\nAfghanistan\n37.478833\n5.098646\n1.582372e+07\n7.114583e+06\n\n\nSierra Leone\n36.769167\n3.937828\n3.605425e+06\n1.270945e+06\n\n\n\n\n142 rows × 4 columns\n\n\n\n\n\n12.2.4 Distinct aggregate functions on multiple columns\nFor aggregating by multiple functions, we pass a list of strings to agg(), where the strings are the function names.\nFor aggregating by multiple functions & changing the column names resulting from those functions, we pass a list of tuples to agg(), where each tuple is of length two, and contains the new column name as the first object and the function to be applied as the second object of the tuple.\nFor aggregating by multiple functions such that a distinct set of functions is applied to each column, we pass a dictionary to agg(), where the keys are the column names on which the function is to be applied, and the values are the list of strings that are the function names, or a list of tuples if we also wish to name the aggregated columns.\n\n# We can use a list to apply multiple aggregation functions to a single column, and a dictionary to specify different functions for multiple columns\n# Use string names for the aggregation functions\ngrouped_country.agg({\"gdpPercap\": [\"mean\", \"std\"], \"lifeExp\": [\"median\", \"std\"], \"pop\": [\"max\", \"min\"]})\n\n\n\n\n\n\n\n\ngdpPercap\nlifeExp\npop\n\n\n\nmean\nstd\nmedian\nstd\nmax\nmin\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n802.674598\n108.202929\n39.1460\n5.098646\n31889923\n8425333\n\n\nAlbania\n3255.366633\n1192.351513\n69.6750\n6.322911\n3600523\n1282697\n\n\nAlgeria\n4426.025973\n1310.337656\n59.6910\n10.340069\n33333216\n9279525\n\n\nAngola\n3607.100529\n1165.900251\n39.6945\n4.005276\n12420476\n4232095\n\n\nArgentina\n8955.553783\n1862.583151\n69.2115\n4.186470\n40301927\n17876956\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nVietnam\n1017.712615\n567.482251\n57.2900\n12.172331\n85262356\n26246839\n\n\nWest Bank and Gaza\n3759.996781\n1716.840614\n62.5855\n11.000069\n4018332\n1030585\n\n\nYemen, Rep.\n1569.274672\n609.939160\n46.6440\n11.019302\n22211743\n4963829\n\n\nZambia\n1358.199409\n247.494984\n46.0615\n4.453246\n11746035\n2672000\n\n\nZimbabwe\n635.858042\n133.689213\n53.1765\n7.071816\n12311143\n3080907\n\n\n\n\n142 rows × 6 columns\n\n\n\nNext, for each country, find the mean and standard deviation of the lifeExp, and the minimum and maximum values of gdpPercap.\n\n#Specifying arguments to the function as a dictionary if distinct functions are to be applied on distinct columns\ngrouped_country.agg({'lifeExp':[('Average','mean'),('Standard deviation','std')],'gdpPercap':['min','max']})\n\n\n\n\n\n\n\n\nlifeExp\ngdpPercap\n\n\n\nAverage\nStandard deviation\nmin\nmax\n\n\ncountry\n\n\n\n\n\n\n\n\nAfghanistan\n37.478833\n5.098646\n635.341351\n978.011439\n\n\nAlbania\n68.432917\n6.322911\n1601.056136\n5937.029526\n\n\nAlgeria\n59.030167\n10.340069\n2449.008185\n6223.367465\n\n\nAngola\n37.883500\n4.005276\n2277.140884\n5522.776375\n\n\nArgentina\n69.060417\n4.186470\n5911.315053\n12779.379640\n\n\n...\n...\n...\n...\n...\n\n\nVietnam\n57.479500\n12.172331\n605.066492\n2441.576404\n\n\nWest Bank and Gaza\n60.328667\n11.000069\n1515.592329\n7110.667619\n\n\nYemen, Rep.\n46.780417\n11.019302\n781.717576\n2280.769906\n\n\nZambia\n45.996333\n4.453246\n1071.353818\n1777.077318\n\n\nZimbabwe\n52.663167\n7.071816\n406.884115\n799.362176\n\n\n\n\n142 rows × 4 columns",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html#grouping-by-multiple-columns",
    "href": "Data aggregation.html#grouping-by-multiple-columns",
    "title": "12  Data Grouping and Aggregation",
    "section": "12.3 Grouping by Multiple Columns",
    "text": "12.3 Grouping by Multiple Columns\nAbove, we demonstrated grouping by a single column, which is useful for summarizing data based on one categorical variable. However, in many cases, we need to group by multiple columns. Grouping by multiple columns allows us to create more detailed summaries by accounting for multiple categorical variables. This approach enables us to analyze data at a finer granularity, revealing insights that might be missed with single-column grouping alone.\n\n12.3.1 Basic Syntax for Grouping by Multiple Columns\nUse groupby() with a list of column names to group data by multiple columns.\n\nDataFrame.groupby(by=[\"col1\", \"col2\"])\n\nConsider the life expectancy dataset, we can group by both country and continent to analyze gdpPercap, lifeExp, and pop trends for each country within each continent, providing a more comprehensive view of the data.\n\n#Grouping by multiple columns\ngrouped_continent_contry = gdp_lifeExp_data.groupby(['continent', 'country'])[ \"lifeExp\"].agg(['mean', 'std', 'max', 'min']).sort_values(by = 'mean', ascending = False)\n\n\ngrouped_continent_contry\n\n\n\n\n\n\n\n\n\nmean\nstd\nmax\nmin\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\nEurope\nIceland\n76.511417\n3.026593\n81.757\n72.490\n\n\nSweden\n76.177000\n3.003990\n80.884\n71.860\n\n\nNorway\n75.843000\n2.423994\n80.196\n72.670\n\n\nNetherlands\n75.648500\n2.486363\n79.762\n72.130\n\n\nSwitzerland\n75.565083\n4.011572\n81.701\n69.620\n\n\n...\n...\n...\n...\n...\n...\n\n\nAfrica\nMozambique\n40.379500\n4.599184\n46.344\n31.286\n\n\nGuinea-Bissau\n39.210250\n4.937369\n46.388\n32.500\n\n\nAngola\n37.883500\n4.005276\n42.731\n30.015\n\n\nAsia\nAfghanistan\n37.478833\n5.098646\n43.828\n28.801\n\n\nAfrica\nSierra Leone\n36.769167\n3.937828\n42.568\n30.331\n\n\n\n\n142 rows × 4 columns\n\n\n\n\n\n12.3.2 Understanding Hierarchical (Multi-Level) Indexing\n\nGrouping by multiple columns creates a hierarchical index (also called a multi-level index).\nThis index allows each level (e.g., continent, country) to act as an independent category that can be accessed individually.\n\nIn the above output, continent and country form a two-level hierarchical index, allowing us to drill down from continent-level to country-level summaries.\n\ngrouped_continent_contry.index.nlevels\n\n2\n\n\n\n# get the first level of the index\ngrouped_continent_contry.index.levels[0]\n\nIndex(['Africa', 'Americas', 'Asia', 'Europe', 'Oceania'], dtype='object', name='continent')\n\n\n\n# get the second level of the index\ngrouped_continent_contry.index.levels[1]\n\nIndex(['Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina', 'Australia',\n       'Austria', 'Bahrain', 'Bangladesh', 'Belgium',\n       ...\n       'Uganda', 'United Kingdom', 'United States', 'Uruguay', 'Venezuela',\n       'Vietnam', 'West Bank and Gaza', 'Yemen, Rep.', 'Zambia', 'Zimbabwe'],\n      dtype='object', name='country', length=142)\n\n\n\n\n12.3.3 Subsetting Data in a Hierarchical Index\ngrouped_continent_country is still a DataFrame with hierarchical indexing. You can use .loc[] for subsetting, just as you would with a single-level index.\n\n# get the observations for the 'Americas' continent\ngrouped_continent_contry.loc['Americas'].head()\n\n\n\n\n\n\n\n\nmean\nstd\nmax\nmin\n\n\ncountry\n\n\n\n\n\n\n\n\nCanada\n74.902750\n3.952871\n80.653\n68.750\n\n\nUnited States\n73.478500\n3.343781\n78.242\n68.440\n\n\nPuerto Rico\n72.739333\n3.984267\n78.746\n64.280\n\n\nCuba\n71.045083\n6.022798\n78.273\n59.421\n\n\nUruguay\n70.781583\n3.342937\n76.384\n66.071\n\n\n\n\n\n\n\n\n# get the mean life expectancy for the 'Americas' continent\ngrouped_continent_contry.loc['Americas']['mean'].head()\n\ncountry\nCanada           74.902750\nUnited States    73.478500\nPuerto Rico      72.739333\nCuba             71.045083\nUruguay          70.781583\nName: mean, dtype: float64\n\n\n\n# another way to get the mean life expectancy for the 'Americas' continent\ngrouped_continent_contry.loc['Americas', 'mean'].head()\n\ncountry\nCanada           74.902750\nUnited States    73.478500\nPuerto Rico      72.739333\nCuba             71.045083\nUruguay          70.781583\nName: mean, dtype: float64\n\n\nYou can use a tuple to access data for specific levels in a multi-level index.\n\n# get the observations for the 'United States' country\ngrouped_continent_contry.loc[( 'Americas', 'United States')]\n\nmean    73.478500\nstd      3.343781\nmax     78.242000\nmin     68.440000\nName: (Americas, United States), dtype: float64\n\n\n\ngrouped_continent_contry.loc[( 'Americas', 'United States'), ['mean', 'std']]\n\nmean    73.478500\nstd      3.343781\nName: (Americas, United States), dtype: float64\n\n\n\ngdp_lifeExp_data.columns\n\nIndex(['country', 'continent', 'year', 'lifeExp', 'pop', 'gdpPercap'], dtype='object')\n\n\nFinally, you can use reset_index() to convert the hierarchical index into a regular index, allowing you to apply the standard subsetting and filtering methods covered in previous chapters\n\ngrouped_continent_contry.reset_index().head()\n\n\n\n\n\n\n\n\ncontinent\ncountry\nmean\nstd\nmax\nmin\n\n\n\n\n0\nEurope\nIceland\n76.511417\n3.026593\n81.757\n72.49\n\n\n1\nEurope\nSweden\n76.177000\n3.003990\n80.884\n71.86\n\n\n2\nEurope\nNorway\n75.843000\n2.423994\n80.196\n72.67\n\n\n3\nEurope\nNetherlands\n75.648500\n2.486363\n79.762\n72.13\n\n\n4\nEurope\nSwitzerland\n75.565083\n4.011572\n81.701\n69.62\n\n\n\n\n\n\n\n\n\n12.3.4 Grouping by multiple columns and aggregating multiple variables\n\n#Grouping by multiple columns\ngrouped_continent_contry_multi = gdp_lifeExp_data.groupby(['continent', 'country','year'])[ ['lifeExp', 'pop', 'gdpPercap']].agg(['mean', 'max', 'min'])\ngrouped_continent_contry_multi\n\n\n\n\n\n\n\n\n\n\nlifeExp\npop\ngdpPercap\n\n\n\n\n\nmean\nmax\nmin\nmean\nmax\nmin\nmean\nmax\nmin\n\n\ncontinent\ncountry\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n1952\n43.077\n43.077\n43.077\n9279525.0\n9279525\n9279525\n2449.008185\n2449.008185\n2449.008185\n\n\n1957\n45.685\n45.685\n45.685\n10270856.0\n10270856\n10270856\n3013.976023\n3013.976023\n3013.976023\n\n\n1962\n48.303\n48.303\n48.303\n11000948.0\n11000948\n11000948\n2550.816880\n2550.816880\n2550.816880\n\n\n1967\n51.407\n51.407\n51.407\n12760499.0\n12760499\n12760499\n3246.991771\n3246.991771\n3246.991771\n\n\n1972\n54.518\n54.518\n54.518\n14760787.0\n14760787\n14760787\n4182.663766\n4182.663766\n4182.663766\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nOceania\nNew Zealand\n1987\n74.320\n74.320\n74.320\n3317166.0\n3317166\n3317166\n19007.191290\n19007.191290\n19007.191290\n\n\n1992\n76.330\n76.330\n76.330\n3437674.0\n3437674\n3437674\n18363.324940\n18363.324940\n18363.324940\n\n\n1997\n77.550\n77.550\n77.550\n3676187.0\n3676187\n3676187\n21050.413770\n21050.413770\n21050.413770\n\n\n2002\n79.110\n79.110\n79.110\n3908037.0\n3908037\n3908037\n23189.801350\n23189.801350\n23189.801350\n\n\n2007\n80.204\n80.204\n80.204\n4115771.0\n4115771\n4115771\n25185.009110\n25185.009110\n25185.009110\n\n\n\n\n1704 rows × 9 columns\n\n\n\nBreaking Down Grouping and Aggregation\n\nGrouping by Multiple Columns:\nIn this example, we are grouping the data by three columns: continent, country, and year. This creates groups based on unique combinations of these columns.\nAggregating Multiple Variables:\nWe apply multiple aggregation functions (mean, std, max, and min) to multiple variables (lifeExp, pop, and gdpPercap).\n\nThis type of operation is commonly referred to as “multi-column grouping with multiple aggregations” in pandas. It’s a powerful approach because it allows us to obtain a detailed statistical summary for each combination of grouping columns across several variables.\n\n# its columns are also two levels deep\ngrouped_continent_contry_multi.columns.nlevels\n\n2\n\n\n\n# pass a tuple to the loc() method to access the values of the multi-level columns with a multi-level index\ngrouped_continent_contry_multi.loc[('Americas','United States'), ('lifeExp', 'mean')]\n\nyear\n1952    68.440\n1957    69.490\n1962    70.210\n1967    70.760\n1972    71.340\n1977    73.380\n1982    74.650\n1987    75.020\n1992    76.090\n1997    76.810\n2002    77.310\n2007    78.242\nName: (lifeExp, mean), dtype: float64",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html#advanced-operations-within-groups-apply-transform-and-filter",
    "href": "Data aggregation.html#advanced-operations-within-groups-apply-transform-and-filter",
    "title": "12  Data Grouping and Aggregation",
    "section": "12.4 Advanced Operations within groups: apply(), transform(), and filter()",
    "text": "12.4 Advanced Operations within groups: apply(), transform(), and filter()\n\n12.4.1 Using apply() on groups\nThe apply() function applies a custom function to each group, allowing for flexible operations. The function can return either a scalar, Series, or DataFrame.\nExample: Consider the life expectancy dataset, find the top 3 life expectancy values for each continent\nWe’ll first define a function that sorts a dataset by decreasing life expectancy and returns the top 3 rows. Then, we’ll apply this function on each group using the apply() method of the GroupBy object.\n\n# Define a function to get the top 3 rows based on life expectancy for each group\ndef top_3_life_expectancy(group):\n    return group.nlargest(3, 'lifeExp')\n\n\n#Defining the groups in the data\ngrouped_gdpcapital_data = gdp_lifeExp_data.groupby('continent')\n\nNow we’ll use the apply() method to apply the top_3_life_expectancy() function on each group of the object grouped_gdpcapital_data.\n\n# Apply the function to each continent group\ntop_life_expectancy = gdp_lifeExp_data.groupby('continent')[['continent', 'country', 'year', 'lifeExp', 'gdpPercap']].apply(top_3_life_expectancy).reset_index(drop=True)\n\n# Display the result\ntop_life_expectancy.head()\n\n\n\n\n\n\n\n\ncontinent\ncountry\nyear\nlifeExp\ngdpPercap\n\n\n\n\n0\nAfrica\nReunion\n2007\n76.442\n7670.122558\n\n\n1\nAfrica\nReunion\n2002\n75.744\n6316.165200\n\n\n2\nAfrica\nReunion\n1997\n74.772\n6071.941411\n\n\n3\nAmericas\nCanada\n2007\n80.653\n36319.235010\n\n\n4\nAmericas\nCanada\n2002\n79.770\n33328.965070\n\n\n\n\n\n\n\nThe top_3_life_expectancy() function is applied to each group, and the results are concatenated internally with the concat() function. The output therefore has a hierarchical index whose outer level indices are the group keys.\nWe can also use a lambda function instead of separately defining the function top_3_life_expectancy():\n\n\n# Use a lambda function to get the top 3 life expectancy values for each continent\ntop_life_expectancy = (\n    gdp_lifeExp_data\n    .groupby('continent')[['continent', 'country', 'year', 'lifeExp', 'gdpPercap']]  # Avoid adding group labels in the index\n    .apply(lambda x: x.nlargest(3, 'lifeExp'))\n    .reset_index(drop=True)\n)\n\n# Display the result\ntop_life_expectancy.head()\n\n\n\n\n\n\n\n\ncontinent\ncountry\nyear\nlifeExp\ngdpPercap\n\n\n\n\n0\nAfrica\nReunion\n2007\n76.442\n7670.122558\n\n\n1\nAfrica\nReunion\n2002\n75.744\n6316.165200\n\n\n2\nAfrica\nReunion\n1997\n74.772\n6071.941411\n\n\n3\nAmericas\nCanada\n2007\n80.653\n36319.235010\n\n\n4\nAmericas\nCanada\n2002\n79.770\n33328.965070\n\n\n\n\n\n\n\n\n\n12.4.2 Using transform() on Groups\nThe transform() function applies a function to each group and returns a Series aligned with the original DataFrame’s index. This makes it suitable for adding or modifying columns based on group-level calculations.\nRecall that in the data cleaning and preparation chapter, we imputed missing values based on correlated variables in the dataset.\nIn the example provided, some countries had missing values for GDP per capita. To handle this, we imputed the missing GDP per capita for each country using the average GDP per capita of its corresponding continent.\nNow, we’ll explore an alternative approach using groupby() and transform() to perform this imputation.\nLet us read the datasets and the function that makes a visualization to compare the imputed values with the actual values.\n\n#Importing data with missing values\ngdp_missing_data = pd.read_csv('./Datasets/GDP_missing_data.csv')\n\n#Importing data with all values\ngdp_complete_data = pd.read_csv('./Datasets/GDP_complete_data.csv')\n\n\n#Index of rows with missing values for GDP per capita\nnull_ind_gdpPC = gdp_missing_data.index[gdp_missing_data.gdpPerCapita.isnull()]\n\n#Defining a function to plot the imputed values vs actual values \ndef plot_actual_vs_predicted():\n    fig, ax = plt.subplots(figsize=(8, 6))\n    plt.rc('xtick', labelsize=15) \n    plt.rc('ytick', labelsize=15) \n    x = gdp_complete_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    y = gdp_imputed_data.loc[null_ind_gdpPC,'gdpPerCapita']\n    plt.scatter(x,y)\n    z=np.polyfit(x,y,1)\n    p=np.poly1d(z)\n    plt.plot(x,x,color='orange')\n    plt.xlabel('Actual GDP per capita',fontsize=20)\n    plt.ylabel('Imputed GDP per capita',fontsize=20)\n    ax.xaxis.set_major_formatter('${x:,.0f}')\n    ax.yaxis.set_major_formatter('${x:,.0f}')\n    rmse = np.sqrt(((x-y).pow(2)).mean())\n    print(\"RMSE=\",rmse)\n\nApproach 1: Using the approach we used in the previous chapter\n\n#Finding the mean GDP per capita of the continent\navg_gdpPerCapita = gdp_missing_data['gdpPerCapita'].groupby(gdp_missing_data['continent']).mean()\n\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_data.copy()\n\n#Replacing missing GDP per capita with the mean GDP per capita for the corresponding continent\nfor cont in avg_gdpPerCapita.index:\n    gdp_imputed_data.loc[(gdp_imputed_data.continent==cont) & (gdp_imputed_data.gdpPerCapita.isnull()),\n                     'gdpPerCapita']=avg_gdpPerCapita[cont]\nplot_actual_vs_predicted()\n\nRMSE= 25473.20645170116\n\n\n\n\n\n\n\n\n\nApproach 2: Using the groupby() and transform() methods.\nThe transform() function is a powerful tool for filling missing values in grouped data. It allows us to apply a function across each group and align the result back to the original DataFrame, making it perfect for filling missing values based on group statistics.\nIn this example, we use transform() to impute missing values in the gdpPerCapita column by filling them with the mean gdpPerCapita of each continent:\n\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_data.copy()\n\n#Grouping data by continent\ngrouped = gdp_missing_data.groupby('continent')\n\n#Imputing missing values with the mean GDP per capita of the continent\ngdp_imputed_data['gdpPerCapita'] = grouped['gdpPerCapita'].transform(lambda x: x.fillna(x.mean()))\n\nplot_actual_vs_predicted()\n\nRMSE= 25473.20645170116\n\n\n\n\n\n\n\n\n\nUsing the transform() function, missing values in gdpPerCapita for each group are filled with the group’s mean gdpPerCapita. This approach is not only more convenient to write but also faster compared to using for loops. While a for loop imputes missing values one group at a time, transform() performs built-in operations (like mean, sum, etc.) in a way that is optimized internally, making it more efficient.\nLet’s use apply() instead of transform() with groupby()\nPlease copy the code below and run it in your notebook:\n#Creating a copy of missing data to impute missing values\ngdp_imputed_data = gdp_missing_data.copy()\n\n#Grouping data by continent\ngrouped = gdp_missing_data.groupby('continent')\n\n#Applying the lambda function on the 'gdpPerCapita' column of the groups\ngdp_imputed_data['gdpPerCapita'] = grouped['gdpPerCapita'].apply(lambda x: x.fillna(x.mean()))\n\nplot_actual_vs_predicted()\nWhy we ran into this error? and apply() doesn’t work?\nHere’s a deeper look at why apply() doesn’t work as expected here:\n\n12.4.2.1 Behavior of groupby().apply() vs. groupby().transform()\n\ngroupby().apply(): This method applies a function to each group and returns the result with a hierarchical (multi-level) index by default. This hierarchical index can make it difficult to align the result back to a single column in the original DataFrame.\ngroupby().transform(): In contrast, transform() is specifically designed to apply a function to each group and return a Series that is aligned with the original DataFrame’s index. This alignment makes it directly compatible for assignment to a new or existing column in the original DataFrame.\n\n\n\n12.4.2.2 Why transform() Works for Imputation\nWhen using transform() to fill missing values, it applies the function (e.g., fillna(x.mean())) based on each group’s statistics, such as the mean, while keeping the result aligned with the original DataFrame’s index. This allows for smooth assignment to a column in the DataFrame without any index mismatch issues.\nAdditionally, transform() applies the function to each element in a group independently and returns a result that has the same shape as the original data, making it ideal for adding or modifying columns.\n\n\n\n12.4.3 Using filter() on Groups\nThe filter() function filters entire groups based on a condition. It evaluates each group and keeps only those that meet the specified criteria.\nExample: Keep only the countries where the mean life expectancy is greater than 70\n\n# keep only the continent where the mean life expectancy is greater than 74\ngdp_lifeExp_data.groupby('continent').filter(lambda x: x['lifeExp'].mean() &gt; 74)['continent'].unique()\n\narray(['Oceania'], dtype=object)\n\n\n\n# keep only the country where the mean life expectancy is greater than 74\ngdp_lifeExp_data.groupby('country').filter(lambda x: x['lifeExp'].mean() &gt; 74)['country'].unique()\n\narray(['Australia', 'Canada', 'Denmark', 'France', 'Iceland', 'Italy',\n       'Japan', 'Netherlands', 'Norway', 'Spain', 'Sweden', 'Switzerland'],\n      dtype=object)\n\n\nUsing .nunique() get the number of countries that satisfy this condition\n\ngdp_lifeExp_data.groupby('country').filter(lambda x: x['lifeExp'].mean() &gt; 74)['country'].nunique()\n\n12",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html#sampling-data-by-group",
    "href": "Data aggregation.html#sampling-data-by-group",
    "title": "12  Data Grouping and Aggregation",
    "section": "12.5 Sampling data by group",
    "text": "12.5 Sampling data by group\nIf a dataset contains a large number of observations, operating on it can be computationally expensive. Instead, working on a sample of entire observations is a more efficient alterative. The groupby() method combined with apply() can be used for stratified random sampling from a large dataset.\nBefore taking the random sample, let us find the number of countries in each continent.\n\ngdp_lifeExp_data.continent.value_counts()\n\ncontinent\nAfrica      624\nAsia        396\nEurope      360\nAmericas    300\nOceania      24\nName: count, dtype: int64\n\n\nLet us take a random sample of 650 observations from the entire dataset.\n\nsample_lifeExp_data = gdp_lifeExp_data.sample(650)\n\nNow, let us see the number of countries of each continent in our sample.\n\nsample_lifeExp_data.continent.value_counts()\n\ncontinent\nAfrica      241\nAsia        149\nEurope      142\nAmericas    109\nOceania       9\nName: count, dtype: int64\n\n\nSome of the continent have a very low representation in the data. To rectify this, we can take a random sample of 130 observations from each of the 5 continents. In other words, we can take a random sample from each of the continent-based groups.\n\nevenly_sampled_lifeExp_data = gdp_lifeExp_data.groupby('continent').apply(lambda x:x.sample(130, replace=True), include_groups=False)\n\ngroup_sizes = evenly_sampled_lifeExp_data.groupby(level=0).size()\nprint(group_sizes)\n\ncontinent\nAfrica      130\nAmericas    130\nAsia        130\nEurope      130\nOceania     130\ndtype: int64\n\n\nThe above stratified random sample equally represents all the continent.",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html#corr-correlation-by-group",
    "href": "Data aggregation.html#corr-correlation-by-group",
    "title": "12  Data Grouping and Aggregation",
    "section": "12.6 corr(): Correlation by group",
    "text": "12.6 corr(): Correlation by group\nThe corr() method of the GroupBy object returns the correlation between all pairs of columns within each group.\nExample: Find the correlation between lifeExp and gdpPercap for each continent-country level combination.\n\ngdp_lifeExp_data.groupby(['continent','country']).apply(lambda x:x['lifeExp'].corr(x['gdpPercap']), include_groups=False)\n\ncontinent  country       \nAfrica     Algeria           0.904471\n           Angola           -0.301079\n           Benin             0.843949\n           Botswana          0.005597\n           Burkina Faso      0.881677\n                               ...   \nEurope     Switzerland       0.980715\n           Turkey            0.954455\n           United Kingdom    0.989893\nOceania    Australia         0.986446\n           New Zealand       0.974493\nLength: 142, dtype: float64\n\n\nLife expectancy is closely associated with GDP per capita across most continent-country combinations.",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html#pivot_table",
    "href": "Data aggregation.html#pivot_table",
    "title": "12  Data Grouping and Aggregation",
    "section": "12.7 pivot_table()",
    "text": "12.7 pivot_table()\nThe pivot_table() function in pandas is a powerful tool for performing groupwise aggregation in a structured format, similar to Excel’s pivot tables. It allows you to create a summary of data by grouping and aggregating it based on specified columns. Here’s an overview of how pivot_table() works for groupwise aggregation:\nNote that pivot_table() is the same as pivot() except that pivot_table() aggregates the data as well in addition to re-arranging it.\nExample: Consider the life expectancy dataset, calculate the average life expectancy for each country and year combination\n\npd.pivot_table(data = gdp_lifeExp_data, values = 'lifeExp',index = 'country', columns ='year',aggfunc = 'mean',margins = True)\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\nAll\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n28.80100\n30.332000\n31.997000\n34.02000\n36.088000\n38.438000\n39.854000\n40.822000\n41.674000\n41.763000\n42.129000\n43.828000\n37.478833\n\n\nAlbania\n55.23000\n59.280000\n64.820000\n66.22000\n67.690000\n68.930000\n70.420000\n72.000000\n71.581000\n72.950000\n75.651000\n76.423000\n68.432917\n\n\nAlgeria\n43.07700\n45.685000\n48.303000\n51.40700\n54.518000\n58.014000\n61.368000\n65.799000\n67.744000\n69.152000\n70.994000\n72.301000\n59.030167\n\n\nAngola\n30.01500\n31.999000\n34.000000\n35.98500\n37.928000\n39.483000\n39.942000\n39.906000\n40.647000\n40.963000\n41.003000\n42.731000\n37.883500\n\n\nArgentina\n62.48500\n64.399000\n65.142000\n65.63400\n67.065000\n68.481000\n69.942000\n70.774000\n71.868000\n73.275000\n74.340000\n75.320000\n69.060417\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nWest Bank and Gaza\n43.16000\n45.671000\n48.127000\n51.63100\n56.532000\n60.765000\n64.406000\n67.046000\n69.718000\n71.096000\n72.370000\n73.422000\n60.328667\n\n\nYemen, Rep.\n32.54800\n33.970000\n35.180000\n36.98400\n39.848000\n44.175000\n49.113000\n52.922000\n55.599000\n58.020000\n60.308000\n62.698000\n46.780417\n\n\nZambia\n42.03800\n44.077000\n46.023000\n47.76800\n50.107000\n51.386000\n51.821000\n50.821000\n46.100000\n40.238000\n39.193000\n42.384000\n45.996333\n\n\nZimbabwe\n48.45100\n50.469000\n52.358000\n53.99500\n55.635000\n57.674000\n60.363000\n62.351000\n60.377000\n46.809000\n39.989000\n43.487000\n52.663167\n\n\nAll\n49.05762\n51.507401\n53.609249\n55.67829\n57.647386\n59.570157\n61.533197\n63.212613\n64.160338\n65.014676\n65.694923\n67.007423\n59.474439\n\n\n\n\n143 rows × 13 columns\n\n\n\nExplanation\n\nvalues: Specifies the column to aggregate (e.g., lifeExp in our example).\nindex: Groups by the rows based on the country column.\ncolumns: Groups by the columns based on the year column.\naggfunc: Uses mean to calculate the average life expectancy.\n\nCommon Aggregation Functions in pivot_table()\nYou can use various aggregation functions within pivot_table() to summarize data:\n\nmean – Calculates the average of values within each group.\nsum – Computes the total of values within each group.\ncount – Counts the number of non-null entries within each group.\nmin and max – Finds the minimum and maximum values within each group.\n\nWe can also use custom GroupBy aggregate functions with pivot_table().\nExample: Find the \\(90^{th}\\) percentile of life expectancy for each country and year combination\n\npd.pivot_table(data = gdp_lifeExp_data, values = 'lifeExp',index = 'country', columns ='year',aggfunc = lambda x:np.percentile(x,90))\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n28.801\n30.332\n31.997\n34.020\n36.088\n38.438\n39.854\n40.822\n41.674\n41.763\n42.129\n43.828\n\n\nAlbania\n55.230\n59.280\n64.820\n66.220\n67.690\n68.930\n70.420\n72.000\n71.581\n72.950\n75.651\n76.423\n\n\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n70.994\n72.301\n\n\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n41.003\n42.731\n\n\nArgentina\n62.485\n64.399\n65.142\n65.634\n67.065\n68.481\n69.942\n70.774\n71.868\n73.275\n74.340\n75.320\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nVietnam\n40.412\n42.887\n45.363\n47.838\n50.254\n55.764\n58.816\n62.820\n67.662\n70.672\n73.017\n74.249\n\n\nWest Bank and Gaza\n43.160\n45.671\n48.127\n51.631\n56.532\n60.765\n64.406\n67.046\n69.718\n71.096\n72.370\n73.422\n\n\nYemen, Rep.\n32.548\n33.970\n35.180\n36.984\n39.848\n44.175\n49.113\n52.922\n55.599\n58.020\n60.308\n62.698\n\n\nZambia\n42.038\n44.077\n46.023\n47.768\n50.107\n51.386\n51.821\n50.821\n46.100\n40.238\n39.193\n42.384\n\n\nZimbabwe\n48.451\n50.469\n52.358\n53.995\n55.635\n57.674\n60.363\n62.351\n60.377\n46.809\n39.989\n43.487\n\n\n\n\n142 rows × 12 columns",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html#crosstab",
    "href": "Data aggregation.html#crosstab",
    "title": "12  Data Grouping and Aggregation",
    "section": "12.8 crosstab()",
    "text": "12.8 crosstab()\n\n12.8.1 Basic Usage of crosstab()\nThe crosstab() method is a special case of a pivot table for computing group frequncies (or size of each group).\n\n# create a basic crosstab to see counts of countries in each continent\npd.crosstab(gdp_lifeExp_data['continent'], columns='count')\n\n\n\n\n\n\n\ncol_0\ncount\n\n\ncontinent\n\n\n\n\n\nAfrica\n624\n\n\nAmericas\n300\n\n\nAsia\n396\n\n\nEurope\n360\n\n\nOceania\n24\n\n\n\n\n\n\n\nUse the margins=True argument to add totals (row and column margins) to the table. The All row and column provide totals for each age group and gender category.\n\n# create a crosstab to see counts of countries in each continent and year\npd.crosstab(gdp_lifeExp_data['continent'], gdp_lifeExp_data['year'], margins=True)\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\nAll\n\n\ncontinent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n52\n624\n\n\nAmericas\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n300\n\n\nAsia\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n33\n396\n\n\nEurope\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n30\n360\n\n\nOceania\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n24\n\n\nAll\n142\n142\n142\n142\n142\n142\n142\n142\n142\n142\n142\n142\n1704\n\n\n\n\n\n\n\nThis table shows the count of each year in each continent group, helping us understand the year distribution across different continent groups. We may often use it to check if the data is representative of all groups that are of interest to us.\n\n\n12.8.2 Using crosstab() with Aggregation Functions\nYou can specify a values column and an aggregation function (aggfunc) to summarize numerical data for each combination of categorical variables.\nExample: find the mean life expectancy for each continent and year\n\n# find the mean life expectancy for each country in each continent and year\npd.crosstab(gdp_lifeExp_data['continent'], gdp_lifeExp_data['year'], values=gdp_lifeExp_data['lifeExp'], aggfunc='mean')\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncontinent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\n39.135500\n41.266346\n43.319442\n45.334538\n47.450942\n49.580423\n51.592865\n53.344788\n53.629577\n53.598269\n53.325231\n54.806038\n\n\nAmericas\n53.279840\n55.960280\n58.398760\n60.410920\n62.394920\n64.391560\n66.228840\n68.090720\n69.568360\n71.150480\n72.422040\n73.608120\n\n\nAsia\n46.314394\n49.318544\n51.563223\n54.663640\n57.319269\n59.610556\n62.617939\n64.851182\n66.537212\n68.020515\n69.233879\n70.728485\n\n\nEurope\n64.408500\n66.703067\n68.539233\n69.737600\n70.775033\n71.937767\n72.806400\n73.642167\n74.440100\n75.505167\n76.700600\n77.648600\n\n\nOceania\n69.255000\n70.295000\n71.085000\n71.310000\n71.910000\n72.855000\n74.290000\n75.320000\n76.945000\n78.190000\n79.740000\n80.719500",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data aggregation.html#independent-study",
    "href": "Data aggregation.html#independent-study",
    "title": "12  Data Grouping and Aggregation",
    "section": "12.9 Independent Study",
    "text": "12.9 Independent Study\n\n12.9.1 Practice exercise 1\nRead the table consisting of GDP per capita of countries from the webpage: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita .\nTo only read the relevant table, read the tables that contain the word ‘Country’.\nEstimate the GDP per capita of each country as the average of the estimates of the three agencies - IMF, United Nations and World Bank.\nWe need to do a bit of data cleaning before we could directly use the groupby() function. Follow the steps below:\n\nDrop the “Year” column\nDrop the level 1 column name (innermost level column)\nApply the following function on all the columns of the “Estimate” to convert them to numeric: f = lambda x:pd.to_numeric(x,errors = 'coerce')\nSet the Country/Territory column as the index\nConvert all other columns into numeric and coerce errors using ‘errors=’coerce’`\nDrop rows with NaN values\nFind the average GDP per capital across the three agencies\n\n\ndfs = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita', match = 'Country')\ngdp_data  = dfs[0]\ngdp_data.head()\n\n\n\n\n\n\n\n\nCountry/Territory\nIMF[4][5]\nWorld Bank[6]\nUnited Nations[7]\n\n\n\nCountry/Territory\nEstimate\nYear\nEstimate\nYear\nEstimate\nYear\n\n\n\n\n0\nMonaco\n—\n—\n240862\n2022\n240535\n2022\n\n\n1\nLiechtenstein\n—\n—\n187267\n2022\n197268\n2022\n\n\n2\nLuxembourg\n135321\n2024\n128259\n2023\n125897\n2022\n\n\n3\nBermuda\n—\n—\n123091\n2022\n117568\n2022\n\n\n4\nSwitzerland\n106098\n2024\n99995\n2023\n93636\n2022\n\n\n\n\n\n\n\n\n# Flatten the MultiIndex to access column names\ngdp_data.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in gdp_data.columns]\n\n# Drop all columns containing \"Year\"\ngdp_data = gdp_data.loc[:, ~gdp_data.columns.str.contains('Year', case=False)]\n\ngdp_data = gdp_data.rename(columns={\"Country/Territory Country/Territory\": \"Country/Territory\"})\n\ngdp_data.head()\n\n\n\n\n\n\n\n\nCountry/Territory\nIMF\nWorld Bank\nUnited Nations\n\n\n\n\n0\nMonaco\n—\n240862\n240535\n\n\n1\nLiechtenstein\n—\n187267\n197268\n\n\n2\nLuxembourg\n135321\n128259\n125897\n\n\n3\nBermuda\n—\n123091\n117568\n\n\n4\nSwitzerland\n106098\n99995\n93636\n\n\n\n\n\n\n\n\nimport re\ncolumn_name_cleaner = lambda x:re.split(r'\\[', x)[0]\n\ngdp_data.columns = gdp_data.columns.map(column_name_cleaner)\ngdp_data.head()\n\n\n\n\n\n\n\n\nCountry/Territory\nIMF\nWorld Bank\nUnited Nations\n\n\n\n\n0\nMonaco\n—\n240862\n240535\n\n\n1\nLiechtenstein\n—\n187267\n197268\n\n\n2\nLuxembourg\n135321\n128259\n125897\n\n\n3\nBermuda\n—\n123091\n117568\n\n\n4\nSwitzerland\n106098\n99995\n93636\n\n\n\n\n\n\n\n\n# set the country column as the index\ngdp_data.set_index('Country/Territory', inplace=True)\n\n# convert all other columns into numeric and coerce errors\ngdp_data = gdp_data.apply(pd.to_numeric, errors='coerce')\n\n# drop rows with NaN values\ngdp_data.dropna(inplace=True)\n\n#find the average GDP per capita\ngdp_data['Average GDP per capita'] = gdp_data.mean(axis=1)\ngdp_data.head()\n\n\n\n\n\n\n\n\nIMF\nWorld Bank\nUnited Nations\nAverage GDP per capita\n\n\nCountry/Territory\n\n\n\n\n\n\n\n\nLuxembourg\n135321.0\n128259.0\n125897.0\n129825.666667\n\n\nSwitzerland\n106098.0\n99995.0\n93636.0\n99909.666667\n\n\nIreland\n103500.0\n103685.0\n105993.0\n104392.666667\n\n\nNorway\n90434.0\n87962.0\n106623.0\n95006.333333\n\n\nSingapore\n89370.0\n84734.0\n78115.0\n84073.000000\n\n\n\n\n\n\n\n\n\n12.9.2 Practice exercise 2\nRead the spotify dataset from spotify_data.csv that contains information about tracks and artists\n\n12.9.2.1 \nFind the mean and standard deviation of the track popularity for each genre.\n\n\n12.9.2.2 \nCreate a new categorical column, energy_lvl, with two levels – ‘Low energy’ and ‘High energy’ – using equal-sized bins based on the track’s energy level. Then, calculate the mean, standard deviation, and 90th percentile of track popularity for each genre and energy level combination\n\n\n12.9.2.3 \nFind the mean and standard deviation of track popularity and danceability for each genre and energy level. What insights you can gain from the generated table\n\n\n12.9.2.4 \nFor each genre and energy level, find the mean and standard deviation of the track popularity, and the minimum and maximum values of loudness.\n\n\n12.9.2.5 \nFind the most popular artist from each genre.\n\n\n12.9.2.6 \nFilter the first 4 columns of the spotify dataset. Drop duplicate observartions in the resulting dataset using the Pandas DataFrame method drop_duplicates(). Find the top 3 most popular artists for each genre.\n\n\n12.9.2.7 \nThe spotify dataset has more than 200k observations. It may be expensive to operate with so many observations. Take a random sample of 650 observations to analyze spotify data, such that all genres are equally represented.\n\n\n12.9.2.8 \nFind the correlation between danceability and track popularity for each genre-energy level combination.\n\n\n12.9.2.9 \nFind the mean of track popularity for each genre-energy lvl combination such that each row corresponds to a genre, and the energy levels correspond to columns.\nHints: using pivot_table()\n\n\n12.9.2.10 \nFind the \\(90^{th}\\) percentile of track popularity for each genre-energy lvl combination such that each row corresponds to a genre, and the energy levels correspond to columns.\nHints: using pivot_table()\n\n\n12.9.2.11 \nFind the number of observations in each group, where each groups corresponds to a distinct genre-energy lvl combination\n\n\n12.9.2.12 \nFind the percentage of observations in each group of the above table.\n\n\n12.9.2.13 \nWhat percentage of unique tracks are contributed by the top 5 artists of each genre?\nHint: Find the top 5 artists based on artist_popularity for each genre. Count the total number of unique tracks (track_name) contributed by these artists. Divide this number by the total number of unique tracks in the data. The nunique() function will be useful.",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Grouping and Aggregation</span>"
    ]
  },
  {
    "objectID": "Data wrangling.html",
    "href": "Data wrangling.html",
    "title": "13  Data Reshaping and Enrichment",
    "section": "",
    "text": "13.1 Data Reshaping\nIn this chapter, we will explore how to reshape and enrich data using pandas. Data reshaping and enriching are essential steps in data preprocessing because they help transform raw data into a more structured, useful, and meaningful format that can be effectively used for analysis or modeling.\nData reshaping involves altering the structure of the data to suit the requirements of the analysis or modeling process. This is often needed when the data is in a format that is difficult to analyze or when it needs to be organized in a specific way.",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Reshaping and Enrichment</span>"
    ]
  },
  {
    "objectID": "Data wrangling.html#data-reshaping",
    "href": "Data wrangling.html#data-reshaping",
    "title": "13  Data Reshaping and Enrichment",
    "section": "",
    "text": "13.1.1 Why Data Reshaping is Important:\n\nImproves Data Organization: Raw data may be in a long format, where each observation is recorded on a separate row, or it may be in a wide format, where the same variable is spread across multiple columns. Reshaping allows you to reorganize the data into a format that is more suitable for analysis, such as turning multiple columns into a single column (melting) or aggregating rows into summary statistics (pivoting).\nFacilitates Analysis: Some analysis techniques require specific data formats. For example, many machine learning algorithms require data in a matrix form, where each row represents a sample and each column represents a feature. Reshaping helps to convert data into this format.\nSimplifies Visualization: Visualizing data is easier when it is in the right shape. For instance, plotting categorical data across time is easier when the data is in a tidy format, rather than having multiple variables spread across columns.\nEfficient Grouping and Aggregation: In some cases, you may want to group data by certain categories or time periods, and reshaping can help aggregate data effectively for summarization.\n\n\n\n13.1.2 Wide vs. Long Data Format in Data Reshaping\n\nWide Format: Each variable in its own column; each row represents a unit, and data for multiple variables is spread across columns.\nLong Format: Variables are stacked into one column per variable, with additional columns used to indicate time, categories, or other distinctions, leading to a greater number of rows.\n\nTo reshape between these two formats, you can use pandas functions such as pivot_table(), melt(), stack(), and unstack(). Let’s next take a look at each of these methods one by one.\nThroughout this section, we will continue using the gdp_lifeExpectancy.csv dataset. Let’s start by reading the CSV file into a pandas DataFrame.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\n# Load the data\ngdp_lifeExp_data = pd.read_csv('./Datasets/gdp_lifeExpectancy.csv')\ngdp_lifeExp_data.head()\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n\n\n\n\n13.1.3 Pivoting “long” to “wide” using pivot_table()\nIn the last chapter, we learned that pivot_table() is a powerful tool for performing groupwise aggregation in a structured format. The pivot_table() function is used to reshape data by specifying columns for the index, columns, and values.\n\n13.1.3.1 Pivoting a single column\nConsider the life expectancy dataset. Let’s calculate the average life expectancy for each combination of country and year.\n\ngdp_lifeExp_data_pivot = gdp_lifeExp_data.pivot_table(index=['continent','country'], columns='year', values='lifeExp', aggfunc='mean')\ngdp_lifeExp_data_pivot.head()\n\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n70.994\n72.301\n\n\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n41.003\n42.731\n\n\nBenin\n38.223\n40.358\n42.618\n44.885\n47.014\n49.190\n50.904\n52.337\n53.919\n54.777\n54.406\n56.728\n\n\nBotswana\n47.622\n49.618\n51.520\n53.298\n56.024\n59.319\n61.484\n63.622\n62.745\n52.556\n46.634\n50.728\n\n\nBurkina Faso\n31.975\n34.906\n37.814\n40.697\n43.591\n46.137\n48.122\n49.557\n50.260\n50.324\n50.650\n52.295\n\n\n\n\n\n\n\nExplanation\n\nvalues: Specifies the column to aggregate (e.g., lifeExp in our example).\nindex: Groups by the rows based on the continent and country column.\ncolumns: Groups by the columns based on the year column.\naggfunc: Uses mean to calculate the average life expectancy.\n\nWith values of year as columns, it is easy to compare any metric for different years.\n\n#visualizing the change in life expectancy of all countries in 2007 as compared to that in 1957, i.e., the overall change in life expectancy in 50 years. \nsns.scatterplot(data = gdp_lifeExp_data_pivot, x = 1957,y=2007,hue = 'continent')\nsns.lineplot(data = gdp_lifeExp_data_pivot, x = 1957,y = 1957);\n\n\n\n\n\n\n\n\nObserve that for some African countries, the life expectancy has decreased after 50 years. It is worth investigating these countries to identify factors associated with the decrease.\nAnother way to visualize a pivot table is by using a heatmap, which represents the 2D table as a matrix of colors, making it easier to interpret patterns and trends.\n\n# Plotting a heatmap to visualize the life expectancy of all countries in all years, figure size is set to 15x10\nplt.figure(figsize=(10,10))\nplt.title(\"Life expectancy of all countries in all years\")\nsns.heatmap(gdp_lifeExp_data_pivot);\n\n\n\n\n\n\n\n\nCommon Aggregation Functions in pivot_table()\nYou can use various aggregation functions within pivot_table() to summarize data:\n\nmean – Calculates the average of values within each group.\nsum – Computes the total of values within each group.\ncount – Counts the number of non-null entries within each group.\nmin and max – Finds the minimum and maximum values within each group.\n\nWe can also define our own custom aggregation function and pass it to the aggfunc parameter of the pivot_table() function\nFor Example: Find the \\(90^{th}\\) percentile of life expectancy for each country and year combination\n\npd.pivot_table(data = gdp_lifeExp_data, values = 'lifeExp',index = 'country', columns ='year',aggfunc = lambda x:np.percentile(x,90))\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n28.801\n30.332\n31.997\n34.020\n36.088\n38.438\n39.854\n40.822\n41.674\n41.763\n42.129\n43.828\n\n\nAlbania\n55.230\n59.280\n64.820\n66.220\n67.690\n68.930\n70.420\n72.000\n71.581\n72.950\n75.651\n76.423\n\n\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n70.994\n72.301\n\n\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n41.003\n42.731\n\n\nArgentina\n62.485\n64.399\n65.142\n65.634\n67.065\n68.481\n69.942\n70.774\n71.868\n73.275\n74.340\n75.320\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nVietnam\n40.412\n42.887\n45.363\n47.838\n50.254\n55.764\n58.816\n62.820\n67.662\n70.672\n73.017\n74.249\n\n\nWest Bank and Gaza\n43.160\n45.671\n48.127\n51.631\n56.532\n60.765\n64.406\n67.046\n69.718\n71.096\n72.370\n73.422\n\n\nYemen, Rep.\n32.548\n33.970\n35.180\n36.984\n39.848\n44.175\n49.113\n52.922\n55.599\n58.020\n60.308\n62.698\n\n\nZambia\n42.038\n44.077\n46.023\n47.768\n50.107\n51.386\n51.821\n50.821\n46.100\n40.238\n39.193\n42.384\n\n\nZimbabwe\n48.451\n50.469\n52.358\n53.995\n55.635\n57.674\n60.363\n62.351\n60.377\n46.809\n39.989\n43.487\n\n\n\n\n142 rows × 12 columns\n\n\n\n\n\n\n13.1.4 Melting “wide” to “long” using melt()\nMelting is the inverse of pivoting. It transforms columns into rows. The melt() function is used when you want to unpivot a DataFrame, turning multiple columns into rows.\nLet’s first consider gdp_lifeExp_data_pivot created in the previous section\n\ngdp_lifeExp_data_pivot.head()\n\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n43.077\n45.685\n48.303\n51.407\n54.518\n58.014\n61.368\n65.799\n67.744\n69.152\n70.994\n72.301\n\n\nAngola\n30.015\n31.999\n34.000\n35.985\n37.928\n39.483\n39.942\n39.906\n40.647\n40.963\n41.003\n42.731\n\n\nBenin\n38.223\n40.358\n42.618\n44.885\n47.014\n49.190\n50.904\n52.337\n53.919\n54.777\n54.406\n56.728\n\n\nBotswana\n47.622\n49.618\n51.520\n53.298\n56.024\n59.319\n61.484\n63.622\n62.745\n52.556\n46.634\n50.728\n\n\nBurkina Faso\n31.975\n34.906\n37.814\n40.697\n43.591\n46.137\n48.122\n49.557\n50.260\n50.324\n50.650\n52.295\n\n\n\n\n\n\n\n\ngdp_lifeExp_data_pivot.melt(value_name='lifeExp', var_name='year', ignore_index=False)\n\n\n\n\n\n\n\n\n\nyear\nlifeExp\n\n\ncontinent\ncountry\n\n\n\n\n\n\nAfrica\nAlgeria\n1952\n43.077\n\n\nAngola\n1952\n30.015\n\n\nBenin\n1952\n38.223\n\n\nBotswana\n1952\n47.622\n\n\nBurkina Faso\n1952\n31.975\n\n\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n2007\n81.701\n\n\nTurkey\n2007\n71.777\n\n\nUnited Kingdom\n2007\n79.425\n\n\nOceania\nAustralia\n2007\n81.235\n\n\nNew Zealand\n2007\n80.204\n\n\n\n\n1704 rows × 2 columns\n\n\n\nWith the above DataFrame, we can visualize the mean life expectancy against year separately for each country in each continent.\n\n\n13.1.5 Stacking and Unstacking using stack() and unstack()\nStacking and unstacking are powerful techniques used to reshape DataFrames by moving data between rows and columns. These operations are particularly useful when working with multi-level index or multi-level columns, where the data is structured in a hierarchical format.\n\nstack(): Converts columns into rows, which means it “stacks” the data into a long format.\nunstack(): Converts rows into columns, reversing the effect of stack().\n\nLet’s use the GDP per capita dataset and create a DataFrame with a multi-level index (e.g., continent, country) and multi-level columns (e.g., lifeExp , gdpPercap, and pop).\n\n# calculate the average and standard deviation of life expectancy, population, and GDP per capita for countries across the years.\n\ngdp_lifeExp_multilevel_data = gdp_lifeExp_data.groupby(['continent', 'country'])[['lifeExp', 'pop', 'gdpPercap']].agg(['mean', 'std'])\ngdp_lifeExp_multilevel_data\n\n\n\n\n\n\n\n\n\nlifeExp\npop\ngdpPercap\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n59.030167\n10.340069\n1.987541e+07\n8.613355e+06\n4426.025973\n1310.337656\n\n\nAngola\n37.883500\n4.005276\n7.309390e+06\n2.672281e+06\n3607.100529\n1165.900251\n\n\nBenin\n48.779917\n6.128681\n4.017497e+06\n2.105002e+06\n1155.395107\n159.741306\n\n\nBotswana\n54.597500\n5.929476\n9.711862e+05\n4.710965e+05\n5031.503557\n4178.136987\n\n\nBurkina Faso\n44.694000\n6.845792\n7.548677e+06\n3.247808e+06\n843.990665\n183.430087\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n75.565083\n4.011572\n6.384293e+06\n8.582009e+05\n27074.334405\n6886.463308\n\n\nTurkey\n59.696417\n9.030591\n4.590901e+07\n1.667768e+07\n4469.453380\n2049.665102\n\n\nUnited Kingdom\n73.922583\n3.378943\n5.608780e+07\n3.174339e+06\n19380.472986\n7388.189399\n\n\nOceania\nAustralia\n74.662917\n4.147774\n1.464931e+07\n3.915203e+06\n19980.595634\n7815.405220\n\n\nNew Zealand\n73.989500\n3.559724\n3.100032e+06\n6.547108e+05\n17262.622813\n4409.009167\n\n\n\n\n142 rows × 6 columns\n\n\n\nThe resulting DataFrame has two levels of index and two levels of columns, as shown below:\n\n# check the level of row and column index\ngdp_lifeExp_multilevel_data.index.nlevels\n\n2\n\n\n\ngdp_lifeExp_multilevel_data.columns.nlevels\n\n2\n\n\nif you want to see the data along with the index and columns (in a more accessible way), you can directly print the .index and .columns attributes.\n\ngdp_lifeExp_multilevel_data.index\n\nMultiIndex([( 'Africa',                  'Algeria'),\n            ( 'Africa',                   'Angola'),\n            ( 'Africa',                    'Benin'),\n            ( 'Africa',                 'Botswana'),\n            ( 'Africa',             'Burkina Faso'),\n            ( 'Africa',                  'Burundi'),\n            ( 'Africa',                 'Cameroon'),\n            ( 'Africa', 'Central African Republic'),\n            ( 'Africa',                     'Chad'),\n            ( 'Africa',                  'Comoros'),\n            ...\n            ( 'Europe',                   'Serbia'),\n            ( 'Europe',          'Slovak Republic'),\n            ( 'Europe',                 'Slovenia'),\n            ( 'Europe',                    'Spain'),\n            ( 'Europe',                   'Sweden'),\n            ( 'Europe',              'Switzerland'),\n            ( 'Europe',                   'Turkey'),\n            ( 'Europe',           'United Kingdom'),\n            ('Oceania',                'Australia'),\n            ('Oceania',              'New Zealand')],\n           names=['continent', 'country'], length=142)\n\n\n\ngdp_lifeExp_multilevel_data.columns\n\nMultiIndex([(  'lifeExp', 'mean'),\n            (  'lifeExp',  'std'),\n            (      'pop', 'mean'),\n            (      'pop',  'std'),\n            ('gdpPercap', 'mean'),\n            ('gdpPercap',  'std')],\n           )\n\n\nAs seen, the index and columns each contain tuples. This is the reason that when performing data subsetting on a DataFrame with multi-level index and columns, you need to supply a tuple to specify the exact location of the data.\n\ngdp_lifeExp_multilevel_data.loc[('Americas', 'United States'), ('lifeExp', 'mean')]\n\n73.4785\n\n\n\ngdp_lifeExp_multilevel_data.columns.values\n\narray([('lifeExp', 'mean'), ('lifeExp', 'std'), ('pop', 'mean'),\n       ('pop', 'std'), ('gdpPercap', 'mean'), ('gdpPercap', 'std')],\n      dtype=object)\n\n\n\n13.1.5.1 Understanding the level in multi-level index and columns\nThe gdp_lifeExp_multilevel_data dataframe have two levels of index and two level of columns, You can reference these levels in various operations, such as grouping, subsetting, or performing aggregations. The level parameter helps identify which part of the multi-level structure you’re working with.\n\ngdp_lifeExp_multilevel_data.index.get_level_values(0)\n\nIndex(['Africa', 'Africa', 'Africa', 'Africa', 'Africa', 'Africa', 'Africa',\n       'Africa', 'Africa', 'Africa',\n       ...\n       'Europe', 'Europe', 'Europe', 'Europe', 'Europe', 'Europe', 'Europe',\n       'Europe', 'Oceania', 'Oceania'],\n      dtype='object', name='continent', length=142)\n\n\n\ngdp_lifeExp_multilevel_data.index.get_level_values(1)\n\n\ngdp_lifeExp_multilevel_data.columns.get_level_values(0)\n\nIndex(['lifeExp', 'lifeExp', 'pop', 'pop', 'gdpPercap', 'gdpPercap'], dtype='object')\n\n\n\ngdp_lifeExp_multilevel_data.columns.get_level_values(1)\n\nIndex(['mean', 'std', 'mean', 'std', 'mean', 'std'], dtype='object')\n\n\n\ngdp_lifeExp_multilevel_data.columns.get_level_values(-1)\n\nIndex(['mean', 'std', 'mean', 'std', 'mean', 'std'], dtype='object')\n\n\nAs seen above, the outermost level corresponds to level=0, and it increases as we move inward to the inner levels. The innermost level can be referred to as -1.\nNow, let’s use the level number in the droplevel() method to see how it works. The syntax for this method is as follows:\nDataFrame.droplevel(level, axis=0)\n\n# drop the first level of row index\ngdp_lifeExp_multilevel_data.droplevel(0, axis=0)\n\n\n\n\n\n\n\n\nlifeExp\npop\ngdpPercap\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\nAlgeria\n59.030167\n10.340069\n1.987541e+07\n8.613355e+06\n4426.025973\n1310.337656\n\n\nAngola\n37.883500\n4.005276\n7.309390e+06\n2.672281e+06\n3607.100529\n1165.900251\n\n\nBenin\n48.779917\n6.128681\n4.017497e+06\n2.105002e+06\n1155.395107\n159.741306\n\n\nBotswana\n54.597500\n5.929476\n9.711862e+05\n4.710965e+05\n5031.503557\n4178.136987\n\n\nBurkina Faso\n44.694000\n6.845792\n7.548677e+06\n3.247808e+06\n843.990665\n183.430087\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nSwitzerland\n75.565083\n4.011572\n6.384293e+06\n8.582009e+05\n27074.334405\n6886.463308\n\n\nTurkey\n59.696417\n9.030591\n4.590901e+07\n1.667768e+07\n4469.453380\n2049.665102\n\n\nUnited Kingdom\n73.922583\n3.378943\n5.608780e+07\n3.174339e+06\n19380.472986\n7388.189399\n\n\nAustralia\n74.662917\n4.147774\n1.464931e+07\n3.915203e+06\n19980.595634\n7815.405220\n\n\nNew Zealand\n73.989500\n3.559724\n3.100032e+06\n6.547108e+05\n17262.622813\n4409.009167\n\n\n\n\n142 rows × 6 columns\n\n\n\n\n# drop the first level of column index\ngdp_lifeExp_multilevel_data.droplevel(0, axis=1)\n\n\n\n\n\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n59.030167\n10.340069\n1.987541e+07\n8.613355e+06\n4426.025973\n1310.337656\n\n\nAngola\n37.883500\n4.005276\n7.309390e+06\n2.672281e+06\n3607.100529\n1165.900251\n\n\nBenin\n48.779917\n6.128681\n4.017497e+06\n2.105002e+06\n1155.395107\n159.741306\n\n\nBotswana\n54.597500\n5.929476\n9.711862e+05\n4.710965e+05\n5031.503557\n4178.136987\n\n\nBurkina Faso\n44.694000\n6.845792\n7.548677e+06\n3.247808e+06\n843.990665\n183.430087\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n75.565083\n4.011572\n6.384293e+06\n8.582009e+05\n27074.334405\n6886.463308\n\n\nTurkey\n59.696417\n9.030591\n4.590901e+07\n1.667768e+07\n4469.453380\n2049.665102\n\n\nUnited Kingdom\n73.922583\n3.378943\n5.608780e+07\n3.174339e+06\n19380.472986\n7388.189399\n\n\nOceania\nAustralia\n74.662917\n4.147774\n1.464931e+07\n3.915203e+06\n19980.595634\n7815.405220\n\n\nNew Zealand\n73.989500\n3.559724\n3.100032e+06\n6.547108e+05\n17262.622813\n4409.009167\n\n\n\n\n142 rows × 6 columns\n\n\n\n\n\n13.1.5.2 Stacking (Columns to Rows ): stack()\nThe stack() function pivots the columns into rows. You can specify which level you want to stack using the level parameter. By default, it will stack the innermost level of the columns.\n\ngdp_lifeExp_multilevel_data.stack(future_stack=True)\n\n\n\n\n\n\n\n\n\n\nlifeExp\npop\ngdpPercap\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\nmean\n59.030167\n1.987541e+07\n4426.025973\n\n\nstd\n10.340069\n8.613355e+06\n1310.337656\n\n\nAngola\nmean\n37.883500\n7.309390e+06\n3607.100529\n\n\nstd\n4.005276\n2.672281e+06\n1165.900251\n\n\nBenin\nmean\n48.779917\n4.017497e+06\n1155.395107\n\n\n...\n...\n...\n...\n...\n...\n\n\nEurope\nUnited Kingdom\nstd\n3.378943\n3.174339e+06\n7388.189399\n\n\nOceania\nAustralia\nmean\n74.662917\n1.464931e+07\n19980.595634\n\n\nstd\n4.147774\n3.915203e+06\n7815.405220\n\n\nNew Zealand\nmean\n73.989500\n3.100032e+06\n17262.622813\n\n\nstd\n3.559724\n6.547108e+05\n4409.009167\n\n\n\n\n284 rows × 3 columns\n\n\n\nLet’s change the default setting by specifying the level=0(the outmost level)\n\ngdp_lifeExp_multilevel_data.stack(level=0, future_stack=True)\n\n\n\n\n\n\n\n\n\n\nmean\nstd\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\nAfrica\nAlgeria\nlifeExp\n5.903017e+01\n1.034007e+01\n\n\npop\n1.987541e+07\n8.613355e+06\n\n\ngdpPercap\n4.426026e+03\n1.310338e+03\n\n\nAngola\nlifeExp\n3.788350e+01\n4.005276e+00\n\n\npop\n7.309390e+06\n2.672281e+06\n\n\n...\n...\n...\n...\n...\n\n\nOceania\nAustralia\npop\n1.464931e+07\n3.915203e+06\n\n\ngdpPercap\n1.998060e+04\n7.815405e+03\n\n\nNew Zealand\nlifeExp\n7.398950e+01\n3.559724e+00\n\n\npop\n3.100032e+06\n6.547108e+05\n\n\ngdpPercap\n1.726262e+04\n4.409009e+03\n\n\n\n\n426 rows × 2 columns\n\n\n\n\n\n13.1.5.3 Unstacking (Rows to Columns) : unstack()\nThe unstack() function pivots the rows back into columns. By default, it will unstack the innermost level of the index.\nLet’s reverse the prevous dataframe to its original shape using unstack().\n\ngdp_lifeExp_multilevel_data.stack(level=0, future_stack=True).unstack()\n\n\n\n\n\n\n\n\n\nmean\nstd\n\n\n\n\nlifeExp\npop\ngdpPercap\nlifeExp\npop\ngdpPercap\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n59.030167\n1.987541e+07\n4426.025973\n10.340069\n8.613355e+06\n1310.337656\n\n\nAngola\n37.883500\n7.309390e+06\n3607.100529\n4.005276\n2.672281e+06\n1165.900251\n\n\nBenin\n48.779917\n4.017497e+06\n1155.395107\n6.128681\n2.105002e+06\n159.741306\n\n\nBotswana\n54.597500\n9.711862e+05\n5031.503557\n5.929476\n4.710965e+05\n4178.136987\n\n\nBurkina Faso\n44.694000\n7.548677e+06\n843.990665\n6.845792\n3.247808e+06\n183.430087\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n75.565083\n6.384293e+06\n27074.334405\n4.011572\n8.582009e+05\n6886.463308\n\n\nTurkey\n59.696417\n4.590901e+07\n4469.453380\n9.030591\n1.667768e+07\n2049.665102\n\n\nUnited Kingdom\n73.922583\n5.608780e+07\n19380.472986\n3.378943\n3.174339e+06\n7388.189399\n\n\nOceania\nAustralia\n74.662917\n1.464931e+07\n19980.595634\n4.147774\n3.915203e+06\n7815.405220\n\n\nNew Zealand\n73.989500\n3.100032e+06\n17262.622813\n3.559724\n6.547108e+05\n4409.009167\n\n\n\n\n142 rows × 6 columns\n\n\n\n\n\n\n13.1.6 Transposing using .T attribute\nIf you simply want to swap rows and columns, you can use the .T attribute.\n\ngdp_lifeExp_multilevel_data.T\n\n\n\n\n\n\n\n\ncontinent\nAfrica\n...\nEurope\nOceania\n\n\n\ncountry\nAlgeria\nAngola\nBenin\nBotswana\nBurkina Faso\nBurundi\nCameroon\nCentral African Republic\nChad\nComoros\n...\nSerbia\nSlovak Republic\nSlovenia\nSpain\nSweden\nSwitzerland\nTurkey\nUnited Kingdom\nAustralia\nNew Zealand\n\n\n\n\nlifeExp\nmean\n5.903017e+01\n3.788350e+01\n4.877992e+01\n54.597500\n4.469400e+01\n4.481733e+01\n4.812850e+01\n4.386692e+01\n4.677358e+01\n52.381750\n...\n6.855100e+01\n7.069608e+01\n7.160075e+01\n7.420342e+01\n7.617700e+01\n7.556508e+01\n5.969642e+01\n7.392258e+01\n7.466292e+01\n7.398950e+01\n\n\nstd\n1.034007e+01\n4.005276e+00\n6.128681e+00\n5.929476\n6.845792e+00\n3.174882e+00\n5.467960e+00\n4.720690e+00\n4.887978e+00\n8.132353\n...\n4.906736e+00\n2.715787e+00\n3.677979e+00\n5.155859e+00\n3.003990e+00\n4.011572e+00\n9.030591e+00\n3.378943e+00\n4.147774e+00\n3.559724e+00\n\n\npop\nmean\n1.987541e+07\n7.309390e+06\n4.017497e+06\n971186.166667\n7.548677e+06\n4.651608e+06\n9.816648e+06\n2.560963e+06\n5.329256e+06\n361683.916667\n...\n8.783887e+06\n4.774507e+06\n1.794381e+06\n3.585180e+07\n8.220029e+06\n6.384293e+06\n4.590901e+07\n5.608780e+07\n1.464931e+07\n3.100032e+06\n\n\nstd\n8.613355e+06\n2.672281e+06\n2.105002e+06\n471096.527435\n3.247808e+06\n1.874538e+06\n4.363640e+06\n1.072158e+06\n2.464304e+06\n182998.737990\n...\n1.192153e+06\n6.425096e+05\n2.022077e+05\n4.323928e+06\n6.365660e+05\n8.582009e+05\n1.667768e+07\n3.174339e+06\n3.915203e+06\n6.547108e+05\n\n\ngdpPercap\nmean\n4.426026e+03\n3.607101e+03\n1.155395e+03\n5031.503557\n8.439907e+02\n4.716630e+02\n1.774634e+03\n9.587847e+02\n1.165454e+03\n1314.380339\n...\n9.305049e+03\n1.041553e+04\n1.407458e+04\n1.402983e+04\n1.994313e+04\n2.707433e+04\n4.469453e+03\n1.938047e+04\n1.998060e+04\n1.726262e+04\n\n\nstd\n1.310338e+03\n1.165900e+03\n1.597413e+02\n4178.136987\n1.834301e+02\n9.932972e+01\n4.195899e+02\n1.919529e+02\n2.305481e+02\n298.334389\n...\n3.829907e+03\n3.650231e+03\n6.470288e+03\n8.046635e+03\n7.723248e+03\n6.886463e+03\n2.049665e+03\n7.388189e+03\n7.815405e+03\n4.409009e+03\n\n\n\n\n6 rows × 142 columns\n\n\n\n\n\n13.1.7 Summary of Common Reshaping Functions\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\npivot()\nReshape data by creating new columns from existing ones.\nPivot data on index/columns.\n\n\nmelt()\nUnpivot data, turning columns into rows.\nConvert wide format to long.\n\n\nstack()\nConvert columns to rows.\nMove column data to rows.\n\n\nunstack()\nConvert rows back to columns.\nMove row data to columns.\n\n\n.T\nTranspose the DataFrame (swap rows and columns).\nTranspose the entire DataFrame.\n\n\n\nPractical Use Cases:\n\nPivoting: Useful for time series analysis or when you want to group and summarize data by specific categories.\nMelting: Helpful when you need to reshape wide data for easier analysis or when preparing data for machine learning algorithms.\nStacking/Unstacking: Useful when you are working with hierarchical index DataFrames.\nTranspose: Helpful for reorienting data for analysis or visualization.\n\n\n\n13.1.8 Converting from Multi-Level Index to Single-Level Index DataFrame\nIf you’re uncomfortable working with DataFrames that have multi-level indexes and columns, you can convert them into single-level DataFrames using the methods you learned in the previous chapter:\n\nreset_index()\nFlattening the multi-level columns\n\n\n# reset the index to convert the multi-level index to a single-level index\ngdp_lifeExp_multilevel_data.reset_index()\n\n\n\n\n\n\n\n\ncontinent\ncountry\nlifeExp\npop\ngdpPercap\n\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\n\n\n\n\n0\nAfrica\nAlgeria\n59.030167\n10.340069\n1.987541e+07\n8.613355e+06\n4426.025973\n1310.337656\n\n\n1\nAfrica\nAngola\n37.883500\n4.005276\n7.309390e+06\n2.672281e+06\n3607.100529\n1165.900251\n\n\n2\nAfrica\nBenin\n48.779917\n6.128681\n4.017497e+06\n2.105002e+06\n1155.395107\n159.741306\n\n\n3\nAfrica\nBotswana\n54.597500\n5.929476\n9.711862e+05\n4.710965e+05\n5031.503557\n4178.136987\n\n\n4\nAfrica\nBurkina Faso\n44.694000\n6.845792\n7.548677e+06\n3.247808e+06\n843.990665\n183.430087\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n137\nEurope\nSwitzerland\n75.565083\n4.011572\n6.384293e+06\n8.582009e+05\n27074.334405\n6886.463308\n\n\n138\nEurope\nTurkey\n59.696417\n9.030591\n4.590901e+07\n1.667768e+07\n4469.453380\n2049.665102\n\n\n139\nEurope\nUnited Kingdom\n73.922583\n3.378943\n5.608780e+07\n3.174339e+06\n19380.472986\n7388.189399\n\n\n140\nOceania\nAustralia\n74.662917\n4.147774\n1.464931e+07\n3.915203e+06\n19980.595634\n7815.405220\n\n\n141\nOceania\nNew Zealand\n73.989500\n3.559724\n3.100032e+06\n6.547108e+05\n17262.622813\n4409.009167\n\n\n\n\n142 rows × 8 columns\n\n\n\n\n# flatten the multi-level column\ngdp_lifeExp_multilevel_data.columns = ['_'.join(col).strip() for col in gdp_lifeExp_multilevel_data.columns.values]\ngdp_lifeExp_multilevel_data\n\n\n\n\n\n\n\n\n\nlifeExp_mean\nlifeExp_std\npop_mean\npop_std\ngdpPercap_mean\ngdpPercap_std\n\n\ncontinent\ncountry\n\n\n\n\n\n\n\n\n\n\nAfrica\nAlgeria\n59.030167\n10.340069\n1.987541e+07\n8.613355e+06\n4426.025973\n1310.337656\n\n\nAngola\n37.883500\n4.005276\n7.309390e+06\n2.672281e+06\n3607.100529\n1165.900251\n\n\nBenin\n48.779917\n6.128681\n4.017497e+06\n2.105002e+06\n1155.395107\n159.741306\n\n\nBotswana\n54.597500\n5.929476\n9.711862e+05\n4.710965e+05\n5031.503557\n4178.136987\n\n\nBurkina Faso\n44.694000\n6.845792\n7.548677e+06\n3.247808e+06\n843.990665\n183.430087\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nEurope\nSwitzerland\n75.565083\n4.011572\n6.384293e+06\n8.582009e+05\n27074.334405\n6886.463308\n\n\nTurkey\n59.696417\n9.030591\n4.590901e+07\n1.667768e+07\n4469.453380\n2049.665102\n\n\nUnited Kingdom\n73.922583\n3.378943\n5.608780e+07\n3.174339e+06\n19380.472986\n7388.189399\n\n\nOceania\nAustralia\n74.662917\n4.147774\n1.464931e+07\n3.915203e+06\n19980.595634\n7815.405220\n\n\nNew Zealand\n73.989500\n3.559724\n3.100032e+06\n6.547108e+05\n17262.622813\n4409.009167\n\n\n\n\n142 rows × 6 columns",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Reshaping and Enrichment</span>"
    ]
  },
  {
    "objectID": "Data wrangling.html#data-enriching",
    "href": "Data wrangling.html#data-enriching",
    "title": "13  Data Reshaping and Enrichment",
    "section": "13.2 Data Enriching",
    "text": "13.2 Data Enriching\nData enriching involves enhancing your existing dataset by adding additional data, often from external sources, to make your dataset more insightful.\n\nThere are 3 common methods for data enriching:\n\nMerging using merge(): Combining datasets based on a common key.\nJoining using join(): Using indices to combine data.\nConcatenation using concat(): Stacking DataFrames vertically or horizontally.\n\nLet’s first load the existing data\n\ndf_existing = pd.read_csv('./datasets/LOTR.csv')\nprint(df_existing.shape)\ndf_existing.head()\n\n(4, 3)\n\n\n\n\n\n\n\n\n\nFellowshipID\nFirstName\nSkills\n\n\n\n\n0\n1001\nFrodo\nHiding\n\n\n1\n1002\nSamwise\nGardening\n\n\n2\n1003\nGandalf\nSpells\n\n\n3\n1004\nPippin\nFireworks\n\n\n\n\n\n\n\nLet’s next load the external data we want to combine to the existing data\n\ndf_external = pd.read_csv(\"./datasets/LOTR 2.csv\")\nprint(df_external.shape)\ndf_external.head()\n\n(5, 3)\n\n\n\n\n\n\n\n\n\nFellowshipID\nFirstName\nAge\n\n\n\n\n0\n1001\nFrodo\n50\n\n\n1\n1002\nSamwise\n39\n\n\n2\n1006\nLegolas\n2931\n\n\n3\n1007\nElrond\n6520\n\n\n4\n1008\nBarromir\n51\n\n\n\n\n\n\n\n\n13.2.1 Combining based on common keys: merge()\n\ndf_enriched_merge = pd.merge(df_existing, df_external)\nprint(df_enriched_merge.shape)\ndf_enriched_merge.head()\n\n(2, 4)\n\n\n\n\n\n\n\n\n\nFellowshipID\nFirstName\nSkills\nAge\n\n\n\n\n0\n1001\nFrodo\nHiding\n50\n\n\n1\n1002\nSamwise\nGardening\n39\n\n\n\n\n\n\n\nYou can also do this way\n\ndf_enriched_merge = df_existing.merge(df_external)\nprint(df_enriched_merge.shape)\ndf_enriched_merge.head()\n\n(2, 4)\n\n\n\n\n\n\n\n\n\nFellowshipID\nFirstName\nSkills\nAge\n\n\n\n\n0\n1001\nFrodo\nHiding\n50\n\n\n1\n1002\nSamwise\nGardening\n39\n\n\n\n\n\n\n\nBy default, the merge() function in pandas performs an inner join, which combines two DataFrames based on the common keys shared by both. Only the rows with matching keys in both DataFrames are retained in the result.\nThis behavior can be observed in the following code:\n\ndf_enriched_merge = pd.merge(df_existing, df_external, how='inner')\nprint(df_enriched_merge.shape)\ndf_enriched_merge.head()\n\n(2, 4)\n\n\n\n\n\n\n\n\n\nFellowshipID\nFirstName\nSkills\nAge\n\n\n\n\n0\n1001\nFrodo\nHiding\n50\n\n\n1\n1002\nSamwise\nGardening\n39\n\n\n\n\n\n\n\nWhen you may use inner join? You should use inner join when you cannot carry out the analysis unless the observation corresponding to the key column(s) is present in both the tables.\nExample: Suppose you wish to analyze the association between vaccinations and covid infection rate based on country-level data. In one of the datasets, you have the infection rate for each country, while in the other one you have the number of vaccinations in each country. The countries which have either the vaccination or the infection rate missing, cannot help analyze the association. In such as case you may be interested only in countries that have values for both the variables. Thus, you will use inner join to discard the countries with either value missing.\nIn merge(), the how parameter specifies the type of merge to be performed, the default option is inner, it could also be left, right, and outer.\n\nThese join types give you flexibility in how you want to merge and combine data from different sources.\nLet’s see how each of these works using our dataframes defined above\nLeft Join\n\nReturns all rows from the left DataFrame and the matching rows from the right DataFrame.\nMissing values (NaN) are introduced for non-matching keys from the right DataFrame.\n\n\ndf_merge_lft = pd.merge(df_existing, df_external, how='left')\nprint(df_merge_lft.shape)\ndf_merge_lft.head()\n\n(4, 4)\n\n\n\n\n\n\n\n\n\nFellowshipID\nFirstName\nSkills\nAge\n\n\n\n\n0\n1001\nFrodo\nHiding\n50.0\n\n\n1\n1002\nSamwise\nGardening\n39.0\n\n\n2\n1003\nGandalf\nSpells\nNaN\n\n\n3\n1004\nPippin\nFireworks\nNaN\n\n\n\n\n\n\n\nWhen you may use left join? You should use left join when the primary variable(s) of interest are present in the one of the datasets, and whose missing values cannot be imputed. The variable(s) in the other dataset may not be as important or it may be possible to reasonably impute their values, if missing corresponding to the observation in the primary dataset.\nExamples:\n\nSuppose you wish to analyze the association between the covid infection rate and the government effectiveness score (a metric used to determine the effectiveness of the government in implementing policies, upholding law and order etc.) based on the data of all countries. Let us say that one of the datasets contains the covid infection rate, while the other one contains the government effectiveness score for each country. If the infection rate for a country is missing, it might be hard to impute. However, the government effectiveness score may be easier to impute based on GDP per capita, crime rate etc. - information that is easily available online. In such a case, you may wish to use a left join where you keep all the countries for which the infection rate is known.\nSuppose you wish to analyze the association between demographics such as age, income etc. and the amount of credit card spend. Let us say one of the datasets contains the demographic information of each customer, while the other one contains the credit card spend for the customers who made at least one purchase. In such as case, you may want to do a left join as customers not making any purchase might be absent in the card spend data. Their spend can be imputed as zero after merging the datasets.\n\nRight join\n\nReturns all rows from the right DataFrame and the matching rows from the left DataFrame.\nMissing values (NaN) are introduced for non-matching keys from the left DataFrame.\n\n\ndf_merge_right = pd.merge(df_existing, df_external, how='right')\nprint(df_merge_right.shape)\ndf_merge_right.head()\n\n(5, 4)\n\n\n\n\n\n\n\n\n\nFellowshipID\nFirstName\nSkills\nAge\n\n\n\n\n0\n1001\nFrodo\nHiding\n50\n\n\n1\n1002\nSamwise\nGardening\n39\n\n\n2\n1006\nLegolas\nNaN\n2931\n\n\n3\n1007\nElrond\nNaN\n6520\n\n\n4\n1008\nBarromir\nNaN\n51\n\n\n\n\n\n\n\nWhen you may use right join? You can always use a left join instead of a right join. Their purpose is the same.\n\n13.2.1.1 outer join\n\nReturns all rows from both DataFrames, with NaN values where no match is found.\n\n\ndf_merge_outer = pd.merge(df_existing, df_external, how='outer')\nprint(df_merge_outer.shape)\ndf_merge_outer\n\n(7, 4)\n\n\n\n\n\n\n\n\n\nFellowshipID\nFirstName\nSkills\nAge\n\n\n\n\n0\n1001\nFrodo\nHiding\n50.0\n\n\n1\n1002\nSamwise\nGardening\n39.0\n\n\n2\n1003\nGandalf\nSpells\nNaN\n\n\n3\n1004\nPippin\nFireworks\nNaN\n\n\n4\n1006\nLegolas\nNaN\n2931.0\n\n\n5\n1007\nElrond\nNaN\n6520.0\n\n\n6\n1008\nBarromir\nNaN\n51.0\n\n\n\n\n\n\n\nWhen you may use outer join? You should use an outer join when you cannot afford to lose data present in either of the tables. All the other joins may result in loss of data.\nExample: Suppose I took two course surveys for this course. If I need to analyze student sentiment during the course, I will take an outer join of both the surveys. Assume that each survey is a dataset, where each row corresponds to a unique student. Even if a student has answered one of the two surverys, it will be indicative of the sentiment, and will be useful to keep in the merged dataset.\n\n\n\n13.2.2 Combining based on Indices: join()\nUnlike merge(), join() in pandas combines data based on the index. When using join(), you must specify the suffixes parameter if there are overlapping column names, as it does not have default values. You can refer to the official function definition here: pandas.DataFrame.join.\nPlease copy the code below and run it in your notebook:\ndf_enriched_join = df_existing.join(df_external)\nprint(df_enriched_join.shape)\ndf_enriched_join.head()\nLet’s set the suffix to fix this issue\n\ndf_enriched_join = df_existing.join(df_external, lsuffix = '_existing',rsuffix = '_external')\nprint(df_enriched_join.shape)\ndf_enriched_join.head()\n\n(4, 6)\n\n\n\n\n\n\n\n\n\nFellowshipID_existing\nFirstName_existing\nSkills\nFellowshipID_external\nFirstName_external\nAge\n\n\n\n\n0\n1001\nFrodo\nHiding\n1001\nFrodo\n50\n\n\n1\n1002\nSamwise\nGardening\n1002\nSamwise\n39\n\n\n2\n1003\nGandalf\nSpells\n1006\nLegolas\n2931\n\n\n3\n1004\nPippin\nFireworks\n1007\nElrond\n6520\n\n\n\n\n\n\n\nAs can be observed, join() merges DataFrames using their index and performs a left join.\nNow, let’s set the “FellowshipID” column as the index in both DataFrames and see what happens.\n\n# set the index of the dataframes as the \"FellowshipID\" column\ndf_existing.set_index('FellowshipID', inplace=True)\ndf_external.set_index('FellowshipID', inplace=True)\n\n\n# join the two dataframes\ndf_enriched_join = df_existing.join(df_external, lsuffix = '_existing',rsuffix = '_external')\nprint(df_enriched_join.shape)\ndf_enriched_join\n\n(4, 4)\n\n\n\n\n\n\n\n\n\nFirstName_existing\nSkills\nFirstName_external\nAge\n\n\nFellowshipID\n\n\n\n\n\n\n\n\n1001\nFrodo\nHiding\nFrodo\n50.0\n\n\n1002\nSamwise\nGardening\nSamwise\n39.0\n\n\n1003\nGandalf\nSpells\nNaN\nNaN\n\n\n1004\nPippin\nFireworks\nNaN\nNaN\n\n\n\n\n\n\n\nIn this case, df_enriched_join will contain all rows from df_existing and the corresponding matching rows from df_external, with NaN values for non-matching entries.\nSimilar to merge(), you can change the type of join in join() by explicitly specifying the how parameter. However, this explanation will be skipped here, as the different join types have already been covered in the merge() section above.\n\n\n13.2.3 Stacking vertically or horizontally: concat()\nconcat() is used in pandas to concatenate DataFrames along a specified axis, either rows or columns. Similar to concat() in NumPy, it defaults to concatenating along the rows.\n\n# let's concatenate the two dataframes using default setting\ndf_concat = pd.concat([df_existing, df_external])\nprint(df_concat.shape)\ndf_concat\n\n(9, 3)\n\n\n\n\n\n\n\n\n\nFirstName\nSkills\nAge\n\n\nFellowshipID\n\n\n\n\n\n\n\n1001\nFrodo\nHiding\nNaN\n\n\n1002\nSamwise\nGardening\nNaN\n\n\n1003\nGandalf\nSpells\nNaN\n\n\n1004\nPippin\nFireworks\nNaN\n\n\n1001\nFrodo\nNaN\n50.0\n\n\n1002\nSamwise\nNaN\n39.0\n\n\n1006\nLegolas\nNaN\n2931.0\n\n\n1007\nElrond\nNaN\n6520.0\n\n\n1008\nBarromir\nNaN\n51.0\n\n\n\n\n\n\n\n\n# Concatenating along columns (horizontal)\ndf_concat_columns = pd.concat([df_existing, df_external], axis=1)\nprint(df_concat_columns.shape)\ndf_concat_columns\n\n(7, 4)\n\n\n\n\n\n\n\n\n\nFirstName\nSkills\nFirstName\nAge\n\n\nFellowshipID\n\n\n\n\n\n\n\n\n1001\nFrodo\nHiding\nFrodo\n50.0\n\n\n1002\nSamwise\nGardening\nSamwise\n39.0\n\n\n1003\nGandalf\nSpells\nNaN\nNaN\n\n\n1004\nPippin\nFireworks\nNaN\nNaN\n\n\n1006\nNaN\nNaN\nLegolas\n2931.0\n\n\n1007\nNaN\nNaN\nElrond\n6520.0\n\n\n1008\nNaN\nNaN\nBarromir\n51.0\n\n\n\n\n\n\n\n\n\n13.2.4 Missing values after data enriching\nUsing either of the approaches mentioned above may introduce missing values for unmatched entries. Handling these missing values is crucial for maintaining the quality and integrity of your dataset after merging or joining DataFrames. It is important to choose a method that best fits your data and analysis goals, and to document your decisions regarding missing value handling for transparency in your analysis.\nQuestion: Read the documentations of the Pandas DataFrame methods merge() and concat(), and identify the differences. Mention examples when you can use (i) either, (ii) only concat(), (iii) only merge()\nSolution:\n\nIf we need to merge datasets using row indices, we can use either function.\nIf we need to stack datasets one on top of the other, we can only use concat()\nIf we need to merge datasets using overlapping columns we can only use merge()\n\nFor a comprehensive user guide, please refer to the official documentation.",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Reshaping and Enrichment</span>"
    ]
  },
  {
    "objectID": "Data wrangling.html#independent-study",
    "href": "Data wrangling.html#independent-study",
    "title": "13  Data Reshaping and Enrichment",
    "section": "13.3 Independent Study",
    "text": "13.3 Independent Study\n\n13.3.1 Practice exercise 1\nYou are given the life expectancy data of each continent as a separate *.csv file.\n\n13.3.1.1 \nVisualize the change of life expectancy over time for different continents.\n\n\n13.3.1.2 \nAppending all the data files, i.e., stacking them on top of each other to form a combined datasets called “data_all_continents”\n\n\n\n13.3.2 Practice exercise 2\n\n13.3.2.1 Preparing GDP per capita data\nRead the GDP per capita data from https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita\nDrop all the Year columns. Use the drop() method with the columns, level and inplace arguments. Print the first 2 rows of the updated DataFrame. If the first row of the DataFrame has missing values for all columns, drop that row as well.\n\n\n13.3.2.2 \nDrop the inner level of column labels. Use the droplevel() method. Print the first 2 rows of the updated DataFrame.\n\n\n13.3.2.3 \nConvert the columns consisting of GDP per capita by IMF, World Bank, and the United Nations to numeric. Apply a lambda function on these columns to convert them to numeric. Print the number of missing values in each column of the updated DataFrame.\nNote: Do not apply the function 3 times. Apply it once on a DataFrame consisting of these 3 columns.\n\n\n13.3.2.4 \nApply the lambda function below on all the column names of the dataset obtained in the previous question to clean the column names.\nimport re\ncolumn_name_cleaner = lambda x:re.split(r'\\[|/', x)[0]\nNote: You will need to edit the parameter of the function, i.e., x in the above function to make sure it is applied on column names and not columns.\nPrint the first 2 rows of the updated DataFrame.\n\n\n13.3.2.5 \nCreate a new column GDP_per_capita that copies the GDP per capita values of the United Nations. If the GDP per capita is missing in the United Nations column, then copy it from the World Bank column. If the GDP per capita is missing both in the United Nations and the World Bank columns, then copy it from the IMF column.\nPrint the number of missing values in the GDP_per_capita column.\n\n\n13.3.2.6 \nDrop all the columns except Country and GDP_per_capita. Print the first 2 rows of the updated DataFrame.\n\n\n13.3.2.7 \nThe country names contain some special characters (characters other than letters) and need to be cleaned. The following function can help clean country names:\nimport re\ncountry_names_clean_gdp_data = lambda x: re.sub(r'[^\\w\\s]', '', x).strip()\nApply the above lambda function on the country column to clean country names. Save the cleaned dataset as gdp_per_capita_data. Print the first 2 rows of the updated DataFrame.\n\n\n13.3.2.8 Preparing population data\nRead the population data from https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations).\n\nDrop all columns except Country, UN Continental Region[1], and Population (1 July 2023).\nDrop the first row since it is the total population in the world\n\n\n\n13.3.2.9 \nApply the lambda function below on all the column names of the dataset obtained in the previous question to clean the column names.\nimport re\ncolumn_name_cleaner = lambda x:re.split(r'\\[|/|\\(| ', x.name)[0]\nNote: You will need to edit the parameter of the function, i.e., x in the above function to make sure it is applied on column names and not columns.\nPrint the first 2 rows of the updated DataFrame.\n\n\n13.3.2.10 \nThe country names contain some special characters (characters other than letters) and need to be cleaned. The following function can help clean country names:\nimport re\ncountry_names_clean_population_data = lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip()\nApply the above lambda function on the country column to clean country names. Save the cleaned dataset as population_data.\n\n\n13.3.2.11 Merging GDP per capita and population datasets\nMerge gdp_per_capita_data with population_data to get the population and GDP per capita of countries in a single dataset. Print the first two rows of the merged DataFrame.\nAssume that:\n\nWe want to keep the GDP per capita of all countries in the merged dataset, even if their population in unavailable in the population dataset. For countries whose population in unavailable, their Population column will show NA.\nWe want to discard an observation of a country if its GDP per capita is unavailable.\n\n\n\n13.3.2.12 \nFor how many countries in gdp_per_capita_data does the population seem to be unavailable in population_data? Note that you don’t need to clean country names any further than cleaned by the functions provided.\nPrint the observations of gdp_per_capita_data with missing Population.\n\n\n\n13.3.3 Merging datasets with similar values in the key column\nWe suspect that population of more countries may be available in population_data. However, due to unclean country names, the observations could not merge. For example, the country Guinea Bissau is mentioned as GuineaBissau in gdp_per_capita_data and Guinea-Bissau in population_data. To resolve this issue, we’ll use a different approach to merge datasts. We’ll merge the population of a country to an observation in the GDP per capita dataset, whose name in population_data is the most ‘similar’ to the name of the country in gdp_per_capita_data.\n\n13.3.3.1 \nProceed as follows:\n\nFor each country in gdp_per_capita_data, find the country with the most ‘similar’ name in population_data, based on the similarity score. Use the lambda function provided below to compute the similarity score between two strings (The higher the score, the more similar are the strings. The similarity score is \\(1.0\\) if two strings are exactly the same).\nMerge the population of the most ‘similar’ country to the country in gdp_per_capita_data. The merged dataset must include 5 columns - the country name as it appears in gdp_per_capita_data, the GDP per capita, the country name of the most ‘similar’ country as it appears in population_data, the population of that country, and the similarity score between the country names.\nAfter creating the merged dataset, print the rows of the dataset that have similarity scores less than 1.\n\nUse the function below to compute the similarity score between the Country names of the two datasets:\nfrom difflib import SequenceMatcher\nsimilar = lambda a,b: SequenceMatcher(None, a, b).ratio()\nNote: You may use one for loop only for this particular question. However, it would be perfect if don’t use a for loop\nHint:\n\nDefine a function that computes the index of the observation having the most ‘similar’ country name in population_data for an observation in gdp_per_capita_data. The function returns a Series consisting of the most ‘similar’ country name, its population, and its similarity score (This function can be written with only one line in its body, excluding the return statement and the definition statement. However, you may use as many lines as you wish).\nApply the function on the Country column of gdp_per_capita_data. A DataFrame will be obtained.\nConcatenate the DataFrame obtained in (2) with gdp_per_capita_data with the pandas concat() function.\n\n\n\n13.3.3.2 \nIn the dataset obtained in the previous question, for all observations where similarity score is less than 0.8, replace the population with Nan.\nPrint the observations of the dataset having missing values of population.",
    "crumbs": [
      "Data Wrangling: Essential skills",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Reshaping and Enrichment</span>"
    ]
  },
  {
    "objectID": "Assignment A.html",
    "href": "Assignment A.html",
    "title": "Appendix A — Assignment A",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment A</span>"
    ]
  },
  {
    "objectID": "Assignment A.html#instructions",
    "href": "Assignment A.html#instructions",
    "title": "Appendix A — Assignment A",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 14th October 2024 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment A</span>"
    ]
  },
  {
    "objectID": "Assignment A.html#list-comprehension",
    "href": "Assignment A.html#list-comprehension",
    "title": "Appendix A — Assignment A",
    "section": "A.1 List comprehension",
    "text": "A.1 List comprehension\nUSA’s GDP per capita from 1960 to 2021 is given by the tuple T in the code cell below. The values are arranged in ascending order of the year, i.e., the first value is for 1960, the second value is for 1961, and so on.\n\nT = (3007, 3067, 3244, 3375,3574, 3828, 4146, 4336, 4696, 5032,5234,5609,6094,6726,7226,7801,8592,9453,10565,11674,12575,13976,14434,15544,17121,18237,19071,20039,21417,22857,23889,24342,25419,26387,27695,28691,29968,31459,32854,34515,36330,37134,37998,39490,41725,44123,46302,48050,48570,47195,48651,50066,51784,53291,55124,56763,57867,59915,62805,65095,63028,69288)\n\n\nA.1.1 \nUse list comprehension to create a list of the gaps between consecutive entries in T, i.e, the increase in GDP per capita with respect to the previous year. The list with gaps should look like: [60, 177, …]. Let the name of this list be GDP_increase.\n(4 points)\n\n\nA.1.2 \nUse GDP_increase to find the maximum gap size, i.e, the maximum increase in GDP per capita.\n(1 point)\n\n\nA.1.3 \nUse list comprehension with GDP_increase to find the percentage of gaps that have size greater than $1000.\n(3 points)\n\n\nA.1.4 \nUse list comprehension with GDP_increase to print the list of years in which the GDP per capita increase was more than $2000.\nHint: The enumerate() function may help.\n(4 points)\n\n\nA.1.5 \nUse list comprehension to:\n\nCreate a list that consists of the difference between the maximum and minimum GDP per capita values for each of the 5 year-periods starting from 1976, i.e., for the periods 1976-1980, 1981-1985, 1986-1990, …, 2016-2020.\nFind the five year period in which the difference (between the maximum and minimum GDP per capita values) was the least.\n\n(4 + 2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment A</span>"
    ]
  },
  {
    "objectID": "Assignment A.html#nested-list-comprehension",
    "href": "Assignment A.html#nested-list-comprehension",
    "title": "Appendix A — Assignment A",
    "section": "A.2 Nested list-comprehension",
    "text": "A.2 Nested list-comprehension\nBelow is the list consisting of the majors / minors of students of the course STAT303-1 Fall 2023. This data is a list of lists, where each sub-list (smaller list within the outer larger list) consists of the majors / minors of a student. Most of the students have majors / minors in one or more of these four areas:\n\nMath / Statistics / Computer Science\nHumanities / Communication\nSocial Sciences / Education\nPhysical Sciences / Natural Sciences / Engineering\n\nThere are some students having majors / minors in other areas as well.\nUse list comprehension for all the questions below.\n\nmajors_minors = majors_minors = [['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Humanities / Communications',  'Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Humanities / Communications',  'Social Sciences / Education',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Cognitive Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Music'], ['Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Data Science'], ['Social Sciences / Education'], ['Math / Statistics / Computer Science', 'jazz'], ['Humanities / Communications', 'Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Humanities / Communications',  'Social Sciences / Education',  'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Econ'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', ''], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Humanities / Communications',  'Social Sciences / Education',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Humanities / Communications', 'Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Humanities / Communications', 'Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education'], ['Humanities / Communications'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education',  'Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Social Sciences / Education'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications',  'Social Sciences / Education',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Humanities / Communications'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Social Sciences / Education',  'Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science', 'Music'], ['Physical Sciences / Natural Sciences / Engineering'], ['Humanities / Communications'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education', 'Math / Statistics / Computer Science'], ['Humanities / Communications', 'Math / Statistics / Computer Science'], ['Physical Sciences / Natural Sciences / Engineering'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science'], ['Social Sciences / Education'], ['Physical Sciences / Natural Sciences / Engineering'], ['Physical Sciences / Natural Sciences / Engineering',  'Math / Statistics / Computer Science'], ['Math / Statistics / Computer Science']]\n\n\nA.2.1 \nHow many students have major / minor in any three of the above mentioned four areas?\n(1 point)\n\n\nA.2.2 \nHow many students have Math / Statistics / Computer Science as an area of their major / minor?\nHint: Nested list comprehension\n(4 points)\n\n\nA.2.3 \nHow many students have Math / Statistics / Computer Science as the only area of their major / minor?\nHint: Nested list comprehension\n(5 points)\n\n\nA.2.4 \nHow many students have Math / Statistics / Computer Science and Social Sciences / Education as a couple of areas of their major / minor?\nHint: The in-built function all() may be useful.\n(6 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment A</span>"
    ]
  },
  {
    "objectID": "Assignment A.html#dictionary",
    "href": "Assignment A.html#dictionary",
    "title": "Appendix A — Assignment A",
    "section": "A.3 Dictionary",
    "text": "A.3 Dictionary\nThe code cell below defines an object having the nutrition information of drinks in starbucks.\n\nstarbucks_drinks_nutrition={'Cool Lime Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Strawberry Acai Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Very Berry Hibiscus Starbucks Refreshers™ Beverage': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Evolution Fresh™ Organic Ginger Limeade': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Coffee': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Iced Espresso Classics - Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Iced Espresso Classics - Caffe Mocha': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 23}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Espresso Classics - Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 21}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 5}, {'Nutrition_type': 'Sodium', 'value': 65}], 'Shaken Sweet Tea': [{'Nutrition_type': 'Calories', 'value': 80}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Berry Blossom White': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Black Mango': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Black with Lemon': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Brambleberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Giant Peach': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled Iced Passion': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Lemon Ginger': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Black Lemonade': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Organic Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 31}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Plum Pomegranate': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Tazo® Bottled Tazoberry': [{'Nutrition_type': 'Calories', 'value': 150}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 38}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Tazo® Bottled White Cranberry': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Teavana® Shaken Iced Black Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Black Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Green Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Green Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 17}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea': [{'Nutrition_type': 'Calories', 'value': 30}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 8}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 5}], 'Teavana® Shaken Iced Passion Tango™ Tea Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Teavana® Shaken Iced Peach Green Tea': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 15}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Raspberry Pomegranate': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks Refreshers™ Strawberry Lemonade': [{'Nutrition_type': 'Calories', 'value': 90}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 27}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Doubleshot Protein Dark Chocolate': [{'Nutrition_type': 'Calories', 'value': 210}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 33}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Starbucks® Doubleshot Protein Vanilla': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 120}], 'Starbucks® Iced Coffee Caramel': [{'Nutrition_type': 'Calories', 'value': 60}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Light Sweetened': [{'Nutrition_type': 'Calories', 'value': 50}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 11}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Starbucks® Iced Coffee Unsweetened': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 2}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Blonde Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Clover® Brewed Coffee': [{'Nutrition_type': 'Calories', 'value': 10}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Decaf Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Featured Dark Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nariño 70 Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 15}], 'Nariño 70 Cold Brew with Milk': [{'Nutrition_type': 'Calories', 'value': 0}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Nitro Cold Brew': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 0}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Nitro Cold Brew with Sweet Cream': [{'Nutrition_type': 'Calories', 'value': 70}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 20}], 'Pike Place® Roast': [{'Nutrition_type': 'Calories', 'value': 5}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 0}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 10}], 'Vanilla Sweet Cream Cold Brew': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 14}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 1}, {'Nutrition_type': 'Sodium', 'value': 25}], 'Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks® Signature Hot Chocolate': [{'Nutrition_type': 'Calories', 'value': 430}, {'Nutrition_type': 'Fat', 'value': 26.0}, {'Nutrition_type': 'Carb', 'value': 45}, {'Nutrition_type': 'Fiber', 'value': 5}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 290}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 42}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 13}, {'Nutrition_type': 'Sodium', 'value': 140}], 'Cappuccino': [{'Nutrition_type': 'Calories', 'value': 120}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 12}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 35}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 40}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 32}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Flat White': [{'Nutrition_type': 'Calories', 'value': 180}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 18}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Iced Caffè Latte': [{'Nutrition_type': 'Calories', 'value': 130}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 13}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 8}, {'Nutrition_type': 'Sodium', 'value': 115}], 'Iced Caffè Mocha': [{'Nutrition_type': 'Calories', 'value': 230}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 36}, {'Nutrition_type': 'Fiber', 'value': 4}, {'Nutrition_type': 'Protein', 'value': 9}, {'Nutrition_type': 'Sodium', 'value': 90}], 'Iced Caramel Macchiato': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 150}], 'Iced Cinnamon Dolce Latte': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 95}], 'Iced Coconutmilk Mocha Macchiato': [{'Nutrition_type': 'Calories', 'value': 260}, {'Nutrition_type': 'Fat', 'value': 9.0}, {'Nutrition_type': 'Carb', 'value': 34}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 11}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Iced Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 4.0}, {'Nutrition_type': 'Carb', 'value': 30}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 7}, {'Nutrition_type': 'Sodium', 'value': 100}], 'Iced White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 8.0}, {'Nutrition_type': 'Carb', 'value': 47}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 190}], 'Latte Macchiato': [{'Nutrition_type': 'Calories', 'value': 190}, {'Nutrition_type': 'Fat', 'value': 7.0}, {'Nutrition_type': 'Carb', 'value': 19}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 160}], 'Starbucks Doubleshot® on Ice Beverage': [{'Nutrition_type': 'Calories', 'value': 45}, {'Nutrition_type': 'Fat', 'value': 1.0}, {'Nutrition_type': 'Carb', 'value': 5}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 40}], 'Vanilla Latte': [{'Nutrition_type': 'Calories', 'value': 250}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 37}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 12}, {'Nutrition_type': 'Sodium', 'value': 150}], 'White Chocolate Mocha': [{'Nutrition_type': 'Calories', 'value': 360}, {'Nutrition_type': 'Fat', 'value': 11.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 14}, {'Nutrition_type': 'Sodium', 'value': 240}], 'Cinnamon Dolce Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 350}, {'Nutrition_type': 'Fat', 'value': 4.5}, {'Nutrition_type': 'Carb', 'value': 64}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 15}, {'Nutrition_type': 'Sodium', 'value': 0}], 'Coffee Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 110}, {'Nutrition_type': 'Fat', 'value': 0.0}, {'Nutrition_type': 'Carb', 'value': 24}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 3}, {'Nutrition_type': 'Sodium', 'value': 200}], 'Mocha Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 280}, {'Nutrition_type': 'Fat', 'value': 2.5}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 2}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 220}], 'Mocha Light Frappuccino® Blended Coffee': [{'Nutrition_type': 'Calories', 'value': 140}, {'Nutrition_type': 'Fat', 'value': 0.5}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 1}, {'Nutrition_type': 'Protein', 'value': 4}, {'Nutrition_type': 'Sodium', 'value': 180}], 'Cinnamon Dolce Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Vanilla Crème': [{'Nutrition_type': 'Calories', 'value': 200}, {'Nutrition_type': 'Fat', 'value': 6.0}, {'Nutrition_type': 'Carb', 'value': 28}, {'Nutrition_type': 'Fiber', 'value': 0}, {'Nutrition_type': 'Protein', 'value': 10}, {'Nutrition_type': 'Sodium', 'value': 135}], 'Chocolate Smoothie': [{'Nutrition_type': 'Calories', 'value': 320}, {'Nutrition_type': 'Fat', 'value': 5.0}, {'Nutrition_type': 'Carb', 'value': 53}, {'Nutrition_type': 'Fiber', 'value': 8}, {'Nutrition_type': 'Protein', 'value': 20}, {'Nutrition_type': 'Sodium', 'value': 170}], 'Strawberry Smoothie': [{'Nutrition_type': 'Calories', 'value': 300}, {'Nutrition_type': 'Fat', 'value': 2.0}, {'Nutrition_type': 'Carb', 'value': 60}, {'Nutrition_type': 'Fiber', 'value': 7}, {'Nutrition_type': 'Protein', 'value': 16}, {'Nutrition_type': 'Sodium', 'value': 130}]}\n\n\nA.3.1 \nWhat is the nested data structure of the object starbucks_drinks_nutrition? An example of a nested data structure can be a list of dictionaries, where the dictionary values are a tuple of dictionaries?\n(1 point)\n\n\nA.3.2 \nUse dictionary-comprehension to print the name and carb content of all drinks that have a carb content of more than 50 units.\nHint: It will be a nested dictionary comprehension.\n(6 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment A</span>"
    ]
  },
  {
    "objectID": "Assignment A.html#ted-talks",
    "href": "Assignment A.html#ted-talks",
    "title": "Appendix A — Assignment A",
    "section": "A.4 Ted talks",
    "text": "A.4 Ted talks\n\nA.4.1 \nRead the data on ted talks from 2006 to 2017.\n(1 point)\n\n\nA.4.2 \nFind the number of talks in the dataset.\n(1 point)\n\n\nA.4.3 \nFind the headline, speaker and year_filmed of the talk with the highest number of views.\n(4 points)\n\n\nA.4.4 \nDo the majority of talks have less views than the average number of views for a talk? Justify your answer.\n(3 points)\nHint: Print summary statistics for questions (4) and (5).\n\n\nA.4.5 \nDo at least 25% of the talks have more views than the average number of views for a talk? Justify your answer.\n(3 points)\n\n\nA.4.6 \nThe last column of the dataset consists of votes obtained by the talk under different categories, such as Funny, Confusing, Fascinating, etc. For each category, create a new column in the dataset that contains the votes obtained by the tedtalk in that category. Print the first 5 rows of the updated dataset.\n(8 points)\n\n\nA.4.7 \nWith the data created in (a), find the headline of the talk that received the highest number of votes as Confusing.\n(4 points)\n\n\nA.4.8 \nWith the data created in (a), find the headline and the year of the talk that received the highest percentage of votes in the Fascinating category.\n\\[\\text{Percentage of } \\textit{Fascinating} \\text{ votes for a ted talk} = \\frac{Number \\ of \\  votes \\ in \\ the \\ category \\ `Fascinating`}{Total \\ votes \\ in \\ all  \\ categories}\\]\n(7 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment A</span>"
    ]
  },
  {
    "objectID": "Assignment A.html#university-rankings",
    "href": "Assignment A.html#university-rankings",
    "title": "Appendix A — Assignment A",
    "section": "A.5 University rankings",
    "text": "A.5 University rankings\n\nA.5.1 \nDownload the data set “univ.txt”. Read it with python.\n(1 point)\n\n\nA.5.2 \nFind summary statistics of the data. Based on the statistics, answer the next four questions.\n(1 point)\n\n\nA.5.3 \nHow many universities are there in the data set?\n(1 point)\n\n\nA.5.4 \nEstimate the maximum Tuition and fees among universities that are in the bottom 25% when ranked by total tuition and fees.\n(2 points)\n\n\nA.5.5 \nHow many universities share the ranking of 220? (If s universities share the same rank, say r, then the next lower rank is r+s, and all the ranks in between r and r+s are dropped)\n(4 points)\n\n\nA.5.6 \nCan you find the mean Tuition and fees for an undergrad student in the US from the summary statistics? Justify your answer.\n(3 points)\n\n\nA.5.7 \nFind the average Tuition and fees for an undergrad student in the US.\n(5 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment A</span>"
    ]
  },
  {
    "objectID": "Assignment A.html#file-formats",
    "href": "Assignment A.html#file-formats",
    "title": "Appendix A — Assignment A",
    "section": "A.6 File formats",
    "text": "A.6 File formats\nConsider the file formats - csv, JSON, txt. Mention one advantage and one disadvantage of each format over the other two formats.\n(2+2+2 = 6 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment A</span>"
    ]
  },
  {
    "objectID": "Assignment B.html",
    "href": "Assignment B.html",
    "title": "Appendix B — Assignment B",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment B</span>"
    ]
  },
  {
    "objectID": "Assignment B.html#instructions",
    "href": "Assignment B.html#instructions",
    "title": "Appendix B — Assignment B",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 21th October 2024 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment B</span>"
    ]
  },
  {
    "objectID": "Assignment B.html#air-quality-sensors",
    "href": "Assignment B.html#air-quality-sensors",
    "title": "Appendix B — Assignment B",
    "section": "B.1 Air quality sensors",
    "text": "B.1 Air quality sensors\n(An application of broadcasting NumPy arrays)\nAir quality sensors are used to measure the amount of contaminants in air. This question will guide you in finding the location of installing 50 air quality sensors in the State of Colorado, such that they are as far away from each other as possible. The approach below is a greedy algorithm to find an approximate Maximin design.\nThe file colorado_coordinate_grid.txt contains the coordinate-pairs (latitude and longitude) of potential locations for installing an air quality sensor.\n\nB.1.1 Data\nRead the file with NumPy. How many coordinate-pairs are there in the file?\nNote that:\n\nA coordinate-pair means a latitude-longitude pair.\n‘Air quality sensor’ will be referred as ‘sensor’ in the questions below for brevity.\n\n(4 points)\n\n\nB.1.2 First sensor\nThe first sensor is to be installed closest to Denver (closest in terms of Euclidean distance). Find the coordinate-pair of the location where the first sensor will be installed. The coordinate-pair of Denver is: [39.7392\\(^{\\circ}\\) N, 104.9903\\(^{\\circ}\\) W]\nNote that the suffixes \\(^{\\circ}\\) N and \\(^{\\circ}\\) W are omitted in the file colorado_coordinate_grid.txt.\nHint: Broadcasting\n(4 points)\n\n\nB.1.3 Second sensor\nFind the coordinate-pair of the installation of the next sensor, such that it is as far as possible from the first sensor installed near Denver.\nHint: Broadcasting\n(4 points)\n\n\nB.1.4 First two sensors\nStack the coordinate-pairs of the first and second sensors vertically to obtain a 2 x 2 NumPy array. Name the array as air_sensor_coordinates.\nRun the code below to check if your results seem correct. The coordinate-pairs of the two air quality sensors will be marked as blue dots.\n(4 points)\n\n\nCode\nimport matplotlib.pyplot as plt\ndef sensor_viz():\n    img = plt.imread(\"colorado.jpg\")\n    fig, ax = plt.subplots(figsize=(10, 100),dpi=80)\n    fig.set_size_inches(10.5, 15)\n    ax.imshow(img,extent=[-109, -102, 37, 41])\n    plt.scatter(y = air_sensor_coordinates[:,0], x = -air_sensor_coordinates[:,1])\n    plt.xlim(-109.05,-101.95)\n    plt.ylim(36.95,41.05)\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\nsensor_viz()\n\n\n\n\nB.1.5 Third sensor\nNow you need to find the coordinate-pair for installing the third sensor such that it is far away from the two already-installed sensors. Proceed as follows:\n\nFind the minimum distance of each coordinate-pair in colorado_coordinate_grid.txt from the two already installed sensors. For example, if a coordinate-pair is at a distance of 5 units from the first sensor, and 10 units from the second sensor, then its minimum distance from the sensors will be \\(\\min(5,10) = 5\\) units.\nSelect the coordinate-pair (from colorado_coordinate_grid.txt) whose minimum distance from the two already installed sensors is the maximum.\nStack the coordinate-pair of the third air quality sensor vertically on the array air_sensor_coordinates.\n\nCall the function sensor_viz() to check if your results seem correct. The coordinate-pairs of the three air quality sensors will be marked as blue dots.\nHint:\nFor step (1) above:\n\nDefine a function which computes the distances of a coordinate-pair from all the coordinates of air_sensor_coordinates, and returns the minimum distance.\nApply the function on all the coordinate-pairs in colorado_coordinate_grid.txt using the NumPy function apply_along_axis().\n\n(20 points)\n\n\nB.1.6 All 50 sensors\nYou need to find 47 more coordinate-pairs to install air quality sensors well-spread across Colorado. We will generalize the steps in the previous question to proceed as follows:\n\nSuppose you have already found the coordinate-pairs for the installation of i sensors.\nFind the minimum distance of each coordinate in colorado_coordinate_grid.txt from the i already installed sensors. For example, if a coordinate-pair is at a distance of \\(d_1\\) from the first sensor, \\(d_2\\) from the second sensor,…, and \\(d_i\\) from the \\(i^{th}\\) sensor, then its minimum distance from the sensors will be \\(min(d_1, d_2, ..., d_i\\)).\nSelect the \\(i+1^{th}\\) coordinate-pair (from colorado_coordinate_grid.txt) as the one whose minimum distance from the \\(i\\) already installed sensors is the maximum.\n\nCall the function sensor_viz() to check if your results seem correct. You should see 50 blue dots well spread across Colorado.\n(10 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment B</span>"
    ]
  },
  {
    "objectID": "Assignment B.html#sales",
    "href": "Assignment B.html#sales",
    "title": "Appendix B — Assignment B",
    "section": "B.2 Sales",
    "text": "B.2 Sales\n(An application of matrix multiplication with NumPy arrays)\nWhen the monthly sales of a product are subject to seasonal fluctuations, a curve that approximates the sales formula might have the form:\n\\[y = a + b*x + c*\\sin\\bigg(2*\\pi*\\frac{x}{12}\\bigg),\\]\nwhere \\(x\\) is the time since the starting point in months and \\(y\\) is the monthly sales in USD (million). The term \\(a + b*x\\) gives the basic sales trend and the \\(\\sin\\) term reflects the seasonal changes in sales. Suppose the model parameters (i.e., \\(a\\), \\(b\\), and \\(c\\)) are estimated and put on the list below for the sales of a certain brand of sunscreen starting June 1, 2017.\n\n\nCode\nmodel_parameters = [2, 5, 18]\n\n\nThen, the total monthly sales in June 2017 will be calculated by plugging 1 as \\(x\\) into the equation.\nUsing matrix multiplication with NumPy, we wish to estimate the total sales between June 1 2017 and March 1, 2020. (So many models failed to predict sales after that - probably due to covid.)\nProceed as follows.\n\nB.2.1 Create first array\nCreate a numpy array where the first column is all \\(1\\)s, the second column is a range of numbers from 1 to the total number of months from June 1 2017 to March 1 2020 and the third column is \\(\\sin(2*\\pi*x/12)\\) values with \\(x\\) values as plugged-in in the second column.\n(10 points)\n\n\nB.2.2 Create second array\nCreate an array from the list model_parameters.\n(3 points)\n\n\nB.2.3 Multiply arrays\nUse matrix multiplication to get the monthly sales estimates for each month in the range: June 1 2017 and March 1, 2020.\n(8 points)\n\n\nB.2.4 Sum array elements\nFind the total sales between June 1 2017 and March 1, 2020.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment B</span>"
    ]
  },
  {
    "objectID": "Assignment B.html#exercise-minutes",
    "href": "Assignment B.html#exercise-minutes",
    "title": "Appendix B — Assignment B",
    "section": "B.3 Exercise minutes",
    "text": "B.3 Exercise minutes\n(An application of parallel computation with NumPy)\nThis problem demonstrates the benefit of generating pseudo random number matrix with NumPy.\nThe list exercise_minutes below consists of exercise minutes per week of the students of STAT303-1 Fall 2022 class.\nWe wish to find the 95% confidence interval of mean exercise_minutes, using Bootstrapping.\nBootstrapping is a non-parametric method for obtaining confidence interval. The method is as follows.\n\nSuppose the list exercise_minutes has \\(N\\) values.\nRandomly sample \\(N\\) values with replacement from exercise_minutes\nFind the mean of the \\(N\\) values obtained in (b)\nRepeat steps (b) and (c) 10,000 times\nThe 95% Confidence interval is the range between the 2.5% and 97.5% percentile values of the 10,000 means obtained in (c)\n\n\n\nCode\nexercise_minutes=[240, 180, 60, 300, 0, 360, 60, 140, 60, 0, 150, 60, 0, 6, 60, 300, 90, 100, 250, 240, 300, 630, 420, 50, 0, 60, 240, 300, 180, 420, 90, 8, 180, 15, 8, 150, 180, 240, 60, 1200, 210, 360, 720, 240, 360, 240, 250, 180, 600, 120, 60, 200, 360, 120, 20, 250, 60, 420, 420, 150, 350, 180, 14, 60, 450, 180, 300, 1, 180, 7, 180, 300, 70, 40, 300, 60, 180, 225, 90, 300, 240, 200, 60, 200, 360, 3, 200, 300, 90, 60, 180, 120, 10, 0, 200, 700, 300, 300, 5, 60, 420, 300, 240, 200, 180, 180, 120, 300, 375, 60, 240, 180, 180, 90, 240, 180, 15, 300, 60, 120, 120, 240, 400, 200, 60, 480, 120, 300, 180, 250, 280, 7, 600, 240, 0, 420, 60, 2, 280, 300, 60, 0, 250, 180, 540, 30, 210, 2, 90, 120, 180, 240, 540, 400, 120, 150, 360, 180, 200, 180, 30, 60, 300, 80, 60, 210, 315, 360, 275, 200, 150, 180, 200, 150, 0, 1200, 240, 120, 300, 360, 180, 240, 630, 250, 240, 5, 30, 0, 300, 60, 90]\n\n\nAnswer the following questions.\n\nB.3.1 Sequential computation without NumPy\nWithout using NumPy, compute the:\n\nConfidence interval of mean exercise_minutes, and\nTime taken to execute the code\n\nHints:\n\nYou may use the library random.\nYou may use the library time for computing the time taken to execute the code.\n\n(12 points)\n\n\nB.3.2 Parallel computation with NumPy\nUsing NumPy, and without using loops, compute the:\n\nConfidence interval of mean exercise_minutes, and\nTime taken to execute the code\n\n(12 points)\n\n\nB.3.3 Time saving with NumPy\nReport the ratio of time taken to execute the code wihout NumPy to the time taken to execute the code with NumPy.\n(1 point)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment B</span>"
    ]
  },
  {
    "objectID": "Assignment C.html",
    "href": "Assignment C.html",
    "title": "Appendix C — Assignment C",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment C</span>"
    ]
  },
  {
    "objectID": "Assignment C.html#instructions",
    "href": "Assignment C.html#instructions",
    "title": "Appendix C — Assignment C",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 28th October 2024 at 11:59 pm. There is an optional bonus question worth 10 points. You can score a maximum of 110 (out of 100) points.\nYou are not allowed to use a for loop or any other kind of loop in this assignment.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment C</span>"
    ]
  },
  {
    "objectID": "Assignment C.html#gdp-per-capita-social-indicators",
    "href": "Assignment C.html#gdp-per-capita-social-indicators",
    "title": "Appendix C — Assignment C",
    "section": "C.1 GDP per capita & social indicators",
    "text": "C.1 GDP per capita & social indicators\nRead the file social_indicator.txt with python. Set the first column as the index when reading the file. How many observations and variables are there in the data?\n(4 points)\n\nC.1.1 \nWhich variables have the strongest and weakest correlations with GDP per capita? Note that lifeFemale and lifeMale are the female and male life expectancies respectively.\nNote that only when the magnitude of the correlation is considered when judging a correlation as strong or weak.\n(4 points)\n\n\nC.1.2 \nDoes the male economic activity (in the column economicActivityMale) have a positive or negative correlation with GDP per capita? Did you expect the positive/negative correlation? If not, why do you think you are observing that correlation?\n(4 points)\n\n\nC.1.3 \nWhat is the rank of the US amongst all countries in terms of GDP per capita? Which countries lie immediately above, and immediately below the US in the ranking in terms of GDP per capita? The country having the highest GDP per capita ranks 1.\nNote that:\n\nThe US is mentioned as United States in the data.\nThe country with the highest GDP per capita will have rank 1, the country with the second highest GDP per capita will have rank 2, and so on.\n\nHint: rank()\n(4 points)\n\n\nC.1.4 \nWhich country or countries rank among the top 20 in terms of each of these social indicators - economicActivityFemale, economicActivityMale, gdpPerCapita, lifeFemale, lifeMale? For each of these social indicators, the country having the largest value ranks 1 for that indicator.\n(6 points)\nHint:\n\nUse rank(). Note that this method is different from the method given in the hint of the previous question. This method is of the DataFrame class, while the one in the previous question is of the Series class. This part of the hint is just for your understanding. You don’t need to write any code in this part.\nUsing rank(), get the DataFrame consisting of the ranks of countries on each of the relevant social indicators (one line of code).\nIn the DataFrame obtained in (2), filter the rows for which the maximum rank is less than or equal to 20 (one line of code).\n\n\n\nC.1.5 \nOn which social indicator among economicActivityFemale, economicActivityMale, gdpPerCapita, lifeFemale, lifeMale, illiteracyFemale, illiteracyMale, infantMortality, and totalfertilityrate does the US have its worst ranking, and what is the rank? Note that for illiteracyFemale, illiteracyMale, and infantMortality, the country having the lowest value will rank 1, in contrast to the other social indicators.\n(8 points)\n\n\nC.1.6 \nFind all the countries that have a lower GDP per capita than the US, despite having lower illiteracy rates (for both genders), higher economic activity (for both genders), higher life expectancy (for both genders), and lower infant mortality rate than the US?\n(6 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment C</span>"
    ]
  },
  {
    "objectID": "Assignment C.html#gdp-per-capita-vs-social-indicators",
    "href": "Assignment C.html#gdp-per-capita-vs-social-indicators",
    "title": "Appendix C — Assignment C",
    "section": "C.2 GDP per capita vs social indicators",
    "text": "C.2 GDP per capita vs social indicators\nWe’ll use the same data as in in the previous question. For the questions below, assume that all numeric columns, except GDP per capita, are social indicators.\n\nC.2.1 \nUse the column geographic_location to create a new column called continent. Merge the values of the geographic_location column appropriately to obtain 6 distinct values for the continent column – Asia, Africa, North America, South America, Europe and Oceania. Drop the column geographic_location. Print the first 5 observations of the updated DataFrame.\n(8 points)\nHint:\n\nUse value_counts() to see the values of geographic_location. The code if 'Asia' in 'something' will return True if ‘something’ contains the string ‘Asia’, for example, if ‘something’ is ‘North Asia’, the code with return True. This part of the hint is just for your understanding. You don’t need to write any code in this part.\nApply a lambda function on the Series geographic_location to replace a string that contains ‘Asia’ with ‘Asia’, replace a string that contains ‘Europe’ with ‘Europe’, and replace a string that contains ‘Africa’ with ‘Africa’. This will be a single line of code.\nRename the column georgaphic_location to continent.\n\n\n\nC.2.2 \nSort the column labels lexicographically. Drop the columns region and contraception. Print the first 5 observations of the updated DataFrame.\nHint: sort_index()\n(4 points)\n\n\nC.2.3 \nFind the percentage of the total countries in each continent.\nHint: One line of code with value_counts() and shape\n(4 points)\n\n\nC.2.4 \nWhich country has the highest GDP per capita? Let us call it country \\(G\\).\n(4 points)\n\n\nC.2.5 \nWe need to find the African country that is the closest to country \\(G\\) with regard to social indicators. Perform the following steps:\n\nC.2.5.1 \nStandardize each of the social indicators to a standard normal distribution so that all of them are on the same scale (remember to exclude GDP per capita from social indicators).\nHint:\n\nFor scaling a random variable to standard normal, subtract the mean from each value of the variable, and divide by its standard deviation.\nUse the apply method with a lambda function to scale all the social indicators to standard normal.\nThe above (1) and (2) together is a single line of code.\n\n(6 points)\n\n\nC.2.5.2 \nCompute the Manhattan distance between country \\(G\\) and each of the African countries, based on the scaled social indicators.\nHint:\n\nBroadcast a Series to a DataFrame\nThe Manhattan distance between two points \\((x_1, x_2, ..., x_p)\\) and \\((y_1, y_2, ..., y_p)\\) is \\(|x_1 - y_1| + |x_2 - y_2| + ... + |x_p-y_p|\\), where \\(|.|\\) stands for absolute value (for example, \\(|-2| = 2;  |3| = 3\\)).\n\n(8 points)\n\n\nC.2.5.3 \nIdentify the African country, say country \\(A\\), with the least Manhattan distance to country \\(G\\).\n(8 points)\n\n\n\nC.2.6 \nFind the correlation between the Manhattan distance from country \\(G\\) and GDP per capita for African countries.\n(6 points)\n\n\nC.2.7 \nBased on the correlation coefficient in \\(2(f)\\), do you think African countries should try to emulate the social characteristics of country \\(G\\)? Justify your answer.\n(4 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment C</span>"
    ]
  },
  {
    "objectID": "Assignment C.html#medical-data",
    "href": "Assignment C.html#medical-data",
    "title": "Appendix C — Assignment C",
    "section": "C.3 Medical data",
    "text": "C.3 Medical data\nRead the data sets conditions.csv and patients.csv. Suppose we are interested in studying patients with prediabetes condition. Do not drop or compute any missing values. In condition.csv, the patient IDs are stored in column PATIENT, and the medical conditions are stored in column DESCRIPTION. In patient.csv, the patient IDs are stored in column Id.\n\nC.3.1 \nPrint the patient IDs of all the patients with prediabetes condition.\n(4 points)\n\n\nC.3.2 \nMake a subset of the data with only prediabetes patients. How many prediabetes patients are there?\n(4 points)\nHint: .isin()\n\n\nC.3.3 \nWhat proportion of the total HEALTHCARE_EXPENSES of all the patients correspond to the HEALTHCARE_EXPENSES of prediabetes patients.\n(4 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment C</span>"
    ]
  },
  {
    "objectID": "Assignment C.html#bonus-question",
    "href": "Assignment C.html#bonus-question",
    "title": "Appendix C — Assignment C",
    "section": "C.4 Bonus question",
    "text": "C.4 Bonus question\nThis is an optional question with no partial credit. You will get points only if your solution is completely correct. We advise you to attempt it only when you are done with the rest of the assignment.\n(10 points)\nRead the file STAT303-1 survey for data analysis.csv. In this question, we’ll work to clean this data a bit. As with every question, you are not allowed to use a for loop or any other loop.\nExecute the following code to read the data and clean the column names.\n\n\nCode\nsurvey_data = pd.read_csv('STAT303-1 survey for data analysis.csv')\nnew_col_names = ['parties_per_month', 'smoke', 'weed', 'introvert_extrovert', 'love_first_sight', 'learning_style', 'left_right_brained', 'personality_type', 'social_media', 'num_insta_followers', 'streaming_platforms', 'expected_marriage_age', 'expected_starting_salary', 'fav_sport', 'minutes_ex_per_week', 'sleep_hours_per_day', 'how_happy', 'farthest_distance_travelled', 'fav_number', 'fav_letter', 'internet_hours_per_day', 'only_child', 'birthdate_odd_even', 'birth_month', 'fav_season', 'living_location_on_campus', 'major', 'num_majors_minors', 'high_school_GPA', 'NU_GPA', 'age', 'height', 'height_father', 'height_mother', 'school_year', 'procrastinator', 'num_clubs', 'student_athlete', 'AP_stats', 'used_python_before', 'dominant_hand', 'childhood_in_US', 'gender', 'region_of_residence', 'political_affliation', 'cant_change_math_ability', 'can_change_math_ability', 'math_is_genetic', 'much_effort_is_lack_of_talent']\nsurvey_data.columns = list(survey_data.columns[0:2])+new_col_names\n\n\nCheck the datatype of the variables using the dtypes attribute of the Pandas DataFrame object. You will notice that only two variables are numeric. However, if you check the first few observations of the data with the function head() you will find the there are several more variables that seem to have numeric values.\nWrite a function that accepts a Pandas Series (or a column of a Pandas DataFrame object) as argument, and if the datatype of the Series is non-numeric, does the following:\n\nChecks if at least 10 values of the Series contain a digit in them.\nIf at least 10 values are found to contain a digit, then:\nA. Eliminate the characters ~, +, and , from all the values of the Series.\nB. Convert the Series to numeric (with coercion if needed).\nIf at least 10 values are NOT found to contain a digit, then:\nA. If the values of the Series are ‘Yes’ and ‘No’, then replace ‘Yes’ with 1 and ‘No’ with 0. The Series datatype must change to numeric as well.\nB. If the values of the Series are ‘Agree’ and ‘Disagree’, then replace ‘Agree’ with 1 and ‘Disagree’ with 0. The Series datatype must change to numeric as well.\n\nApply the function to each column of survey_data using the apply() method. Save the updated DataFrame as survey_data_clean. Then, execute the following code.\n\n\nCode\nsurvey_data_clean.describe().loc['mean',:]\n\n\nThe above code should print out the mean values of 28 numeric columns in the data survey_data_clean.\nThe variables that you should see as numeric in survey_data_clean are given in the list numeric_columns below (this is just to check your work):\n\n\nCode\nnumeric_columns = ['parties_per_month', 'love_first_sight', 'num_insta_followers',\n       'expected_marriage_age', 'expected_starting_salary',\n       'minutes_ex_per_week', 'sleep_hours_per_day',\n       'farthest_distance_travelled', 'fav_number', 'internet_hours_per_day',\n       'only_child', 'num_majors_minors', 'high_school_GPA', 'NU_GPA', 'age',\n       'height', 'height_father', 'height_mother', 'procrastinator',\n       'num_clubs', 'student_athlete', 'AP_stats', 'used_python_before',\n       'childhood_in_US', 'cant_change_math_ability',\n       'can_change_math_ability', 'math_is_genetic',\n       'much_effort_is_lack_of_talent']\n\n\nNote that your function must be general, i.e., it must work for any other dataset as well. This means, you cannot hard code a column name, or anything specific to survey_data in the function.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment C</span>"
    ]
  },
  {
    "objectID": "Assignment D.html",
    "href": "Assignment D.html",
    "title": "Appendix D — Assignment D",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment D</span>"
    ]
  },
  {
    "objectID": "Assignment D.html#instructions",
    "href": "Assignment D.html#instructions",
    "title": "Appendix D — Assignment D",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 4th November 2024 at 11:59 pm.\nSome questions in this assignment do not have a single correct answer. As data visualization is subject to interpretation, any logically sound answer / explanation is acceptable.\nThere is a bonus question worth 20 points. However, there is no partial credit for the bonus question. You will get 20 or 0. If everything is correct, you can score 120 out of 100 in the assignment.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment D</span>"
    ]
  },
  {
    "objectID": "Assignment D.html#time-trend",
    "href": "Assignment D.html#time-trend",
    "title": "Appendix D — Assignment D",
    "section": "D.1 Time trend",
    "text": "D.1 Time trend\nLet us analyze if the profitability of a movie is associated with the time of its release.\n\nD.1.1 Month of release\n\nD.1.1.1 \nMake an appropriate plot to visualize the mean profit of movies released each month.\nHint:\n\nUse the Pandas function to_datetime() to convert Release Date to a datetime datatype.\nUse the library datetime to extract the month from Release Date.\n\n(6 points)\n\n\nD.1.1.2 \nBased on the plot, which seasons have been the most and least profitable (on an average) for a movie release. Don’t worry about the exact start and end date of seasons. Don’t perform any computations. Just make comments based on the plot. You can use seasons such as early summer, late spring etc.\n(2 points)\n\n\n\nD.1.2 Month of release with number of movies in each genre\n\nD.1.2.1 \nNow that we know the most profitable season for releasing movies, let us visualize if some genres are more popular during certain seasons.\nUse the code below to create a new column called genre.\n\n\nCode\n#Combining Major Genre\nmovies_data['genre'] = movies_data['Major Genre'].apply(lambda x:'Comedy' if x!=None and 'Comedy' in x else 'Horror' if x!=None and 'Thriller' in x else 'Action/Adventure' if x!=None and ('Action' in x or 'Adventure' in x) else 'Musical/Western' if x!=None and ('Musical' in x or 'Western' in x or 'Concert' in x) else x)\n\n\nMake an appropriate plot to visualize the number of movies released for each genre in each calendar month.\n(8 points)\nHint:\n\nUse barplot() with estimator as len\nUse the hue argument\n\n\n\nD.1.2.2 \nBased on the above plot, which genre is the most popular during the most profitable season of release? And which genre is the most popular during the least profitable season of release?\n(2 points)\n\n\n\nD.1.3 Month of release with proportion of movies in each genre\n\nD.1.3.1 \nVisualize the proportion of movies in each genre for each month of release.\nUse the code below to re-arrange your data that will help with creating the visualization\n\n\nCode\ngenre_proportion_release_month = pd.crosstab(index=movies_data['release_month'],\n                             columns=movies_data['genre'],\n                             normalize=\"index\")\ngenre_proportion_release_month.head()\n\n\nHint:\n\nMake a 100% stacked barplot with the Pandas plot() function\nUse the argument bbox_to_anchor with the Matplotlib function legend() to place the legend outside the plot area.\n\n(8 points)\n\n\nD.1.3.2 \nWhich genre is the most popular during the month of May, and which one is the most popular during December?\n(2 points)\n\n\n\nD.1.4 Year of release with genre\n\nD.1.4.1 \nMake an appropriate figure to visualize the average profit of movies of each genre for each year. Consider only the movies released from 1991 to 2010. Also show the 95% confidence interval in the average profit.\nHint:\n\nUse the library datetime to extract year from Release Date.\nUse the Seaborn Facetgrid() object.\nA figure can have multiple subplots. Put the figure for each genre in a separate subplot.\n\n(6 points)\n\n\nD.1.4.2 \nBased on the figure above, which genre’s profitability seems to be increasing over the years, and which genre has the least uncertainty in profit for most of the years.\n(2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment D</span>"
    ]
  },
  {
    "objectID": "Assignment D.html#associations",
    "href": "Assignment D.html#associations",
    "title": "Appendix D — Assignment D",
    "section": "D.2 Associations",
    "text": "D.2 Associations\n\nD.2.1 Pairplot / heatmap\n\nD.2.1.1 \nMake a pairplot and heatmap of all the continuous variables in the data.\n(8 points)\n\n\nD.2.1.2 \nAre there any trends that you can see in the pairplot, but not in the heatmap?\n(2 points)\n\n\nD.2.1.3 \nBased on the plots in 2(a)(i), which variables are associated with profit?\n(2 points)\n\n\nD.2.1.4 \nAmong the variables listed in 2(a)(iii), select a subset of variables such that none of them are highly associated with each other. The rest of the variables identified in 2(a)(iii) are redundant with regard to association with profit.\n(2 points)\n\n\n\nD.2.2 Nested associations\n\nD.2.2.1 \nUse the code below to create some new columns.\n\n\nCode\nmovies_data['screenplay'] = movies_data.Source.apply(lambda x:'Non-original' if x!='Original Screenplay' else x)\nmovies_data['rating'] = movies_data['MPAA Rating'].apply(lambda x:'R rated' if x=='R' else 'Not R rated')\nmovies_data['fiction'] = movies_data['Creative Type'].apply(lambda x:'Contemporary' if x=='Contemporary Fiction' else 'other')\n\n\nMake an appropriate figure to visualize the association of the number of IMDB votes with profit for each genre (use the variable genre). Which genre has the highest association between profit and IMDB votes?\n(8 points)\n\n\nD.2.2.2 \nMake an appropriate figure to visualize the association between the number of IMDB votes and profit, for each combination of the fiction type (use the variable fiction) and the movie rating (use the variable rating).\nFor which combination of fiction and rating categories do you observe the highest association between IMDB votes and profit?\n(8 points)\nHint: Use row and col attributes of the Seaborn Facetgrid() object.\n\n\n\nD.2.3 Profit based on movie director\n\nD.2.3.1 \nConsider the directors who have directed more than 10 movies (based on the dataset). Make a horizontal barplot that shows the mean profit of the movies of these directors along with the 95% confidence interval. Sort the bars of the barplot such that the director with the highest mean profit is at the top.\nIf the dataset director_with_more_than_10_movies has only those movies that correspond to directors with more than 10 movies, then the following code will give you the order in which the names of the directors must appear in the barplot:\n(8 points)\n\n\nCode\ndirector_with_more_than_10_movies[['Director','profit']].groupby('Director').mean().sort_values(by = 'profit',\n                                            ascending= False).index.to_list()\n\n\n\n\nD.2.3.2 \nBased on the above plot, which director has the highest mean profitability, and which one has the highest variation in profitability?\n(2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment D</span>"
    ]
  },
  {
    "objectID": "Assignment D.html#distributions",
    "href": "Assignment D.html#distributions",
    "title": "Appendix D — Assignment D",
    "section": "D.3 Distributions",
    "text": "D.3 Distributions\n\nD.3.1 Distribution of profit based on genre (boxplots)\n\nD.3.1.1 \nMake boxplots to visualize the distribution of profit based on genre. Based on the plot, which genre has the most profitable movies?\n(6 points)\n\n\nD.3.1.2 \nWhich genre has the most variation in profit, and which one has the least?\n(2 points)\n\n\n\nD.3.2 Distribution of profit based on genre (density plots)\n\nD.3.2.1 \nMake density plots of profit based on genre. Adjust the limit on the horizonal axis, so that the plots are clearly visible.\n(6 points)\n\n\nD.3.2.2 \nWhat additional insight / trend can you seen in the above plot that you cannot see with the boxplots?\n(2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment D</span>"
    ]
  },
  {
    "objectID": "Assignment D.html#insights",
    "href": "Assignment D.html#insights",
    "title": "Appendix D — Assignment D",
    "section": "D.4 Insights",
    "text": "D.4 Insights\nFrom all the visualizations above, describe the insights you get about the factors associated with the profitability of a movie.\nAlso, elaborate on the extent to which these trends can be generalized. For example, comment on whether these trends be generalized to the current time and all the Hollywood movies? If not, is there any time period or type of movie to which these trends can be applicable?\n(4+ 4 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment D</span>"
    ]
  },
  {
    "objectID": "Assignment D.html#return-on-investment",
    "href": "Assignment D.html#return-on-investment",
    "title": "Appendix D — Assignment D",
    "section": "D.5 Return on investment",
    "text": "D.5 Return on investment\n\nD.5.1 Impatient investor (daily)\nSuppose there is an investor who only holds the index for a single day (buy yesterday sell today).\nBased on the data,\n\nD.5.1.1 \nShow the histogram graph for all the possible returns.\n\n\nD.5.1.2 \nWhat is the expected return, risk and sharpe ratio?\n\n\nD.5.1.3 \nIs the return significantly greater than zero (a.k.a positive return) (use a threshold 0.01 for \\(p\\)-value) ?\nHINT: use scipy.stats.ttest_1samp to do one-sided mean test. (We ignore the fact that T-test requires the data are sampled from a population of normal distribution, which might not be true in this exercise)\n(6 points)\n\n\n\nD.5.2 Patient Investor (yearly)\nSuppose there is an investor who will hold the index for a year (suppose there are 250 trading days in a year). Do the same analysis as the above:\n\nD.5.2.1 \nShow the histogram graph for all the possible returns.\n\n\nD.5.2.2 \nWhat is the expected return, risk and sharpe ratio?\n\n\nD.5.2.3 \nIs the return significantly greater than zero (a.k.a positive return) (use a threshold 0.01 for \\(p\\)-value) ?\n(6 points)\n\n\n\nD.5.3 From daily to yearly\nExplore how the expected return/risk/shape ratio change as we increase our holding period from 1 day to 1 year(250 days).\nShow/answer:\n\nD.5.3.1 \nAt least how many days do you need to hold the index in order to make a significant positive return (threshold 0.01)?\n\n\nD.5.3.2 \nHow are the returns associated with the risks for different investment strategies?\n\n\nD.5.3.3 \nMake a graph as shown below.\n(18 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment D</span>"
    ]
  },
  {
    "objectID": "Assignment E.html",
    "href": "Assignment E.html",
    "title": "Appendix E — Assignment E",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Assignment E</span>"
    ]
  },
  {
    "objectID": "Assignment E.html#instructions",
    "href": "Assignment E.html#instructions",
    "title": "Appendix E — Assignment E",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 11th November 2024 at 11:59 pm.\nThere is a bonus question worth 20 points, and an ultra bonus question worth 30 points. However, there is no partial credit for these questions. You will get 20 or 0 for the bonus question, and 30 or 0 for the ultra-bonus question. If everything is correct, you can score 150 out of 100 in the assignment.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Assignment E</span>"
    ]
  },
  {
    "objectID": "Assignment E.html#missing-value-imputation",
    "href": "Assignment E.html#missing-value-imputation",
    "title": "Appendix E — Assignment E",
    "section": "E.1 Missing value imputation",
    "text": "E.1 Missing value imputation\nRead Housing_missing_price.csv as housing_missing_price and Housing_complete.csv as housing_complete. The datasets consist of housing features, like number of bedrooms, bathrooms, etc., and the price. Both datasets are exactly the same, except that Housing_missing_price.csv has some missing values of price. In this question, you will try different methods to impute the missing values of house price in the file Housing_missing_price.csv. You will use the prices in Housing_complete.csv to check the accuracy of your imputation.\nNote that you cannot use Housing_complete.csv to impute missing price in any of the questions. Housing_complete.csv is just to check the accuracy of your imputation, after you have done the imputation. Before imputing the missing price, assume you do not have Housing_complete.csv.\n\nE.1.1 Number of missing values\nHow many values of price are missing in Housing_missing_price.csv?\n(3 points)\n\n\nE.1.2 Indices of missing values\nFind the row labels, where the price is missing. Assign those row labels as an array or a list to index_null_price.\n(4 points)\n\n\nE.1.3 Correlation of continuous variables with price\nFind the continuous variable having the highest correlation with price. Let the variable be \\(A_{cont}\\).\n(2 points)\n\n\nE.1.4 Missing value imputation using correlated continuous variable\nMake a scatterplot of price against \\(A_{cont}\\), with a trendline. Using the trendline, impute the missing values of price.\nPlot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\n(10 points)\nHint:\n\nMake the trendline using non-missing values of price and \\(A_{cont}\\).\nImpute the missing values of price only where they are missing, i.e., at row indices index_null_price.\nYou may use the function below to plot the imputed price vs true price (from Housing_complete.csv) and print the RMSE. The function assumes that housing_imputed_price is the Housing_missing_price.csv dataset, with imputed values of price.\n\n\n\nCode\n#Defining a function to plot the imputed values vs actual values \ndef plot_actual_vs_predicted():\n    fig, ax = plt.subplots(figsize=(9, 6))\n    plt.rc('xtick', labelsize=15) \n    plt.rc('ytick', labelsize=15) \n    x = housing_complete.loc[index_null_price,'price']/1e3\n    y = housing_imputed_price.loc[index_null_price,'price']/1e3\n    plt.scatter(x,y)\n    z=np.polyfit(x,y,1)\n    p=np.poly1d(z)\n    plt.plot(x,x,color='orange')\n    plt.xlabel('Actual price',fontsize=20)\n    plt.ylabel('Imputed price',fontsize=20)\n    ax.xaxis.set_major_formatter('${x:,.0f}k')\n    ax.yaxis.set_major_formatter('${x:,.0f}k')\n    rmse = np.sqrt(((x-y).pow(2)).mean())\n    print(\"RMSE= $\"+str(np.round(rmse,2))+\"k\")\n\n\n\n\nE.1.5 Correlation of categorical variables with price\nSplit the categorical columns of the Housing_missing_price.csv, such that they transform into dummy variables with each category corresponding to a column of 0s and 1s. The continuous variables remain as they were in the original dataset. Name this dataset as housing_dummy.\nWhich categorical variable is the most highly correlated with price? Let that variable be \\(V_{cat}\\).\n(3 + 2 points)\nHint: pd.get_dummies()\n\n\nE.1.6 Missing value imputation using correlated categorical variable\nImpute the missing value of the price of a house as the average price of all the houses that have the same value of \\(V_{cat}\\). For example, if \\(V_{cat}\\) is basement, the missing price of a house that has a basement must be imputed as the average price of all the houses that have a basement, and the missing price of a house that lacks a basement must be imputed as the average price of all the houses that lack a basement.\nPlot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\n(10 points)\nHint: You may use the following code to get the mean house price for each level of the variable \\(V_{cat}\\):\n\n\nCode\n#Replace 'Vcat' by the variable that you found to be the most correlated with 'price'\nprice_Vcat = housing_missing_price['price'].groupby(housing_missing_price[Vcat]).mean()\nprice_Vcat\n\n\n\n\nE.1.7 Missing value imputation using correlated continuous variable within the categories of correlated categorical variable\nExecute the following code. Note that the trendlines of price against area are different based on airconditioning.\nFor each house, select the appropriate trendline to impute the missing price.\nPlot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\n(15 points)\n\n\nCode\nsns.set(font_scale=1.25)\na = sns.FacetGrid(housing_missing_price, hue = 'airconditioning',height=6, aspect=1.5)\na.map(sns.regplot,'area','price')\na.add_legend();\n\n\n\n\n\n\n\n\n\nE.1.8 Bonus question: Missing value imputation with KNN\n(20 points)\n\nE.1.8.1 Identifying optimal \\(K\\) by \\(k\\)-fold cross validation\nYou need to impute the missing price using the KNN (\\(K\\)-nearest neighbor) algorithm. However, before implementing the algorithm, find the optimal value of \\(K\\), using \\(k\\)-fold cross-validation. Use all the variables in housing_dummy.\nNote that you cannot use Housing_complete.csv to find the optimal \\(K\\).\nFollow the \\(k\\)-fold cross validation procedure below to find the optimal \\(K\\), i.e., the optimal number of nearest neighbors to consider when imputing missing values:\n\nRemove observations with missing price from housing_dummy. Let us call the resulting DataFrame as housing_missing_removed.\nSplit housing_missing_removed into \\(k\\)-folds. Take \\(k=10\\). Each fold will have one-tenth of the observations of housing_missing_removed.\nIterate over the \\(K^{th}\\) potential value of \\(K\\), where \\(K \\in \\{1,2,...,50\\}\\).\nA. Iterate over the \\(i^{th}\\) fold, where \\(i \\in \\{1,2,...,10\\}\\)\nI. Assume that the all the price values of the \\(i^{th}\\) fold are missing. Impute the value of price for an observation in the \\(i^{th}\\) fold as the mean price of the \\(K\\)-nearest neighbors to the observation, where the neighbors are from among the observations in the remaining 9 folds.\n\nCompute the RMSE (Root mean squared error) for the \\(i^{th}\\) fold. Let us denote the RMSE for \\(i^{th}\\) fold, and considering \\(K\\) nearest neighbors as \\(RMSE_{iK}\\).\n\nB. Find the average of the 10 RMSEs obtained in 3(A). Let us denote it as \\(RMSE_K\\), i.e., RMSE for a given value of \\(K\\). Then, \\[RMSE_K = \\frac{1}{10} \\sum_{i=1}^{i=10} RMSE_{iK}\\]\nThe optimal value of \\(K\\) is the one for which \\(RMSE_K\\) is the minimum, i.e., \\[K_{optimal}=\\underset{K \\in \\{1,...,50\\}}{\\operatorname{\\ argmin}} RMSE_K\\]\n\nAssumption to make it a bit simpler: Ideally you should split the dataset randomly into \\(k\\)-folds. However, to make it simpler, you may assume that the data is already shuffled, and you may take the first \\(1/10^{th}\\) observations to be in the \\(1^{st}\\) fold, the next \\(1/10^{th}\\) to be in the \\(2^{nd}\\) fold and so on.\nMore explanation about \\(k\\)-fold cross validation and the optimal \\(K\\):\nYou need to impute the missing price using the KNN (K-nearest neighbor) algorithm. However, before implementing the algorithm, find the optimal value of \\(K\\), using \\(k\\)-fold cross-validation. This is an optimization method used for more advanced Machine Learning methods, such as KNN. In KNN, the number of neighbors, \\(K\\), is called a hyperparameter, which cannot be optimized with a mathematical method. Therefore, it needs a more coding-based optimization method called cross-validation.\nThe idea of cross-validation is to split the dataset into subsets, called folds. After that a range of \\(K\\) values is picked. For each \\(K\\) value in the range, the KNN imputer is created and evaluated on each fold separately, returning an RMSE value for each fold. The average value of these RMSE values is the cross-validation RMSE value of that \\(K\\) value. Cross-validation is a robust method to find the best \\(K\\) in the KNN algorithm for the data at hand because it evaluates different parts of the data separately and takes the average of all results. It is called \\(k\\)-fold cross-validation, with \\(k\\) as the number of folds we want to use, usually 3, 5 or 10. In this problem, we will use 10-fold cross-validation. Note that you need nested for loops to iterate over both each \\(K\\) value and each fold for this algorithm.\n\n\nE.1.8.2 Implementing KNN with optimal \\(K\\)\nUsing the optimal value of \\(K = K_{optimal}\\) obtained in the previous question, impute the missing values of price in housing_missing_price. Use all the variables in housing_dummy for implementing the KNN algorithm. Plot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\nAnswer check: The RMSE obtained with KNN should be lower than that obtained with any of the earlier methods. If not, then there may be some mistake in your KNN implementation.\n\n\n\nE.1.9 Ultra-bonus question\n(30 points)\nDevelop an algorithm to impute the missing price, such that it reduces the imputation RMSE to below $650k. Plot the imputed price vs true price (from Housing_complete.csv) and print the RMSE (Root mean squared error). This is to be done only for the imputed values of price.\nNote that we have not attempted to solve this question yet. We are not sure if a solution exists. However, if you find a solution, you will get 30 points.\nHint: In the bonus question, all variables were given equal weights when imputing missing values with KNN. However, shouldn’t some variables be given higher weight than others?\nIf you think you are successful, email your solution to your instructor for grading.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Assignment E</span>"
    ]
  },
  {
    "objectID": "Assignment E.html#binning",
    "href": "Assignment E.html#binning",
    "title": "Appendix E — Assignment E",
    "section": "E.2 Binning",
    "text": "E.2 Binning\nRead house_features_and_price.csv. We will bin a couple of variables to better analyze the trend of house_price with those variables.\n\nE.2.1 Trend with house_age\nMake a scatterplot of house_price against house_age, along with the trendline. What is the trend indicated by the trendline?\n(4 points)\n\n\nE.2.2 Trend with bins of house_age\n\nE.2.2.1 \nBin house_age into 5 approximately equal-sized bins.\n(3 points)\n\n\nE.2.2.2 \nAfter binning, plot the mean house_price against the house age bins.\n(3 points)\n\n\nE.2.2.3 \nIs the trend seen with this plot different from that seen with the trendline in the previous question? If yes, then is one of the trends incorrect? Why or why not?\n(4 points)\n\n\nE.2.2.4 \nIs one of the trends more informative? If yes, then which one and how?\n(4 points)\n\n\n\nE.2.3 Trend with number_convenience_stores\nMake a barplot to visualize the mean house_price against number_convenience_stores.\n(3 points)\n\n\nE.2.4 Trend with bins of number_convenience_stores\nBin number_convenience_stores into an appropriate number of bins such that the non-linear trend of the variation of house_price with the bins of number_convenience_stores is retained, while minimizing the number of bins.\nAfter binning, plot the mean house_price against the number_convenience_stores bins.\n(8 points)\n\n\nE.2.5 Size of number_convenience_stores bins\nPrint the size of bins obtained in the previous question. Are the bins of approximately equal size? If not, is it reasonable to have bins of unequal sizes to visualize the trend of house_price with number_convenience_stores.?\n(2 + 4 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Assignment E</span>"
    ]
  },
  {
    "objectID": "Assignment E.html#outliers",
    "href": "Assignment E.html#outliers",
    "title": "Appendix E — Assignment E",
    "section": "E.3 Outliers",
    "text": "E.3 Outliers\n\nE.3.1 Identifying outlying prices\nContinue working with house_features_and_price.csv. Determine how many houses have outlying values in house_price using both the Z-score method and the IQR method.\n\nQuestion: Which method is more conservative for this dataset? (6 + 2 points)\n\n\n\nE.3.2 Quick EDA\nHow are these houses (identified in the previous question as outliers) different from the houses in the rest of the dataset, which might be making them extremely expensive / extremely cheap? Explore the data and mention your hypothesis.\n(8 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Assignment E</span>"
    ]
  },
  {
    "objectID": "Assignment F.html",
    "href": "Assignment F.html",
    "title": "Appendix F — Assignment F",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Assignment F</span>"
    ]
  },
  {
    "objectID": "Assignment F.html#instructions",
    "href": "Assignment F.html#instructions",
    "title": "Appendix F — Assignment F",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on 18th Nov 2024 at 11:59 pm. No extension is possible on this assignment due to tight grading deadlines.\nYou are not allowed to use a for loop in this assignment.\nIf you are updating a dataset (imputing missing values / creating new variables), then use the updated dataset in a subsequent question.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Assignment F</span>"
    ]
  },
  {
    "objectID": "Assignment F.html#canadian-fish-biodiversity",
    "href": "Assignment F.html#canadian-fish-biodiversity",
    "title": "Appendix F — Assignment F",
    "section": "F.1 Canadian Fish Biodiversity",
    "text": "F.1 Canadian Fish Biodiversity\nRead data from the file Canadian_Fish_Biodiversity.csv on Canvas. Each row records a unique fishing event from a 2013 sample of fish populations in Ontario, Canada. To analyze the results of these fishing surveys, we need to understand the dynamics of projects, sites, and geographic locations.\n\nF.1.1 Top 3 projects\nEach site (identified by the column SITEID) represents a time and place at which fishing events occurred. Sites are grouped into broader projects (identified by the column Project Name). We want to understand the scope of these projects.\nUsing groupby(), find the top three projects by number of unique sites.\n(4 points)\n\n\nF.1.2 Missing value imputation with groupby()\n\nF.1.2.1 Number of missing values\nHow many values are missing for the air temperature column (Air Temperature (C))?\n(1 point)\n\n\nF.1.2.2 Missing value imputation: attempt 1\nUsing groupby(), impute the missing values of air temperature with the median air temperature of the corresponding water body (Waterbody Name) and Month.\n(4 points)\n\n\nF.1.2.3 Missing values remaining after attempt 1\nHow many missing values still remain for the air temperature column after the imputation in the previous question?\n(1 point)\n\n\nF.1.2.4 Missing value imputation: attempt 2\nWe will try to impute the remaining missing values for air temperature. Try to impute the remaining missing values of air temperature with the median air temperature of the corresponding project (Project Name) and Month.\n(4 points)\n\n\nF.1.2.5 Missing values remaining after attempt 2\nHow many missing values still remain for the air temperature column after the imputation in the previous question?\n(1 point)\n\n\nF.1.2.6 Air-water temperatures correlation\nFind the correlation between air temperature and water temperature.\n(1 point)\n\n\nF.1.2.7 Missing values remaning after hypothetical attempt 3\nAs you found a high correlation between air temperature and water temperature, you can use water temperature to estimate the air temperature (using the trendline, like you did in assignment 5). Assuming you already did that, how many missing values will still remain for the air temperature column?\nNote: Do not impute the missing values using the trendline, just assume you already did that.\n(3 points)\n\n\nF.1.2.8 Visualizing missing value imputation\nMake a scatterplot of air temperature against water temperature. Highlight the points for which the air temperature was imputed in attempts 1 and 2 with a different color.\n(8 points)\n\n\n\nF.1.3 Living conditions\nThis section begins to investigate the living conditions of fish at different locations and time periods. Continue using the updated dataset with the imputed missing values in attempts 1 and 2 of the previous section.\n\nF.1.3.1 Air-water temperatures: Summary statistics\nUse a single groupby statement to view the minimum, mean, standard deviation, and maximum air temperature and water temperature for each project during the month of August (use the Month column).\n(5 points)\n\n\nF.1.3.2 Air-water temperatures: visualizing yearly trend\nMake lineplots showing maximum air temperature and water temperature by Month and Region. To construct Region, use the Pandas function cut() to satisfy the following conditions:\n\nRows with a latitude lower than 42.4 should have Southern in the Region column\nRows with a latitude between 42.4 and 42.8 should have Central in the Region column\nRows with a latitude higher than 42.8 should have Northern in the Region column\n\nYou can have the month on the horizontal axis, the temperature on the vertical axis, different colors for different regions, and different styles (solid line / dotted line) to indicate air/water temperature.\nDoes anything in the visualization surprise you? Why or why not?\n(14 points)\n\n\n\nF.1.4 Fish diversity\nFinally let’s focus on the stars of this survey—the fish, of course.\n\nF.1.4.1 Top 3 species by Region\nLet’s continue using our Region categorization. Find the top three fish Species in each region by Number Captured.\n(10 points)\n\n\nF.1.4.2 Species spread across Region\nAre certain fish only found in some regions? Visualize how many species are in all three regions, how many are in two of three, and how many were only captured in one region.\n(10 points)\n\n\nF.1.4.3 Exclusive fishes by region\nWhat percentage of all species are exclusively captured in the Southern region? How about the Northern Region? And the Central region?\n(10 points)\nHint:\n\nFind the number of distinct regions in which each species is found.\nFilter the species that are found only in one region.\nGroup the data, containing only the species found in (2), by region, count the number of unique species in each group, and divide by the total number of distinct species.\n\n\n\nF.1.4.4 Turbidity\nTurbidity (Turbidity (ntu)) quantifies the level of cloudiness in liquid. For fish in each of the three regions, is there a linear association between turbidity and number of fish caught? You may consider a correlation higher than 50% in magnitude as presence of a linear association.\n(5 points)\n\n\nF.1.4.5 Fish dimensions\nNow let’s turn to the length of fish captured, given by Maximum (mm) and Minimum (mm). Find the overall maximum and minimum lengths of all fish in each region. Which region has the largest range in captured fish length?\n(4 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Assignment F</span>"
    ]
  },
  {
    "objectID": "Assignment F.html#gdp-surplus-and-compensation",
    "href": "Assignment F.html#gdp-surplus-and-compensation",
    "title": "Appendix F — Assignment F",
    "section": "F.2 GDP, surplus, and compensation",
    "text": "F.2 GDP, surplus, and compensation\nThe dataset Real GDP.csv contains the GDP of each US State for all years starting from 1997 until 2020. The data is at State level, i.e., each observation corresponds to a unique State.\nThe dataset Surplus.csv contains the surplus of each US State for all years starting from 1997 until 2020. The data is at year level, i.e., each observation corresponds to a unique year.\nThe dataset Compensation.csv contains Compensation and Chain-type quantity indexes for real GDP for each US State and year starting from 1997 to 2020. The dataset is at Year-State-Description level, i.e., each observation corresponds to a unique Year-State-Description combination where Description refers to either Compensation or Chain-type quantity indexes for real GDP.\n\nF.2.1 Combining datasets\nCombine all these datasets to obtain a dataset at State-Year level, i.e., each observation corresponds to a unique State-Year combination. The combined dataset must contain the GDP, surplus, Compensation, and Chain-type quantity indexes for real GDP for each US State and all years starting from 1997 until 2020. Note that each observation must contain the name of the US State, year, and the four values (GDP, surplus, compensation, and Chain-type quantity indexes for real GDP).\nHint: Here is one way to do it:\n\nMelt the GDP dataset to year-State level\nMelt the Surplus dataset to year-State level\nPivot the compensation dataset to year-State level\nNow that all the datasets are at the year-State level, merge them!\n\n(3 + 3 + 3 + 1 = 10 points)\n\n\nF.2.2 Time trend: GDP with region\nMerge the file State_region_mapping.csv with the dataset obtained in the previous question. Make a lineplot showing the mean GDP for each of the five regions with year. Do not display the confidence interval. Which two regions seems to have the least growth in GDP over the past 24 years?\n(5 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Assignment F</span>"
    ]
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix G — Datasets & Templates",
    "section": "",
    "text": "Datasets used in the book, and assignment / project templates can be found here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Datasets & Templates</span>"
    ]
  }
]